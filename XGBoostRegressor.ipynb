{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSh88zzk6NKSlfmOvvVKqn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vengottip/Work/blob/main/XGBoostRegressor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyIkmUsqnPJC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J0T9OFuppjBd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_features(X, features_to_remove):\n",
        "    \"\"\"\n",
        "    Removes specified features from the DataFrame.\n",
        "\n",
        "    Args:\n",
        "    - X (pd.DataFrame): The original DataFrame.\n",
        "    - features_to_remove (list): List of column names to remove from X.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: DataFrame after removing specified features.\n",
        "    \"\"\"\n",
        "    return X.drop(columns=features_to_remove, errors='ignore')\n"
      ],
      "metadata": {
        "id": "bxKkX_k7IRmH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Loop through each fold (from fold1 to fold10)\n",
        "for fold_num in range(1, 11):\n",
        "    print(f\"Processing fold{fold_num}...\")\n",
        "    np.random.seed(1390)\n",
        "    # Set the paths for the current fold\n",
        "    folder = f'fold{fold_num}'\n",
        "    train_file_path = os.path.join(folder, 'train.csv')\n",
        "    test_file_path = os.path.join(folder, 'test.csv')\n",
        "    test_y_file_path = os.path.join(folder, 'test_y.csv')\n",
        "\n",
        "    # Load the train, test, and test_y datasets\n",
        "    housing_data_train = pd.read_csv(train_file_path)\n",
        "    housing_data_test = pd.read_csv(test_file_path)\n",
        "    test_y_data = pd.read_csv(test_y_file_path)  # Load the actual Sale Price for the test set\n",
        "\n",
        "    # EXperiment starts\n",
        "    # Define the list of features to remove (this is just an example, adjust as needed)\n",
        "    features_to_remove = [\"Longitude\", \"Latitude\", \"Street\", \"Utilities\", \"Condition_2\",\n",
        "                      \"Roof_Matl\", \"Heating\", \"Pool_QC\", \"Misc_Feature\", \"Misc_Val\",\n",
        "                        \"Low_Qual_Fin_SF\", \"Pool_Area\"]  # Specify the features you want to remove\n",
        "    # Call the remove_features function to remove unwanted features from X_trn\n",
        "    housing_data_train = remove_features(housing_data_train, features_to_remove)\n",
        "\n",
        "    # Create X_train by removing the SalePrice column from housing_data_train\n",
        "    X_train = housing_data_train.drop('Sale_Price', axis=1)\n",
        "\n",
        "    # Create y_train by selecting only the SalePrice column from housing_data_train\n",
        "    y_train = np.log(housing_data_train['Sale_Price'])\n",
        "    #print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "\n",
        "    # Create X_test from the housing_data_test\n",
        "    X_test = housing_data_test\n",
        "\n",
        "    # Create y_test by selecting only the SalePrice column from the test_y_data\n",
        "    y_test = test_y_data['Sale_Price']\n",
        "    #print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "    # Apply one-hot encoding to both X_train and X_test\n",
        "    X_train = pd.get_dummies(X_train)\n",
        "    X_test = pd.get_dummies(X_test)\n",
        "\n",
        "    # Align the columns of X_train and X_test so they have the same structure\n",
        "    X_train, X_test = X_train.align(X_test, join='left', axis=1)\n",
        "\n",
        "    # Fill any missing columns in X_test (which may occur due to one-hot encoding) with 0\n",
        "    X_test = X_test.fillna(0)\n",
        "\n",
        "    #print(f\"Aligned X_train shape: {X_train.shape}, Aligned X_test shape: {X_test.shape}\")\n",
        "\n",
        "    # Train a RandomForestRegressor\n",
        "    #rfModel = RandomForestRegressor(n_estimators=400, oob_score=True, max_features=1.0/3)\n",
        "    #rfModel.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    #yhat_test_rf = rfModel.predict(X_test)\n",
        "\n",
        "    # Calculate Test RMSE for Random Forest\n",
        "    #test_rmse_rf = np.sqrt(mean_squared_error(np.log(y_test), yhat_test_rf))\n",
        "    #print(f\"Fold{fold_num} - Random Forest Test RMSE: {test_rmse_rf}\")\n",
        "\n",
        "    # Calculate Train RMSE for Random Forest (using the out-of-bag predictions)\n",
        "    #yhat_train_oob_rf = rfModel.oob_prediction_\n",
        "    #train_rmse_rf = np.sqrt(mean_squared_error(y_train, yhat_train_oob_rf))\n",
        "    #print(f\"Fold{fold_num} - Random Forest Train RMSE (OOB): {train_rmse_rf}\")\n",
        "\n",
        "    \"\"\"\n",
        "    # Define the model\n",
        "    xgbModel = XGBRegressor(random_state=42)\n",
        "\n",
        "    # Define the parameter grid\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200, 300, 400, 500],  # Number of trees\n",
        "        'learning_rate': [0.01, 0.05, 0.1, 0.2],  # Step size shrinkage\n",
        "        'max_depth': [3, 4, 5, 6],  # Maximum depth of trees\n",
        "        'subsample': [0.6, 0.8, 1.0],  # Fraction of samples to train on\n",
        "        'colsample_bytree': [0.6, 0.8, 1.0],  # Fraction of features used per tree\n",
        "        'gamma': [0, 0.1, 0.2],  # Minimum loss reduction\n",
        "        'reg_alpha': [0, 0.01, 0.1],  # L1 regularization\n",
        "        'reg_lambda': [1, 0.1, 0.01]  # L2 regularization\n",
        "    }\n",
        "\n",
        "    # Randomized search with cross-validation\n",
        "    random_search = RandomizedSearchCV(xgbModel, param_distributions=param_grid,\n",
        "                                      n_iter=50, scoring='neg_root_mean_squared_error',\n",
        "                                      cv=5, verbose=1, random_state=42, n_jobs=-1)\n",
        "\n",
        "    # Fit the random search model\n",
        "    random_search.fit(X_train, y_train)\n",
        "\n",
        "    # Best hyperparameters and RMSE\n",
        "    print(\"Best parameters found: \", random_search.best_params_)\n",
        "    print(\"Best RMSE score: \", -random_search.best_score_)\n",
        "    \"\"\"\n",
        "    # Train an XGBRegressor\n",
        "    #xgbModel = XGBRegressor(n_estimators=400, learning_rate=0.05, max_depth=3, random_state=42)\n",
        "    # Replace with best parameters from RandomizedSearchCV\n",
        "    xgbModel = XGBRegressor(n_estimators=400,\n",
        "                            learning_rate=0.05,\n",
        "                            max_depth=4,\n",
        "                            subsample=0.8,\n",
        "                            colsample_bytree=0.6,\n",
        "                            reg_lambda=0.1,\n",
        "                            reg_alpha=0.01,\n",
        "                            gamma=0,\n",
        "                            random_state=42)\n",
        "    # Accessing best parameters directly from random_search.best_params_\n",
        "    #best_params = random_search.best_params_\n",
        "\n",
        "    # Train the XGBRegressor with the best parameters found from RandomizedSearchCV\n",
        "    #xgbModel = XGBRegressor(**best_params, random_state=42)\n",
        "\n",
        "    xgbModel.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the training set (in-sample predictions)\n",
        "    yhat_train_xgb = xgbModel.predict(X_train)\n",
        "\n",
        "    # Calculate Train RMSE for XGBoost\n",
        "    train_rmse_xgb = np.sqrt(mean_squared_error(y_train, yhat_train_xgb))\n",
        "    print(f\"Fold{fold_num} - XGBoost Train RMSE: {train_rmse_xgb}\")\n",
        "\n",
        "    # Predict on the test set using XGBoost\n",
        "    yhat_test_xgb = xgbModel.predict(X_test)\n",
        "\n",
        "    # Calculate Test RMSE for XGBoost\n",
        "    test_rmse_xgb = np.sqrt(mean_squared_error(np.log(y_test), yhat_test_xgb))\n",
        "    print(f\"Fold{fold_num} - XGBoost Test RMSE: {test_rmse_xgb}\")\n",
        "\n",
        "    #print(f\"Finished processing fold{fold_num}.\\n\")\n",
        "\n",
        "print(\"Completed processing all folds.\")\n"
      ],
      "metadata": {
        "id": "WBNnhqECCWe5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be348bf0-18fb-46f3-c26e-d84825815e97"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing fold1...\n",
            "Fold1 - XGBoost Train RMSE: 0.05193366683570815\n",
            "Fold1 - XGBoost Test RMSE: 0.1153433061435926\n",
            "Processing fold2...\n",
            "Fold2 - XGBoost Train RMSE: 0.049750192640706646\n",
            "Fold2 - XGBoost Test RMSE: 0.11791694689180016\n",
            "Processing fold3...\n",
            "Fold3 - XGBoost Train RMSE: 0.051731474230395945\n",
            "Fold3 - XGBoost Test RMSE: 0.11613088454525501\n",
            "Processing fold4...\n",
            "Fold4 - XGBoost Train RMSE: 0.05140311661923465\n",
            "Fold4 - XGBoost Test RMSE: 0.10989938310462323\n",
            "Processing fold5...\n",
            "Fold5 - XGBoost Train RMSE: 0.05248526007996803\n",
            "Fold5 - XGBoost Test RMSE: 0.10775687038225883\n",
            "Processing fold6...\n",
            "Fold6 - XGBoost Train RMSE: 0.05140486676469129\n",
            "Fold6 - XGBoost Test RMSE: 0.12788898422166814\n",
            "Processing fold7...\n",
            "Fold7 - XGBoost Train RMSE: 0.05048320241997869\n",
            "Fold7 - XGBoost Test RMSE: 0.131695504551595\n",
            "Processing fold8...\n",
            "Fold8 - XGBoost Train RMSE: 0.051276160306289884\n",
            "Fold8 - XGBoost Test RMSE: 0.12434747652836972\n",
            "Processing fold9...\n",
            "Fold9 - XGBoost Train RMSE: 0.052266051753495434\n",
            "Fold9 - XGBoost Test RMSE: 0.129281780870555\n",
            "Processing fold10...\n",
            "Fold10 - XGBoost Train RMSE: 0.052322137961813196\n",
            "Fold10 - XGBoost Test RMSE: 0.12165029453727548\n",
            "Completed processing all folds.\n"
          ]
        }
      ]
    }
  ]
}