{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvU443HWVH8VxgrOj9F4jJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vengottip/Work/blob/main/CS_598_Project1_Ames_Housing_Price_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n",
        "!pip install category_encoders\n",
        "!pip install feature_engine\n",
        "!pip install glmnet_py #Install the correct glmnet package\n",
        "!pip install xgboost\n",
        "!pip install lightgbm\n",
        "!pip install --upgrade scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cs5pmU1ntcd2",
        "outputId": "f899dbe6-e9c2-414f-cfab-cf5b1de0d205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.2.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.7\n",
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.6.4-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.13.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.14.4)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.5.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2024.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.5.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->category_encoders) (24.1)\n",
            "Downloading category_encoders-2.6.4-py2.py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.6.4\n",
            "Collecting feature_engine\n",
            "  Downloading feature_engine-1.8.1-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.10/dist-packages (from feature_engine) (1.26.4)\n",
            "Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from feature_engine) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from feature_engine) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from feature_engine) (1.13.1)\n",
            "Requirement already satisfied: statsmodels>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from feature_engine) (0.14.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.2.0->feature_engine) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.2.0->feature_engine) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.2.0->feature_engine) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.0->feature_engine) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.0->feature_engine) (3.5.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.11.1->feature_engine) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.11.1->feature_engine) (24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels>=0.11.1->feature_engine) (1.16.0)\n",
            "Downloading feature_engine-1.8.1-py2.py3-none-any.whl (364 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.1/364.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: feature_engine\n",
            "Successfully installed feature_engine-1.8.1\n",
            "Collecting glmnet_py\n",
            "  Downloading glmnet_py-0.1.0b2-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: joblib>=0.10.3 in /usr/local/lib/python3.10/dist-packages (from glmnet_py) (1.4.2)\n",
            "Downloading glmnet_py-0.1.0b2-py3-none-any.whl (378 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.3/378.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: glmnet_py\n",
            "Successfully installed glmnet_py-0.1.0b2\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.13.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Collecting scipy\n",
            "  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.26.4)\n",
            "Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scipy-1.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJxYDiYVrvQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64e0b873-55c7-427f-a1e9-7654d8ec5981"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Packages successfully imported!\n"
          ]
        }
      ],
      "source": [
        "# Data manipulation and scientific computation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Machine Learning models\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import catboost as cb\n",
        "\n",
        "# Scikit-learn (for various ML algorithms and utilities)\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Categorical encoders\n",
        "import category_encoders as ce\n",
        "\n",
        "# Handling outliers\n",
        "from feature_engine.outliers import Winsorizer\n",
        "\n",
        "# GLMNET for regularized regression\n",
        "#from glmnet_python import glmnet\n",
        "#from glmnet_python import glmnetPrint\n",
        "\n",
        "# GLMNET for regularized regression\n",
        "#from glmnet import glmnet  # Correct glmnet package for Python\n",
        "\n",
        "\n",
        "# R interface for Python\n",
        "import rpy2.robjects as ro\n",
        "from rpy2.robjects import pandas2ri\n",
        "\n",
        "# Suppressing warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Activate pandas to R data conversion\n",
        "pandas2ri.activate()\n",
        "\n",
        "print(\"Packages successfully imported!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def load_data(train_path, test_path):\n",
        "    \"\"\"Load train and test CSV files into pandas DataFrames.\"\"\"\n",
        "    try:\n",
        "        # Load the CSV files\n",
        "        train_df = pd.read_csv(train_path)\n",
        "        test_df = pd.read_csv(test_path)\n",
        "\n",
        "        print(f\"Train data loaded: {train_df.shape[0]} rows, {train_df.shape[1]} columns\")\n",
        "        print(f\"Test data loaded: {test_df.shape[0]} rows, {test_df.shape[1]} columns\")\n",
        "\n",
        "        return train_df, test_df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading files: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def main():\n",
        "    # Paths to train and test CSV files\n",
        "    train_path = \"train.csv\"\n",
        "    test_path = \"test.csv\"\n",
        "\n",
        "    # Load the train and test datasets\n",
        "    train_df, test_df = load_data(train_path, test_path)\n",
        "\n",
        "    if train_df is not None and test_df is not None:\n",
        "        # Example: Print the first few rows of the train and test datasets\n",
        "        print(\"\\nFirst 5 rows of the train dataset:\")\n",
        "        print(train_df.head())\n",
        "\n",
        "        print(\"\\nFirst 5 rows of the test dataset:\")\n",
        "        print(test_df.head())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZJROp22yc5h",
        "outputId": "168fc32a-ab45-4f23-b52b-c8541c22926e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading files: [Errno 2] No such file or directory: 'train.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "housing_data = pd.read_csv('train.csv')\n",
        "\n",
        "# List all the column names\n",
        "column_names = housing_data.columns.tolist()\n",
        "#column_names\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_columns = ['MS_SubClass', 'MS_Zoning', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities',\n",
        "                       'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type',\n",
        "                       'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style', 'Roof_Matl', 'Exterior_1st',\n",
        "                       'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual', 'Exter_Cond', 'Foundation', 'Bsmt_Qual',\n",
        "                       'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating', 'Heating_QC',\n",
        "                       'Central_Air', 'Electrical', 'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type',\n",
        "                       'Garage_Finish', 'Garage_Qual', 'Garage_Cond', 'Paved_Drive', 'Pool_QC', 'Fence',\n",
        "                       'Misc_Feature', 'Sale_Type', 'Sale_Condition']\n",
        "\n",
        "# Perform dummy encoding (drop_first=True to avoid multicollinearity)\n",
        "housing_data_encoded = pd.get_dummies(housing_data, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "# Get all columns that were created after dummy encoding\n",
        "#encoded_columns = [col for col in housing_data_encoded.columns if col not in housing_data.columns]\n",
        "#print(\"encoded_columns.count\",encoded_columns)\n",
        "\n",
        "# Correct approach to only get the newly created dummy-encoded columns\n",
        "encoded_columns = [col for col in housing_data_encoded.columns if any(cat_col in col for cat_col in categorical_columns)]\n",
        "print(\"encoded_columns.count\",encoded_columns)\n",
        "# Convert boolean columns to integers (True to 1, False to 0)\n",
        "housing_data_encoded[encoded_columns] = housing_data_encoded[encoded_columns].astype(int)\n",
        "\n",
        "# Display the first few rows of the encoded DataFrame with integers\n",
        "housing_data_encoded[encoded_columns].head()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "3U5ZkYnk76VA",
        "outputId": "500de1f8-38f3-4e69-b28f-eddb22ed6158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoded_columns.count ['MS_SubClass_One_Story_1945_and_Older', 'MS_SubClass_One_Story_1946_and_Newer_All_Styles', 'MS_SubClass_One_Story_PUD_1946_and_Newer', 'MS_SubClass_One_Story_with_Finished_Attic_All_Ages', 'MS_SubClass_One_and_Half_Story_Finished_All_Ages', 'MS_SubClass_One_and_Half_Story_Unfinished_All_Ages', 'MS_SubClass_PUD_Multilevel_Split_Level_Foyer', 'MS_SubClass_Split_Foyer', 'MS_SubClass_Split_or_Multilevel', 'MS_SubClass_Two_Family_conversion_All_Styles_and_Ages', 'MS_SubClass_Two_Story_1945_and_Older', 'MS_SubClass_Two_Story_1946_and_Newer', 'MS_SubClass_Two_Story_PUD_1946_and_Newer', 'MS_SubClass_Two_and_Half_Story_All_Ages', 'MS_Zoning_C_all', 'MS_Zoning_Floating_Village_Residential', 'MS_Zoning_I_all', 'MS_Zoning_Residential_High_Density', 'MS_Zoning_Residential_Low_Density', 'MS_Zoning_Residential_Medium_Density', 'Street_Pave', 'Alley_No_Alley_Access', 'Alley_Paved', 'Lot_Shape_Moderately_Irregular', 'Lot_Shape_Regular', 'Lot_Shape_Slightly_Irregular', 'Land_Contour_HLS', 'Land_Contour_Low', 'Land_Contour_Lvl', 'Utilities_NoSewr', 'Lot_Config_CulDSac', 'Lot_Config_FR2', 'Lot_Config_FR3', 'Lot_Config_Inside', 'Land_Slope_Mod', 'Land_Slope_Sev', 'Neighborhood_Blueste', 'Neighborhood_Briardale', 'Neighborhood_Brookside', 'Neighborhood_Clear_Creek', 'Neighborhood_College_Creek', 'Neighborhood_Crawford', 'Neighborhood_Edwards', 'Neighborhood_Gilbert', 'Neighborhood_Green_Hills', 'Neighborhood_Greens', 'Neighborhood_Iowa_DOT_and_Rail_Road', 'Neighborhood_Meadow_Village', 'Neighborhood_Mitchell', 'Neighborhood_North_Ames', 'Neighborhood_Northpark_Villa', 'Neighborhood_Northridge', 'Neighborhood_Northridge_Heights', 'Neighborhood_Northwest_Ames', 'Neighborhood_Old_Town', 'Neighborhood_Sawyer', 'Neighborhood_Sawyer_West', 'Neighborhood_Somerset', 'Neighborhood_South_and_West_of_Iowa_State_University', 'Neighborhood_Stone_Brook', 'Neighborhood_Timberland', 'Neighborhood_Veenker', 'Condition_1_Feedr', 'Condition_1_Norm', 'Condition_1_PosA', 'Condition_1_PosN', 'Condition_1_RRAe', 'Condition_1_RRAn', 'Condition_1_RRNe', 'Condition_1_RRNn', 'Condition_2_Feedr', 'Condition_2_Norm', 'Condition_2_PosA', 'Condition_2_PosN', 'Condition_2_RRNn', 'Bldg_Type_OneFam', 'Bldg_Type_Twnhs', 'Bldg_Type_TwnhsE', 'Bldg_Type_TwoFmCon', 'House_Style_One_and_Half_Fin', 'House_Style_One_and_Half_Unf', 'House_Style_SFoyer', 'House_Style_SLvl', 'House_Style_Two_Story', 'House_Style_Two_and_Half_Fin', 'House_Style_Two_and_Half_Unf', 'Overall_Qual_Average', 'Overall_Qual_Below_Average', 'Overall_Qual_Excellent', 'Overall_Qual_Fair', 'Overall_Qual_Good', 'Overall_Qual_Poor', 'Overall_Qual_Very_Excellent', 'Overall_Qual_Very_Good', 'Overall_Qual_Very_Poor', 'Overall_Cond_Average', 'Overall_Cond_Below_Average', 'Overall_Cond_Excellent', 'Overall_Cond_Fair', 'Overall_Cond_Good', 'Overall_Cond_Poor', 'Overall_Cond_Very_Good', 'Overall_Cond_Very_Poor', 'Roof_Style_Gable', 'Roof_Style_Gambrel', 'Roof_Style_Hip', 'Roof_Style_Mansard', 'Roof_Style_Shed', 'Roof_Matl_CompShg', 'Roof_Matl_Membran', 'Roof_Matl_Metal', 'Roof_Matl_Roll', 'Roof_Matl_Tar&Grv', 'Roof_Matl_WdShake', 'Roof_Matl_WdShngl', 'Exterior_1st_AsphShn', 'Exterior_1st_BrkComm', 'Exterior_1st_BrkFace', 'Exterior_1st_CBlock', 'Exterior_1st_CemntBd', 'Exterior_1st_HdBoard', 'Exterior_1st_ImStucc', 'Exterior_1st_MetalSd', 'Exterior_1st_Plywood', 'Exterior_1st_Stone', 'Exterior_1st_Stucco', 'Exterior_1st_VinylSd', 'Exterior_1st_Wd Sdng', 'Exterior_1st_WdShing', 'Exterior_2nd_AsphShn', 'Exterior_2nd_Brk Cmn', 'Exterior_2nd_BrkFace', 'Exterior_2nd_CBlock', 'Exterior_2nd_CmentBd', 'Exterior_2nd_HdBoard', 'Exterior_2nd_ImStucc', 'Exterior_2nd_MetalSd', 'Exterior_2nd_Other', 'Exterior_2nd_Plywood', 'Exterior_2nd_Stone', 'Exterior_2nd_Stucco', 'Exterior_2nd_VinylSd', 'Exterior_2nd_Wd Sdng', 'Exterior_2nd_Wd Shng', 'Mas_Vnr_Type_BrkFace', 'Mas_Vnr_Type_CBlock', 'Mas_Vnr_Type_Stone', 'Exter_Qual_Fair', 'Exter_Qual_Good', 'Exter_Qual_Typical', 'Exter_Cond_Fair', 'Exter_Cond_Good', 'Exter_Cond_Poor', 'Exter_Cond_Typical', 'Foundation_CBlock', 'Foundation_PConc', 'Foundation_Slab', 'Foundation_Stone', 'Foundation_Wood', 'Bsmt_Qual_Fair', 'Bsmt_Qual_Good', 'Bsmt_Qual_No_Basement', 'Bsmt_Qual_Poor', 'Bsmt_Qual_Typical', 'Bsmt_Cond_Fair', 'Bsmt_Cond_Good', 'Bsmt_Cond_No_Basement', 'Bsmt_Cond_Poor', 'Bsmt_Cond_Typical', 'Bsmt_Exposure_Gd', 'Bsmt_Exposure_Mn', 'Bsmt_Exposure_No', 'Bsmt_Exposure_No_Basement', 'BsmtFin_Type_1_BLQ', 'BsmtFin_Type_1_GLQ', 'BsmtFin_Type_1_LwQ', 'BsmtFin_Type_1_No_Basement', 'BsmtFin_Type_1_Rec', 'BsmtFin_Type_1_Unf', 'BsmtFin_Type_2_BLQ', 'BsmtFin_Type_2_GLQ', 'BsmtFin_Type_2_LwQ', 'BsmtFin_Type_2_No_Basement', 'BsmtFin_Type_2_Rec', 'BsmtFin_Type_2_Unf', 'Heating_GasW', 'Heating_Grav', 'Heating_OthW', 'Heating_Wall', 'Heating_QC_Fair', 'Heating_QC_Good', 'Heating_QC_Poor', 'Heating_QC_Typical', 'Central_Air_Y', 'Electrical_FuseF', 'Electrical_FuseP', 'Electrical_Mix', 'Electrical_SBrkr', 'Electrical_Unknown', 'Kitchen_Qual_Fair', 'Kitchen_Qual_Good', 'Kitchen_Qual_Typical', 'Functional_Maj2', 'Functional_Min1', 'Functional_Min2', 'Functional_Mod', 'Functional_Sev', 'Functional_Typ', 'Fireplace_Qu_Fair', 'Fireplace_Qu_Good', 'Fireplace_Qu_No_Fireplace', 'Fireplace_Qu_Poor', 'Fireplace_Qu_Typical', 'Garage_Type_Basment', 'Garage_Type_BuiltIn', 'Garage_Type_CarPort', 'Garage_Type_Detchd', 'Garage_Type_More_Than_Two_Types', 'Garage_Type_No_Garage', 'Garage_Finish_No_Garage', 'Garage_Finish_RFn', 'Garage_Finish_Unf', 'Garage_Qual_Fair', 'Garage_Qual_Good', 'Garage_Qual_No_Garage', 'Garage_Qual_Poor', 'Garage_Qual_Typical', 'Garage_Cond_Fair', 'Garage_Cond_Good', 'Garage_Cond_No_Garage', 'Garage_Cond_Poor', 'Garage_Cond_Typical', 'Paved_Drive_Partial_Pavement', 'Paved_Drive_Paved', 'Pool_QC_Fair', 'Pool_QC_Good', 'Pool_QC_No_Pool', 'Pool_QC_Typical', 'Fence_Good_Wood', 'Fence_Minimum_Privacy', 'Fence_Minimum_Wood_Wire', 'Fence_No_Fence', 'Misc_Feature_Othr', 'Misc_Feature_Shed', 'Misc_Feature_TenC', 'Sale_Type_CWD', 'Sale_Type_Con', 'Sale_Type_ConLD', 'Sale_Type_ConLI', 'Sale_Type_ConLw', 'Sale_Type_New', 'Sale_Type_Oth', 'Sale_Type_VWD', 'Sale_Type_WD ', 'Sale_Condition_AdjLand', 'Sale_Condition_Alloca', 'Sale_Condition_Family', 'Sale_Condition_Normal', 'Sale_Condition_Partial']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   MS_SubClass_One_Story_1945_and_Older  \\\n",
              "0                                     0   \n",
              "1                                     0   \n",
              "2                                     0   \n",
              "3                                     0   \n",
              "4                                     0   \n",
              "\n",
              "   MS_SubClass_One_Story_1946_and_Newer_All_Styles  \\\n",
              "0                                                1   \n",
              "1                                                1   \n",
              "2                                                0   \n",
              "3                                                0   \n",
              "4                                                0   \n",
              "\n",
              "   MS_SubClass_One_Story_PUD_1946_and_Newer  \\\n",
              "0                                         0   \n",
              "1                                         0   \n",
              "2                                         0   \n",
              "3                                         0   \n",
              "4                                         1   \n",
              "\n",
              "   MS_SubClass_One_Story_with_Finished_Attic_All_Ages  \\\n",
              "0                                                  0    \n",
              "1                                                  0    \n",
              "2                                                  0    \n",
              "3                                                  0    \n",
              "4                                                  0    \n",
              "\n",
              "   MS_SubClass_One_and_Half_Story_Finished_All_Ages  \\\n",
              "0                                                 0   \n",
              "1                                                 0   \n",
              "2                                                 0   \n",
              "3                                                 0   \n",
              "4                                                 0   \n",
              "\n",
              "   MS_SubClass_One_and_Half_Story_Unfinished_All_Ages  \\\n",
              "0                                                  0    \n",
              "1                                                  0    \n",
              "2                                                  0    \n",
              "3                                                  0    \n",
              "4                                                  0    \n",
              "\n",
              "   MS_SubClass_PUD_Multilevel_Split_Level_Foyer  MS_SubClass_Split_Foyer  \\\n",
              "0                                             0                        0   \n",
              "1                                             0                        0   \n",
              "2                                             0                        0   \n",
              "3                                             0                        0   \n",
              "4                                             0                        0   \n",
              "\n",
              "   MS_SubClass_Split_or_Multilevel  \\\n",
              "0                                0   \n",
              "1                                0   \n",
              "2                                0   \n",
              "3                                0   \n",
              "4                                0   \n",
              "\n",
              "   MS_SubClass_Two_Family_conversion_All_Styles_and_Ages  ...  \\\n",
              "0                                                  0      ...   \n",
              "1                                                  0      ...   \n",
              "2                                                  0      ...   \n",
              "3                                                  0      ...   \n",
              "4                                                  0      ...   \n",
              "\n",
              "   Sale_Type_ConLw  Sale_Type_New  Sale_Type_Oth  Sale_Type_VWD  \\\n",
              "0                0              0              0              0   \n",
              "1                0              0              0              0   \n",
              "2                0              0              0              0   \n",
              "3                0              0              0              0   \n",
              "4                0              0              0              0   \n",
              "\n",
              "   Sale_Type_WD   Sale_Condition_AdjLand  Sale_Condition_Alloca  \\\n",
              "0              1                       0                      0   \n",
              "1              1                       0                      0   \n",
              "2              1                       0                      0   \n",
              "3              1                       0                      0   \n",
              "4              1                       0                      0   \n",
              "\n",
              "   Sale_Condition_Family  Sale_Condition_Normal  Sale_Condition_Partial  \n",
              "0                      0                      1                       0  \n",
              "1                      0                      1                       0  \n",
              "2                      0                      1                       0  \n",
              "3                      0                      1                       0  \n",
              "4                      0                      1                       0  \n",
              "\n",
              "[5 rows x 259 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b7232e66-ad94-489f-99e7-763707d822a9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MS_SubClass_One_Story_1945_and_Older</th>\n",
              "      <th>MS_SubClass_One_Story_1946_and_Newer_All_Styles</th>\n",
              "      <th>MS_SubClass_One_Story_PUD_1946_and_Newer</th>\n",
              "      <th>MS_SubClass_One_Story_with_Finished_Attic_All_Ages</th>\n",
              "      <th>MS_SubClass_One_and_Half_Story_Finished_All_Ages</th>\n",
              "      <th>MS_SubClass_One_and_Half_Story_Unfinished_All_Ages</th>\n",
              "      <th>MS_SubClass_PUD_Multilevel_Split_Level_Foyer</th>\n",
              "      <th>MS_SubClass_Split_Foyer</th>\n",
              "      <th>MS_SubClass_Split_or_Multilevel</th>\n",
              "      <th>MS_SubClass_Two_Family_conversion_All_Styles_and_Ages</th>\n",
              "      <th>...</th>\n",
              "      <th>Sale_Type_ConLw</th>\n",
              "      <th>Sale_Type_New</th>\n",
              "      <th>Sale_Type_Oth</th>\n",
              "      <th>Sale_Type_VWD</th>\n",
              "      <th>Sale_Type_WD</th>\n",
              "      <th>Sale_Condition_AdjLand</th>\n",
              "      <th>Sale_Condition_Alloca</th>\n",
              "      <th>Sale_Condition_Family</th>\n",
              "      <th>Sale_Condition_Normal</th>\n",
              "      <th>Sale_Condition_Partial</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 259 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b7232e66-ad94-489f-99e7-763707d822a9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b7232e66-ad94-489f-99e7-763707d822a9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b7232e66-ad94-489f-99e7-763707d822a9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fc25542b-83aa-428f-912f-785e94ceee98\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fc25542b-83aa-428f-912f-785e94ceee98')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fc25542b-83aa-428f-912f-785e94ceee98 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate RMSE\n",
        "def calc_rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n"
      ],
      "metadata": {
        "id": "aPbFbkPrClWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Backward elimination function (based on coefficient values for regularization)\n",
        "def backward_elimination_ridge(X, y, alpha=1.0):\n",
        "    # Fit the Ridge model\n",
        "    model = Ridge(alpha=alpha).fit(X, y)\n",
        "    # Identify features with very small coefficients and drop them\n",
        "    while True:\n",
        "        coefs = model.coef_\n",
        "        smallest_coef = np.argmin(np.abs(coefs))\n",
        "        if np.abs(coefs[smallest_coef]) < 1e-4:  # Threshold for eliminating small coefficients\n",
        "            X = X.drop(X.columns[smallest_coef], axis=1)\n",
        "            model = Ridge(alpha=alpha).fit(X, y)\n",
        "        else:\n",
        "            break\n",
        "    return model"
      ],
      "metadata": {
        "id": "wjVIF0LUCoFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate AIC for a linear regression model\n",
        "def calculate_aic(n, rss, k):\n",
        "    \"\"\"Calculate AIC (Akaike Information Criterion).\n",
        "    n: Number of observations\n",
        "    rss: Residual Sum of Squares (from model)\n",
        "    k: Number of parameters (number of features + intercept)\n",
        "    \"\"\"\n",
        "    return n * np.log(rss / n) + 2 * k\n"
      ],
      "metadata": {
        "id": "cuM4QYsIWp4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Backward elimination based on AIC with Ridge regression\n",
        "def backward_elimination_ridge_aic(X, y, alpha=1.0):\n",
        "    # Initialize the Ridge regression model\n",
        "    model = Ridge(alpha=alpha)\n",
        "\n",
        "    # Start with all features\n",
        "    X_current = X.copy()\n",
        "\n",
        "    # Track removed features and how many were removed\n",
        "    removed_features = []\n",
        "    num_removed = 0\n",
        "\n",
        "    # Fit the model with all predictors\n",
        "    model.fit(X_current, y)\n",
        "    n = len(y)  # Number of observations\n",
        "    residuals = y - model.predict(X_current)\n",
        "    rss = np.sum(residuals**2)  # Calculate Residual Sum of Squares (RSS)\n",
        "    k = X_current.shape[1] + 1  # Number of features + intercept\n",
        "\n",
        "    aic_current = calculate_aic(n, rss, k)\n",
        "\n",
        "    while True:\n",
        "        aic_old = aic_current\n",
        "\n",
        "\n",
        "        # Fit the model and calculate t-statistics (approximate p-values using t-statistics)\n",
        "        model.fit(X_current, y)\n",
        "        pvalues = np.abs(model.coef_) / np.std(X_current, axis=0)  # Approximation of t-statistics\n",
        "        print(\"pvalues\", pvalues)\n",
        "        # Remove feature with the smallest t-statistic (or largest p-value approximation)\n",
        "        worst_feature_idx = np.argmin(pvalues)\n",
        "        worst_feature = X_current.columns[worst_feature_idx]\n",
        "        print(\"Worst Feature\", worst_feature)\n",
        "        # Drop the worst feature\n",
        "        X_new = X_current.drop(columns=[worst_feature])\n",
        "\n",
        "        # Track the removed feature\n",
        "        removed_features.append(worst_feature)\n",
        "        num_removed += 1\n",
        "\n",
        "\n",
        "        # Fit the new Ridge model and calculate AIC again\n",
        "        model.fit(X_new, y)\n",
        "        residuals_new = y - model.predict(X_new)\n",
        "        rss_new = np.sum(residuals_new**2)  # Recalculate RSS\n",
        "        k_new = X_new.shape[1] + 1  # Number of features + intercept\n",
        "        aic_new = calculate_aic(n, rss_new, k_new)\n",
        "        print(\"AIC_old\", aic_old)\n",
        "        print(\"AIC_new\", aic_new)\n",
        "        # If AIC improves (decreases), keep the new model, else stop\n",
        "        if aic_new < aic_old:\n",
        "            X_current = X_new\n",
        "            aic_current = aic_new\n",
        "            print(\"AIC_current\", aic_current)\n",
        "            print(\"X_current\", X_current.columns.size)\n",
        "        else:\n",
        "            print(\"AIC_current_before_Break\", aic_current)\n",
        "            break\n",
        "    print(\"Num Removed\", num_removed)\n",
        "    print(\"Removed Features\", removed_features)\n",
        "    print(\"X_current\", X_current.columns.size)\n",
        "    return model, X_new, removed_features"
      ],
      "metadata": {
        "id": "IW3BypFSqtLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming ridge_model_mse is already defined somewhere in your code\n",
        "# from ridge_model_mse import ridge_model_mse\n",
        "\n",
        "def calc_rmse(actual, predicted):\n",
        "    \"\"\"Calculate Root Mean Squared Error.\"\"\"\n",
        "    return np.sqrt(np.mean((actual - predicted) ** 2))"
      ],
      "metadata": {
        "id": "fEohQU0AGyjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Load the dataset\n",
        "    file_path = 'train.csv'\n",
        "    housing_data = pd.read_csv(file_path)\n",
        "\n",
        "    # Verify column names and check if Sale_Price exists\n",
        "    if 'Sale_Price' not in housing_data.columns:\n",
        "        print(\"Error: 'Sale_Price' column not found in the dataset.\")\n",
        "        print(\"Available columns:\", housing_data.columns)\n",
        "        return\n",
        "\n",
        "    # Identify categorical columns\n",
        "    categorical_columns = ['MS_SubClass', 'MS_Zoning', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities',\n",
        "                           'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type',\n",
        "                           'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style', 'Roof_Matl', 'Exterior_1st',\n",
        "                           'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual', 'Exter_Cond', 'Foundation', 'Bsmt_Qual',\n",
        "                           'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating', 'Heating_QC',\n",
        "                           'Central_Air', 'Electrical', 'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type',\n",
        "                           'Garage_Finish', 'Garage_Qual', 'Garage_Cond', 'Paved_Drive', 'Pool_QC', 'Fence',\n",
        "                           'Misc_Feature', 'Sale_Type', 'Sale_Condition']\n",
        "\n",
        "    # Perform dummy encoding on the entire dataset\n",
        "    housing_data_encoded = pd.get_dummies(housing_data, columns=categorical_columns, drop_first=True)\n",
        "    housing_data_encoded.head()\n",
        "    # Handle missing values by imputing them (e.g., using median imputation)\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    housing_data_encoded_imputed = pd.DataFrame(imputer.fit_transform(housing_data_encoded), columns=housing_data_encoded.columns)\n",
        "\n",
        "    # Ensure no missing values remain\n",
        "    if housing_data_encoded_imputed.isna().sum().any():\n",
        "        print(\"Error: There are still NaN values after imputation.\")\n",
        "        return\n",
        "\n",
        "    # Now split the imputed dataset into 80% training and 20% testing\n",
        "    #car_data_trn, car_data_tst = train_test_split(housing_data_encoded_imputed, test_size=0.20, random_state=42)\n",
        "\n",
        "    # ** Remove PID from the features as it is just an identifier **\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_imputed.drop(columns=['PID'])\n",
        "    #car_data_tst = car_data_tst.drop(columns=['PID'])\n",
        "\n",
        "    # Ensure consistent feature names\n",
        "    housing_data_encoded_imputed_train.columns = housing_data_encoded_imputed_train.columns.str.strip()\n",
        "    #car_data_tst.columns = car_data_tst.columns.str.strip()\n",
        "\n",
        "\n",
        "    # Separate predictors (X) and target variable (y) for training and testing sets\n",
        "    X_trn = housing_data_encoded_imputed_train.drop(columns='Sale_Price')\n",
        "    y_trn = np.log(housing_data_encoded_imputed_train['Sale_Price'])  # Log-transform the target variable for training set\n",
        "\n",
        "    #X_tst = car_data_tst.drop(columns='Sale_Price')\n",
        "    #y_tst = np.log(car_data_tst['Sale_Price'])  # Log-transform the target variable for test set\n",
        "\n",
        "    # Ensure Sale_Price contains no zeros or negative values before log-transforming\n",
        "    if (housing_data_encoded_imputed_train['Sale_Price'] <= 0).sum() > 0 or (housing_data_encoded_imputed_train['Sale_Price'] <= 0).sum() > 0:\n",
        "        print(\"Error: 'Sale_Price' contains non-positive values, cannot apply log transformation.\")\n",
        "        return\n",
        "\n",
        "    # Apply backward elimination using Ridge regression based on small coefficients\n",
        "    model_ridge, final_features_trn, _ = backward_elimination_ridge_aic(X_trn, y_trn, alpha=1.0)\n",
        "    print(\"X_trn\", X_trn.columns)\n",
        "    #print(\"X_tst\", X_tst.columns)\n",
        "    print(\"Final Features (Ridge):\", final_features_trn.columns)\n",
        "\n",
        "    # Number of features in training and test sets\n",
        "    print(\"Number of Features in X_trn:\", X_trn.shape[1])\n",
        "    #print(\"Number of Features in X_tst:\", X_tst.shape[1])\n",
        "    print(\"Number of Features in final_features_trn:\", final_features_trn.shape[1])\n",
        "    #print(\"Number of Features in final_features_tst:\", final_features_tst.shape[1])\n",
        "    # Find the intersection between final features and categorical columns\n",
        "    categorical_features_in_final = [col for col in final_features_trn.columns if col in categorical_columns]\n",
        "\n",
        "    # Print the categorical features\n",
        "    print(\"Categorical Features in final_features_trn:\", categorical_features_in_final)\n",
        "\n",
        "    # Print adjusted R-squared for the Ridge model after backward elimination\n",
        "    y_trn_pred = model_ridge.predict(final_features_trn)\n",
        "    r2_adj = 1 - (1 - r2_score(y_trn, y_trn_pred)) * (len(y_trn) - 1) / (len(y_trn) - final_features_trn.shape[1] - 1)\n",
        "    print(\"Adjusted R-squared (Training):\", r2_adj)\n",
        "\n",
        "    # For the test set, we need to reduce the test set to the same final set of features\n",
        "    #final_features_tst = X_tst[final_features_trn.columns]\n",
        "\n",
        "    # Predict on the test set using the reduced features\n",
        "    #y_tst_pred = model_ridge.predict(final_features_tst)\n",
        "    #test_rmse = calc_rmse(y_tst, y_tst_pred)\n",
        "    #print(\"Test RMSE:\", test_rmse)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-jQ8qCZtHqF",
        "outputId": "a470350f-b396-4a46-b0f0-68eea308d0f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pvalues Lot_Frontage              3.601671e-06\n",
            "Lot_Area                  2.401535e-10\n",
            "Year_Built                4.333100e-05\n",
            "Year_Remod_Add            3.156582e-05\n",
            "Mas_Vnr_Area              1.046480e-07\n",
            "                              ...     \n",
            "Sale_Condition_AdjLand    2.298149e+00\n",
            "Sale_Condition_Alloca     1.178436e+00\n",
            "Sale_Condition_Family     1.259268e-01\n",
            "Sale_Condition_Normal     2.044621e-01\n",
            "Sale_Condition_Partial    4.622664e-01\n",
            "Length: 294, dtype: float64\n",
            "Worst Feature Lot_Area\n",
            "AIC_old -8626.592797409803\n",
            "AIC_new -8615.812035321345\n",
            "AIC_current_before_Break -8626.592797409803\n",
            "Num Removed 1\n",
            "Removed Features ['Lot_Area']\n",
            "X_current 294\n",
            "X_trn Index(['Lot_Frontage', 'Lot_Area', 'Year_Built', 'Year_Remod_Add',\n",
            "       'Mas_Vnr_Area', 'BsmtFin_SF_1', 'BsmtFin_SF_2', 'Bsmt_Unf_SF',\n",
            "       'Total_Bsmt_SF', 'First_Flr_SF',\n",
            "       ...\n",
            "       'Sale_Type_ConLw', 'Sale_Type_New', 'Sale_Type_Oth', 'Sale_Type_VWD',\n",
            "       'Sale_Type_WD', 'Sale_Condition_AdjLand', 'Sale_Condition_Alloca',\n",
            "       'Sale_Condition_Family', 'Sale_Condition_Normal',\n",
            "       'Sale_Condition_Partial'],\n",
            "      dtype='object', length=294)\n",
            "Final Features (Ridge): Index(['Lot_Frontage', 'Year_Built', 'Year_Remod_Add', 'Mas_Vnr_Area',\n",
            "       'BsmtFin_SF_1', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF',\n",
            "       'First_Flr_SF', 'Second_Flr_SF',\n",
            "       ...\n",
            "       'Sale_Type_ConLw', 'Sale_Type_New', 'Sale_Type_Oth', 'Sale_Type_VWD',\n",
            "       'Sale_Type_WD', 'Sale_Condition_AdjLand', 'Sale_Condition_Alloca',\n",
            "       'Sale_Condition_Family', 'Sale_Condition_Normal',\n",
            "       'Sale_Condition_Partial'],\n",
            "      dtype='object', length=293)\n",
            "Number of Features in X_trn: 294\n",
            "Number of Features in final_features_trn: 293\n",
            "Categorical Features in final_features_trn: []\n",
            "Adjusted R-squared (Training): 0.9231607997178729\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Load the train and test datasets\n",
        "    train_file_path = 'train.csv'\n",
        "    test_file_path = 'test.csv'\n",
        "    housing_data_train = pd.read_csv(train_file_path)\n",
        "    housing_data_test = pd.read_csv(test_file_path)\n",
        "\n",
        "    # Identify categorical columns\n",
        "    categorical_columns = ['MS_SubClass', 'MS_Zoning', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities',\n",
        "                           'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type',\n",
        "                           'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style', 'Roof_Matl', 'Exterior_1st',\n",
        "                           'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual', 'Exter_Cond', 'Foundation', 'Bsmt_Qual',\n",
        "                           'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating', 'Heating_QC',\n",
        "                           'Central_Air', 'Electrical', 'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type',\n",
        "                           'Garage_Finish', 'Garage_Qual', 'Garage_Cond', 'Paved_Drive', 'Pool_QC', 'Fence',\n",
        "                           'Misc_Feature', 'Sale_Type', 'Sale_Condition']\n",
        "\n",
        "    # Perform dummy encoding on the train dataset\n",
        "    housing_data_encoded_train = pd.get_dummies(housing_data_train, columns=categorical_columns, drop_first=True)\n",
        "    housing_data_encoded_test = pd.get_dummies(housing_data_test, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "    # Handle missing values by imputing them (median imputation) for numeric columns\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    numeric_columns = housing_data_encoded_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numeric_columns = [col for col in numeric_columns if col != 'Sale_Price']  # Exclude 'Sale_Price'\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_train.copy()\n",
        "    housing_data_encoded_imputed_train[numeric_columns] = imputer.fit_transform(housing_data_encoded_train[numeric_columns])\n",
        "\n",
        "    # For test data, apply the same imputation strategy\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_test.copy()\n",
        "    housing_data_encoded_imputed_test[numeric_columns] = imputer.transform(housing_data_encoded_test[numeric_columns])\n",
        "\n",
        "    # Remove 'PID' from train and test datasets\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_imputed_train.drop(columns=['PID'])\n",
        "    if 'PID' in housing_data_encoded_imputed_test.columns:\n",
        "        housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.drop(columns=['PID'])\n",
        "\n",
        "    # Align the columns of the test dataset to match the train dataset\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.reindex(columns=housing_data_encoded_imputed_train.columns.drop('Sale_Price'), fill_value=0)\n",
        "\n",
        "    # Separate predictors (X) and target variable (y) for training\n",
        "    X_trn = housing_data_encoded_imputed_train.drop(columns='Sale_Price')\n",
        "    y_trn = np.log(housing_data_encoded_imputed_train['Sale_Price'])\n",
        "\n",
        "    # Apply backward elimination using Ridge regression based on small coefficients\n",
        "    model_ridge, final_features_trn, removed_features = backward_elimination_ridge_aic(X_trn, y_trn, alpha=1.0)\n",
        "\n",
        "    # Remove the same features from the test dataset\n",
        "    X_tst = housing_data_encoded_imputed_test.drop(columns=removed_features, errors='ignore')\n",
        "\n",
        "    # Predict on the test set using the trained Ridge model\n",
        "    y_tst_pred = model_ridge.predict(X_tst)\n",
        "\n",
        "    # Output some model info\n",
        "    print(\"Final Features (Ridge):\", final_features_trn.columns)\n",
        "    print(\"Number of Features in X_trn:\", X_trn.shape[1])\n",
        "    print(\"Number of Features in final_features_trn:\", final_features_trn.shape[1])\n",
        "    print(\"Removed Features:\", removed_features)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1y2SMXrW1C_",
        "outputId": "9f83764f-a910-4b8c-8852-64c7e977d48a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pvalues Lot_Frontage              3.601671e-06\n",
            "Lot_Area                  2.401535e-10\n",
            "Year_Built                4.333100e-05\n",
            "Year_Remod_Add            3.156582e-05\n",
            "Mas_Vnr_Area              1.046480e-07\n",
            "                              ...     \n",
            "Sale_Condition_AdjLand    2.298149e+00\n",
            "Sale_Condition_Alloca     1.178436e+00\n",
            "Sale_Condition_Family     1.259268e-01\n",
            "Sale_Condition_Normal     2.044621e-01\n",
            "Sale_Condition_Partial    4.622664e-01\n",
            "Length: 294, dtype: float64\n",
            "Worst Feature Lot_Area\n",
            "AIC_old -8626.592797409803\n",
            "AIC_new -8615.812035321345\n",
            "AIC_current_before_Break -8626.592797409803\n",
            "Num Removed 1\n",
            "Removed Features ['Lot_Area']\n",
            "X_current 294\n",
            "Final Features (Ridge): Index(['Lot_Frontage', 'Year_Built', 'Year_Remod_Add', 'Mas_Vnr_Area',\n",
            "       'BsmtFin_SF_1', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF',\n",
            "       'First_Flr_SF', 'Second_Flr_SF',\n",
            "       ...\n",
            "       'Sale_Type_ConLw', 'Sale_Type_New', 'Sale_Type_Oth', 'Sale_Type_VWD',\n",
            "       'Sale_Type_WD ', 'Sale_Condition_AdjLand', 'Sale_Condition_Alloca',\n",
            "       'Sale_Condition_Family', 'Sale_Condition_Normal',\n",
            "       'Sale_Condition_Partial'],\n",
            "      dtype='object', length=293)\n",
            "Number of Features in X_trn: 294\n",
            "Number of Features in final_features_trn: 293\n",
            "Removed Features: ['Lot_Area']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "def ridge_model_mse(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Performs Ridge regression with cross-validation to select the optimal alpha\n",
        "    and returns the Mean Squared Error (MSE) on the test data.\n",
        "\n",
        "    Parameters:\n",
        "    X_train: Training feature matrix (DataFrame or numpy array)\n",
        "    y_train: Training response vector (Series or numpy array)\n",
        "    X_test: Testing feature matrix (DataFrame or numpy array)\n",
        "    y_test: Testing response vector (Series or numpy array)\n",
        "\n",
        "    Returns:\n",
        "    mse: Mean Squared Error on the test data\n",
        "    alpha: Optimal alpha value selected by cross-validation\n",
        "    \"\"\"\n",
        "    # Ignore any warnings that may arise from using this method\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "    # Define the range of alpha values for Ridge regression\n",
        "    ridge_alphas = np.logspace(-10, 1, 100)\n",
        "\n",
        "    # Create a Ridge regression model with cross-validation\n",
        "    ridgecv = RidgeCV(alphas=ridge_alphas, cv=10, scoring='neg_mean_squared_error', normalize=True)\n",
        "\n",
        "    # Fit the model on the training data\n",
        "    ridgecv.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test data\n",
        "    y_pred = ridgecv.predict(X_test)\n",
        "\n",
        "    # Calculate the Mean Squared Error (MSE)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    # Get the optimal alpha selected by RidgeCV\n",
        "    alpha = ridgecv.alpha_\n",
        "\n",
        "    return mse, alpha\n",
        "\n",
        "# Example usage:\n",
        "# mse, alpha = ridge_model_mse(X_train, y_train, X_test, y_test)\n",
        "# print(f\"Optimal Alpha: {alpha}, Test MSE: {mse}\")\n"
      ],
      "metadata": {
        "id": "Pv3L9irImKSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "def calc_rmse(actual, predicted):\n",
        "    \"\"\"Calculate Root Mean Squared Error (RMSE).\"\"\"\n",
        "    return np.sqrt(np.mean((actual - predicted) ** 2))\n",
        "\n",
        "def adjusted_r_squared(r2, n, k):\n",
        "    \"\"\"Calculate the adjusted R-squared.\"\"\"\n",
        "    return 1 - (1 - r2) * (n - 1) / (n - k - 1)\n",
        "\n",
        "def main():\n",
        "    # Load the train and test datasets\n",
        "    train_file_path = 'fold8/train.csv'\n",
        "    test_file_path = 'fold8/test.csv'\n",
        "    housing_data_train = pd.read_csv(train_file_path)\n",
        "    housing_data_test = pd.read_csv(test_file_path)\n",
        "\n",
        "    # Identify categorical columns\n",
        "    categorical_columns = ['MS_SubClass', 'MS_Zoning', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities',\n",
        "                           'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type',\n",
        "                           'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style', 'Roof_Matl', 'Exterior_1st',\n",
        "                           'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual', 'Exter_Cond', 'Foundation', 'Bsmt_Qual',\n",
        "                           'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating', 'Heating_QC',\n",
        "                           'Central_Air', 'Electrical', 'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type',\n",
        "                           'Garage_Finish', 'Garage_Qual', 'Garage_Cond', 'Paved_Drive', 'Pool_QC', 'Fence',\n",
        "                           'Misc_Feature', 'Sale_Type', 'Sale_Condition']\n",
        "\n",
        "    # Perform dummy encoding on the train dataset\n",
        "    housing_data_encoded_train = pd.get_dummies(housing_data_train, columns=categorical_columns, drop_first=True)\n",
        "    housing_data_encoded_test = pd.get_dummies(housing_data_test, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "    # Handle missing values by imputing them in train and test data (median imputation) for numeric columns\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    numeric_columns = housing_data_encoded_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numeric_columns = [col for col in numeric_columns if col != 'Sale_Price']  # Exclude 'Sale_Price'\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_train.copy()\n",
        "    housing_data_encoded_imputed_train[numeric_columns] = imputer.fit_transform(housing_data_encoded_train[numeric_columns])\n",
        "\n",
        "    # For test data, apply the same imputation strategy\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_test.copy()\n",
        "    housing_data_encoded_imputed_test[numeric_columns] = imputer.transform(housing_data_encoded_test[numeric_columns])\n",
        "\n",
        "    # Remove 'PID' from train and test datasets\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_imputed_train.drop(columns=['PID'])\n",
        "    if 'PID' in housing_data_encoded_imputed_test.columns:\n",
        "        housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.drop(columns=['PID'])\n",
        "\n",
        "    # Align the columns of the test dataset to match the train dataset\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.reindex(columns=housing_data_encoded_imputed_train.columns.drop('Sale_Price'), fill_value=0)\n",
        "\n",
        "    # Separate predictors (X) and target variable (y) for training\n",
        "    X_trn = housing_data_encoded_imputed_train.drop(columns='Sale_Price')\n",
        "    y_trn = np.log(housing_data_encoded_imputed_train['Sale_Price'])\n",
        "\n",
        "    # Apply backward elimination using Ridge regression based on small coefficients\n",
        "    model_ridge, final_features_trn, removed_features = backward_elimination_ridge_aic(X_trn, y_trn, alpha=1.0)\n",
        "\n",
        "    # Remove the same features from the test dataset\n",
        "    X_tst = housing_data_encoded_imputed_test.drop(columns=removed_features, errors='ignore')\n",
        "\n",
        "    # Predict on the training set\n",
        "    y_trn_pred = model_ridge.predict(final_features_trn)\n",
        "\n",
        "    # Calculate R-squared and adjusted R-squared for the training data\n",
        "    r2_train = r2_score(y_trn, y_trn_pred)\n",
        "    adj_r2_train = adjusted_r_squared(r2_train, len(y_trn), final_features_trn.shape[1])\n",
        "\n",
        "    # Calculate RMSE for the training data\n",
        "    train_rmse = calc_rmse(y_trn, y_trn_pred)\n",
        "\n",
        "    print(\"Training R-squared:\", r2_train)\n",
        "    print(\"Adjusted R-squared (Training):\", adj_r2_train)\n",
        "    print(\"Training RMSE:\", train_rmse)\n",
        "\n",
        "    # Predict on the test set using the trained Ridge model\n",
        "    y_tst_pred = model_ridge.predict(X_tst)\n",
        "\n",
        "    # Evaluate on the test data (assuming test.csv has a Sale_Price column)\n",
        "    if 'Sale_Price' in housing_data_encoded_test.columns:\n",
        "        y_tst = np.log(housing_data_encoded_test['Sale_Price'])  # Log-transform the target variable for test set\n",
        "\n",
        "        # Calculate R-squared and adjusted R-squared for the test data\n",
        "        r2_test = r2_score(y_tst, y_tst_pred)\n",
        "        adj_r2_test = adjusted_r_squared(r2_test, len(y_tst), X_tst.shape[1])\n",
        "\n",
        "        # Calculate RMSE for the test data\n",
        "        test_rmse = calc_rmse(y_tst, y_tst_pred)\n",
        "\n",
        "        print(\"Test R-squared:\", r2_test)\n",
        "        print(\"Adjusted R-squared (Test):\", adj_r2_test)\n",
        "        print(\"Test RMSE:\", test_rmse)\n",
        "    else:\n",
        "        print(\"Warning: 'Sale_Price' column not found in test.csv. Unable to calculate RMSE.\")\n",
        "\n",
        "    # Output some model info\n",
        "    print(\"Final Features (Ridge):\", final_features_trn.columns)\n",
        "    print(\"Number of Features in X_trn:\", X_trn.shape[1])\n",
        "    print(\"Number of Features in final_features_trn:\", final_features_trn.shape[1])\n",
        "    print(\"Removed Features:\", removed_features)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7doAfNdJfg0",
        "outputId": "772e23e7-27b1-4ee4-837e-13465db15b90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pvalues Lot_Frontage              7.021858e-06\n",
            "Lot_Area                  2.483740e-10\n",
            "Year_Built                5.506122e-05\n",
            "Year_Remod_Add            3.579071e-05\n",
            "Mas_Vnr_Area              2.808985e-07\n",
            "                              ...     \n",
            "Sale_Condition_AdjLand    2.902729e+00\n",
            "Sale_Condition_Alloca     9.076733e-01\n",
            "Sale_Condition_Family     1.054023e-01\n",
            "Sale_Condition_Normal     1.976220e-01\n",
            "Sale_Condition_Partial    1.124487e-01\n",
            "Length: 299, dtype: float64\n",
            "Worst Feature Lot_Area\n",
            "AIC_old -8630.848000367878\n",
            "AIC_new -8623.131287761094\n",
            "AIC_current_before_Break -8630.848000367878\n",
            "Num Removed 1\n",
            "Removed Features ['Lot_Area']\n",
            "X_current 299\n",
            "Training R-squared: 0.9347714813063736\n",
            "Adjusted R-squared (Training): 0.9236766761861106\n",
            "Training RMSE: 0.10561447472218422\n",
            "Warning: 'Sale_Price' column not found in test.csv. Unable to calculate RMSE.\n",
            "Final Features (Ridge): Index(['Lot_Frontage', 'Year_Built', 'Year_Remod_Add', 'Mas_Vnr_Area',\n",
            "       'BsmtFin_SF_1', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF',\n",
            "       'First_Flr_SF', 'Second_Flr_SF',\n",
            "       ...\n",
            "       'Sale_Type_ConLw', 'Sale_Type_New', 'Sale_Type_Oth', 'Sale_Type_VWD',\n",
            "       'Sale_Type_WD ', 'Sale_Condition_AdjLand', 'Sale_Condition_Alloca',\n",
            "       'Sale_Condition_Family', 'Sale_Condition_Normal',\n",
            "       'Sale_Condition_Partial'],\n",
            "      dtype='object', length=298)\n",
            "Number of Features in X_trn: 299\n",
            "Number of Features in final_features_trn: 298\n",
            "Removed Features: ['Lot_Area']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Helper functions\n",
        "def preprocess_data(X_trn):\n",
        "    # Initialize the StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit the scaler on the training data and transform it\n",
        "    X_trn_scaled = scaler.fit_transform(X_trn)\n",
        "\n",
        "    # Return the scaled training data as a DataFrame (to preserve column names) and the scaler object\n",
        "    X_trn_scaled_df = pd.DataFrame(X_trn_scaled, columns=X_trn.columns)\n",
        "\n",
        "    return X_trn_scaled_df, scaler\n",
        "\n",
        "def calc_rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def adjusted_r_squared(r2, n, p):\n",
        "    # Calculate adjusted R-squared\n",
        "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "\n",
        "def main():\n",
        "    # Load the train, test, and test_y datasets\n",
        "    train_file_path = 'fold8/train.csv'\n",
        "    test_file_path = 'fold8/test.csv'\n",
        "    test_y_file_path = 'fold8/test_y.csv'\n",
        "    housing_data_train = pd.read_csv(train_file_path)\n",
        "    housing_data_test = pd.read_csv(test_file_path)\n",
        "    test_y_data = pd.read_csv(test_y_file_path)  # Load the actual Sale Price for the test set\n",
        "\n",
        "    # Ensure that test_y_data and housing_data_test have matching PIDs\n",
        "    merged_test_data = pd.merge(housing_data_test, test_y_data, on='PID', how='inner')\n",
        "\n",
        "    # Identify categorical columns\n",
        "    categorical_columns = ['MS_SubClass', 'MS_Zoning', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities',\n",
        "                           'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type',\n",
        "                           'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style', 'Roof_Matl', 'Exterior_1st',\n",
        "                           'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual', 'Exter_Cond', 'Foundation', 'Bsmt_Qual',\n",
        "                           'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating', 'Heating_QC',\n",
        "                           'Central_Air', 'Electrical', 'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type',\n",
        "                           'Garage_Finish', 'Garage_Qual', 'Garage_Cond', 'Paved_Drive', 'Pool_QC', 'Fence',\n",
        "                           'Misc_Feature', 'Sale_Type', 'Sale_Condition']\n",
        "\n",
        "    # Perform dummy encoding on both train and test datasets\n",
        "    housing_data_encoded_train = pd.get_dummies(housing_data_train, columns=categorical_columns, drop_first=True)\n",
        "    housing_data_encoded_test = pd.get_dummies(merged_test_data, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "    # Align columns of train and test after dummy encoding:\n",
        "    all_columns = list(set(housing_data_encoded_train.columns) | set(housing_data_encoded_test.columns))\n",
        "    all_columns.remove('Sale_Price')  # Remove 'Sale_Price' if it's in all_columns\n",
        "\n",
        "    housing_data_encoded_train = housing_data_encoded_train.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    housing_data_encoded_test = housing_data_encoded_test.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    print(\"housing_data_encoded_train shape\", housing_data_encoded_train.shape)\n",
        "    print(\"housing_data_encoded_test shape\", housing_data_encoded_test.shape)\n",
        "\n",
        "    # Handle missing values by imputing them in train and test data\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    numeric_columns = housing_data_encoded_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numeric_columns = [col for col in numeric_columns if col != 'Sale_Price']\n",
        "\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_train.copy()\n",
        "    housing_data_encoded_imputed_train[numeric_columns] = imputer.fit_transform(housing_data_encoded_train[numeric_columns])\n",
        "\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_test.copy()\n",
        "    housing_data_encoded_imputed_test[numeric_columns] = imputer.transform(housing_data_encoded_test[numeric_columns])\n",
        "    print(\"housing_data_encoded_imputed_train shape\", housing_data_encoded_imputed_train.shape)\n",
        "    print(\"housing_data_encoded_imputed_test shape\", housing_data_encoded_imputed_test.shape)\n",
        "\n",
        "    # Remove 'PID' from train and test datasets\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_imputed_train.drop(columns=['PID'])\n",
        "    if 'PID' in housing_data_encoded_imputed_test.columns:\n",
        "        housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.drop(columns=['PID'])\n",
        "\n",
        "    # Separate predictors (X) and target variable (y) for training\n",
        "    X_trn = housing_data_encoded_imputed_train.drop(columns='Sale_Price')\n",
        "    y_trn = np.log(housing_data_encoded_imputed_train['Sale_Price'])\n",
        "\n",
        "    # Scale the training data using preprocess_data function (returns DataFrame with column names)\n",
        "    X_trn_scaled, scaler = preprocess_data(X_trn)\n",
        "\n",
        "    # 1. Use Lasso regression for feature selection\n",
        "    lasso_model = Lasso(alpha=0.01)  # Adjust the alpha value as necessary\n",
        "    lasso_model.fit(X_trn_scaled, y_trn)\n",
        "\n",
        "    # Select the features with non-zero coefficients\n",
        "    selected_features = X_trn_scaled.columns[lasso_model.coef_ != 0]\n",
        "    print(f\"Selected features after Lasso: {len(selected_features)} features\")\n",
        "\n",
        "    # Scale the test data using the same scaler\n",
        "    X_tst_scaled = pd.DataFrame(scaler.transform(housing_data_encoded_imputed_test.drop(columns=['Sale_Price'])), columns=housing_data_encoded_imputed_test.drop(columns=['Sale_Price']).columns)\n",
        "    print(\"X_tst_scaled shape\", X_tst_scaled.shape)\n",
        "\n",
        "\n",
        "\n",
        "    # 2. Grid Search for Ridge Regression to find the best alpha\n",
        "    param_grid = {'alpha': [0.01, 0.1, 1.0, 10, 100]}\n",
        "    ridge_model = Ridge()\n",
        "    grid_search = GridSearchCV(ridge_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "    grid_search.fit(X_trn_scaled[selected_features], y_trn)\n",
        "\n",
        "    # Get the best alpha value from grid search\n",
        "    best_alpha = grid_search.best_params_['alpha']\n",
        "    print(\"Best alpha for Ridge regression:\", best_alpha)\n",
        "\n",
        "    # 3. Train the final Ridge model with the best alpha\n",
        "    final_ridge_model = Ridge(alpha=best_alpha)\n",
        "    final_ridge_model.fit(X_trn_scaled[selected_features], y_trn)\n",
        "\n",
        "    # Predict on the training set\n",
        "    y_trn_pred = final_ridge_model.predict(X_trn_scaled[selected_features])\n",
        "\n",
        "    # Calculate R-squared and RMSE for training data\n",
        "    r2_train = r2_score(y_trn, y_trn_pred)\n",
        "    adj_r2_train = adjusted_r_squared(r2_train, len(y_trn), X_trn_scaled[selected_features].shape[1])\n",
        "    train_rmse = calc_rmse(y_trn, y_trn_pred)\n",
        "    print(f\"Training R-squared: {r2_train}\")\n",
        "    print(f\"Adjusted R-squared (Training): {adj_r2_train}\")\n",
        "    print(f\"Training RMSE: {train_rmse}\")\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_tst_pred_log = final_ridge_model.predict(X_tst_scaled[selected_features])\n",
        "\n",
        "    # Reverse the log-transformation to get predictions in the original scale\n",
        "    y_tst_pred = np.exp(y_tst_pred_log)\n",
        "\n",
        "    # Merge the predictions with actual sale prices from test_y.csv using 'PID'\n",
        "    predictions = pd.DataFrame({\n",
        "        'PID': merged_test_data['PID'],\n",
        "        'Predicted_Sale_Price': y_tst_pred\n",
        "    })\n",
        "    merged_test_data = pd.merge(test_y_data, predictions, on='PID', how='inner')\n",
        "\n",
        "    # Calculate RMSE and R-squared for the test set using actual sale prices\n",
        "    test_rmse = calc_rmse(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    r2_test = r2_score(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    adj_r2_test = adjusted_r_squared(r2_test, len(merged_test_data), X_tst_scaled.shape[1])\n",
        "    print(f\"Test RMSE: {test_rmse}\")\n",
        "    print(f\"Test R-squared: {r2_test}\")\n",
        "    print(f\"Adjusted R-squared (Test): {adj_r2_test}\")\n",
        "\n",
        "    # Save the predictions to a CSV file\n",
        "    predictions.to_csv('test_predictions.csv', index=False)\n",
        "    print(\"Predictions for test set saved to 'test_predictions.csv'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vY5UTvHkysz5",
        "outputId": "3b6fef97-7ca9-41ff-9440-ecc23d4528c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "housing_data_encoded_train shape (2051, 307)\n",
            "housing_data_encoded_test shape (879, 307)\n",
            "housing_data_encoded_imputed_train shape (2051, 307)\n",
            "housing_data_encoded_imputed_test shape (879, 307)\n",
            "Selected features after Lasso: 69 features\n",
            "X_tst_scaled shape (879, 305)\n",
            "Best alpha for Ridge regression: 100\n",
            "Training R-squared: 0.9167256386576015\n",
            "Adjusted R-squared (Training): 0.9138251182473918\n",
            "Training RMSE: 0.11933294006933579\n",
            "Test RMSE: 0.1316017987470567\n",
            "Test R-squared: 0.887759206256692\n",
            "Adjusted R-squared (Test): 0.8280149792205508\n",
            "Predictions for test set saved to 'test_predictions.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Helper function to calculate RMSE\n",
        "def calc_rmse(actual, predicted):\n",
        "    \"\"\"Calculate Root Mean Squared Error.\"\"\"\n",
        "    return np.sqrt(np.mean((actual - predicted) ** 2))\n",
        "\n",
        "# Helper function to calculate adjusted R-squared\n",
        "def adjusted_r_squared(r2, n, k):\n",
        "    \"\"\"Calculate adjusted R-squared.\"\"\"\n",
        "    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
        "\n",
        "# Placeholder for backward elimination with Ridge regression\n",
        "def backward_elimination_ridge_aic1(X, y, alpha=1.0):\n",
        "    model = Ridge(alpha=alpha)\n",
        "    X_current = X.copy()\n",
        "\n",
        "    # Assume backward elimination is based on Ridge with small coefficients removed\n",
        "    removed_features = []\n",
        "\n",
        "    model.fit(X_current, y)\n",
        "    print(\"removed_features\", removed_features)\n",
        "    return model, X_current, removed_features\n",
        "\n",
        "# Backward elimination based on AIC with Ridge regression\n",
        "def backward_elimination_ridge_aic2(X, y, alpha=1.0, threshold=1e-4):\n",
        "    \"\"\"\n",
        "    Perform backward elimination based on Ridge regression coefficients.\n",
        "\n",
        "    Parameters:\n",
        "    X : DataFrame\n",
        "        The feature matrix.\n",
        "    y : Series or array\n",
        "        The target variable.\n",
        "    alpha : float, optional (default=1.0)\n",
        "        Regularization strength for Ridge regression.\n",
        "    threshold : float, optional (default=1e-4)\n",
        "        Threshold for removing features with small coefficients.\n",
        "\n",
        "    Returns:\n",
        "    model : Ridge object\n",
        "        The final Ridge model after backward elimination.\n",
        "    X_current : DataFrame\n",
        "        The feature matrix after feature removal.\n",
        "    removed_features : list\n",
        "        The list of features that were removed.\n",
        "    \"\"\"\n",
        "    model = Ridge(alpha=alpha)\n",
        "    X_current = X.copy()\n",
        "    removed_features = []\n",
        "\n",
        "    while True:\n",
        "        model.fit(X_current, y)\n",
        "        coefs = np.abs(model.coef_)\n",
        "\n",
        "        # Find the index of the feature with the smallest coefficient\n",
        "        smallest_coef_idx = np.argmin(coefs)\n",
        "        smallest_coef = coefs[smallest_coef_idx]\n",
        "\n",
        "        # Check if the smallest coefficient is below the threshold\n",
        "        if smallest_coef < threshold:\n",
        "            # Remove the feature with the smallest coefficient\n",
        "            removed_feature = X_current.columns[smallest_coef_idx]\n",
        "            removed_features.append(removed_feature)\n",
        "            X_current = X_current.drop(columns=[removed_feature])\n",
        "\n",
        "            print(f\"Removed feature: {removed_feature} with coefficient: {smallest_coef}\")\n",
        "        else:\n",
        "            # Stop if no more features have coefficients below the threshold\n",
        "            break\n",
        "\n",
        "    return model, X_current, removed_features\n",
        "\n",
        "def main():\n",
        "    # Load the train and test datasets\n",
        "    train_file_path = 'fold8/train.csv'\n",
        "    test_file_path = 'fold8/test.csv'\n",
        "    housing_data_train = pd.read_csv(train_file_path)\n",
        "    housing_data_test = pd.read_csv(test_file_path)\n",
        "\n",
        "    # Identify categorical columns\n",
        "    categorical_columns = ['MS_SubClass', 'MS_Zoning', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities',\n",
        "                           'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type',\n",
        "                           'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style', 'Roof_Matl', 'Exterior_1st',\n",
        "                           'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual', 'Exter_Cond', 'Foundation', 'Bsmt_Qual',\n",
        "                           'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating', 'Heating_QC',\n",
        "                           'Central_Air', 'Electrical', 'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type',\n",
        "                           'Garage_Finish', 'Garage_Qual', 'Garage_Cond', 'Paved_Drive', 'Pool_QC', 'Fence',\n",
        "                           'Misc_Feature', 'Sale_Type', 'Sale_Condition']\n",
        "\n",
        "    # Perform dummy encoding on the train and test datasets\n",
        "    housing_data_encoded_train = pd.get_dummies(housing_data_train, columns=categorical_columns, drop_first=True)\n",
        "    housing_data_encoded_test = pd.get_dummies(housing_data_test, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "    # Handle missing values by imputing them in train and test data (median imputation) for numeric columns\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    numeric_columns = housing_data_encoded_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numeric_columns = [col for col in numeric_columns if col != 'Sale_Price']  # Exclude 'Sale_Price'\n",
        "\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_train.copy()\n",
        "    housing_data_encoded_imputed_train[numeric_columns] = imputer.fit_transform(housing_data_encoded_train[numeric_columns])\n",
        "\n",
        "    # For test data, apply the same imputation strategy\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_test.copy()\n",
        "    housing_data_encoded_imputed_test[numeric_columns] = imputer.transform(housing_data_encoded_test[numeric_columns])\n",
        "\n",
        "    # Remove 'PID' from train and test datasets\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_imputed_train.drop(columns=['PID'])\n",
        "    if 'PID' in housing_data_encoded_imputed_test.columns:\n",
        "        housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.drop(columns=['PID'])\n",
        "\n",
        "    # Align the columns of the test dataset to match the train dataset\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.reindex(columns=housing_data_encoded_imputed_train.columns.drop('Sale_Price'), fill_value=0)\n",
        "\n",
        "    # Separate predictors (X) and target variable (y) for training\n",
        "    X_trn = housing_data_encoded_imputed_train.drop(columns='Sale_Price')\n",
        "    y_trn = np.log(housing_data_encoded_imputed_train['Sale_Price'])\n",
        "\n",
        "    # Apply backward elimination using Ridge regression based on small coefficients\n",
        "    model_ridge, final_features_trn, removed_features = backward_elimination_ridge_aic2(X_trn, y_trn, alpha=1.0)\n",
        "    print(\"Final features after backward elimination:\", final_features_trn.columns)\n",
        "    print(\"Removed features:\", removed_features)\n",
        "    # Remove the same features from the test dataset\n",
        "    X_tst = housing_data_encoded_imputed_test.drop(columns=removed_features, errors='ignore')\n",
        "\n",
        "    # Predict on the training set\n",
        "    y_trn_pred = model_ridge.predict(final_features_trn)\n",
        "\n",
        "    # Calculate R-squared and adjusted R-squared for the training data\n",
        "    r2_train = r2_score(y_trn, y_trn_pred)\n",
        "    adj_r2_train = adjusted_r_squared(r2_train, len(y_trn), final_features_trn.shape[1])\n",
        "\n",
        "    # Calculate RMSE for the training data\n",
        "    train_rmse = calc_rmse(y_trn, y_trn_pred)\n",
        "\n",
        "    print(\"Training R-squared:\", r2_train)\n",
        "    print(\"Adjusted R-squared (Training):\", adj_r2_train)\n",
        "    print(\"Training RMSE:\", train_rmse)\n",
        "\n",
        "    # Predict on the test set using the trained Ridge model\n",
        "    y_tst_pred_log = model_ridge.predict(X_tst)\n",
        "\n",
        "    # Reverse the log-transformation to get predictions in the original scale\n",
        "    y_tst_pred = np.exp(y_tst_pred_log)\n",
        "\n",
        "    # Merge the predictions with PID for submission\n",
        "    predictions = pd.DataFrame({\n",
        "        'PID': housing_data_test['PID'],\n",
        "        'Predicted_Sale_Price': y_tst_pred\n",
        "    })\n",
        "\n",
        "    # Save the predictions to 'mysubmission1.txt'\n",
        "    predictions.to_csv('mysubmission1.txt', index=False)\n",
        "    print(\"Predictions for test set saved to 'mysubmission1.txt'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VZ66zbx_d_x",
        "outputId": "4ed02ec5-e1e0-444a-bb4f-c58a3d8a6722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed feature: Lot_Area with coefficient: 1.6815505795190869e-06\n",
            "Removed feature: Misc_Val with coefficient: 6.8529609526047715e-06\n",
            "Removed feature: Mo_Sold with coefficient: 1.267671178909046e-05\n",
            "Removed feature: BsmtFin_SF_2 with coefficient: 1.871063878142956e-05\n",
            "Removed feature: Bsmt_Unf_SF with coefficient: 2.5633833178674623e-05\n",
            "Removed feature: Total_Bsmt_SF with coefficient: 1.675603785119002e-05\n",
            "Removed feature: Low_Qual_Fin_SF with coefficient: 2.8836632907011108e-05\n",
            "Removed feature: Second_Flr_SF with coefficient: 8.764679551269944e-06\n",
            "Removed feature: Garage_Area with coefficient: 4.140640363089076e-05\n",
            "Removed feature: Mas_Vnr_Area with coefficient: 5.7845672158041756e-05\n",
            "Removed feature: Open_Porch_SF with coefficient: 6.368243331475089e-05\n",
            "Removed feature: Wood_Deck_SF with coefficient: 6.734616526796381e-05\n",
            "Removed feature: First_Flr_SF with coefficient: 6.646231253084586e-05\n",
            "Removed feature: Garage_Finish_RFn with coefficient: 8.5326032206746e-05\n",
            "Final features after backward elimination: Index(['Lot_Frontage', 'Year_Built', 'Year_Remod_Add', 'BsmtFin_SF_1',\n",
            "       'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath',\n",
            "       'Half_Bath', 'Bedroom_AbvGr',\n",
            "       ...\n",
            "       'Sale_Type_ConLw', 'Sale_Type_New', 'Sale_Type_Oth', 'Sale_Type_VWD',\n",
            "       'Sale_Type_WD ', 'Sale_Condition_AdjLand', 'Sale_Condition_Alloca',\n",
            "       'Sale_Condition_Family', 'Sale_Condition_Normal',\n",
            "       'Sale_Condition_Partial'],\n",
            "      dtype='object', length=285)\n",
            "Removed features: ['Lot_Area', 'Misc_Val', 'Mo_Sold', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Low_Qual_Fin_SF', 'Second_Flr_SF', 'Garage_Area', 'Mas_Vnr_Area', 'Open_Porch_SF', 'Wood_Deck_SF', 'First_Flr_SF', 'Garage_Finish_RFn']\n",
            "Training R-squared: 0.9335322988210455\n",
            "Adjusted R-squared (Training): 0.9227995538714693\n",
            "Training RMSE: 0.10661296331081088\n",
            "Predictions for test set saved to 'mysubmission1.txt'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Load the train, test, and test_y datasets\n",
        "    train_file_path = 'fold8/train.csv'\n",
        "    test_file_path = 'fold8/test.csv'\n",
        "    test_y_file_path = 'fold8/test_y.csv'\n",
        "    housing_data_train = pd.read_csv(train_file_path)\n",
        "    housing_data_test = pd.read_csv(test_file_path)\n",
        "    test_y_data = pd.read_csv(test_y_file_path)  # Load the actual Sale Price for the test set\n",
        "\n",
        "    # Identify categorical columns\n",
        "    categorical_columns = ['MS_SubClass', 'MS_Zoning', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities',\n",
        "                           'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type',\n",
        "                           'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style', 'Roof_Matl', 'Exterior_1st',\n",
        "                           'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual', 'Exter_Cond', 'Foundation', 'Bsmt_Qual',\n",
        "                           'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating', 'Heating_QC',\n",
        "                           'Central_Air', 'Electrical', 'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type',\n",
        "                           'Garage_Finish', 'Garage_Qual', 'Garage_Cond', 'Paved_Drive', 'Pool_QC', 'Fence',\n",
        "                           'Misc_Feature', 'Sale_Type', 'Sale_Condition']\n",
        "\n",
        "    # Perform dummy encoding on the train and test datasets\n",
        "    housing_data_encoded_train = pd.get_dummies(housing_data_train, columns=categorical_columns, drop_first=True)\n",
        "    housing_data_encoded_test = pd.get_dummies(housing_data_test, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "    # Handle missing values by imputing them in train and test data (median imputation) for numeric columns\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    numeric_columns = housing_data_encoded_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numeric_columns = [col for col in numeric_columns if col != 'Sale_Price']  # Exclude 'Sale_Price'\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_train.copy()\n",
        "    housing_data_encoded_imputed_train[numeric_columns] = imputer.fit_transform(housing_data_encoded_train[numeric_columns])\n",
        "\n",
        "    # For test data, apply the same imputation strategy\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_test.copy()\n",
        "    housing_data_encoded_imputed_test[numeric_columns] = imputer.transform(housing_data_encoded_test[numeric_columns])\n",
        "\n",
        "    # Remove 'PID' from train and test datasets\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_imputed_train.drop(columns=['PID'])\n",
        "    if 'PID' in housing_data_encoded_imputed_test.columns:\n",
        "        housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.drop(columns=['PID'])\n",
        "\n",
        "    # Align the columns of the test dataset to match the train dataset\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.reindex(columns=housing_data_encoded_imputed_train.columns.drop('Sale_Price'), fill_value=0)\n",
        "\n",
        "    # Separate predictors (X) and target variable (y) for training\n",
        "    X_trn = housing_data_encoded_imputed_train.drop(columns='Sale_Price')\n",
        "    y_trn = np.log(housing_data_encoded_imputed_train['Sale_Price'])\n",
        "    print(\"X_trn shape\", X_trn.shape)\n",
        "    # Apply backward elimination using Ridge regression based on small coefficients\n",
        "    model_ridge, final_features_trn, removed_features = backward_elimination_ridge_aic2(X_trn, y_trn, alpha=1.0)\n",
        "\n",
        "    # Remove the same features from the test dataset\n",
        "    X_tst = housing_data_encoded_imputed_test.drop(columns=removed_features, errors='ignore')\n",
        "    print(\"X_tst shape\", X_tst.shape)\n",
        "    # Predict on the training set\n",
        "    y_trn_pred = model_ridge.predict(final_features_trn)\n",
        "    print(\"y_trn_pred shape\",y_trn_pred.shape)\n",
        "    # Calculate R-squared and adjusted R-squared for the training data\n",
        "    r2_train = r2_score(y_trn, y_trn_pred)\n",
        "    adj_r2_train = adjusted_r_squared(r2_train, len(y_trn), final_features_trn.shape[1])\n",
        "\n",
        "    # Calculate RMSE for the training data\n",
        "    train_rmse = calc_rmse(y_trn, y_trn_pred)\n",
        "\n",
        "    print(\"Training R-squared:\", r2_train)\n",
        "    print(\"Adjusted R-squared (Training):\", adj_r2_train)\n",
        "    print(\"Training RMSE:\", train_rmse)\n",
        "\n",
        "    # Predict on the test set using the trained Ridge model\n",
        "    y_tst_pred_log = model_ridge.predict(X_tst)\n",
        "\n",
        "    # Reverse the log-transformation to get predictions in the original scale\n",
        "    y_tst_pred = np.exp(y_tst_pred_log)\n",
        "    print(\"y_tst_pred shape\", y_tst_pred.shape)\n",
        "    # Merge the predictions with actual values from test_y.csv using 'PID'\n",
        "    predictions = pd.DataFrame({\n",
        "        'PID': housing_data_test['PID'],\n",
        "        'Predicted_Sale_Price': y_tst_pred\n",
        "    })\n",
        "    merged_test_data = pd.merge(test_y_data, predictions, on='PID', how='inner')\n",
        "\n",
        "    # Calculate RMSE and R-squared for the test set\n",
        "    test_rmse = calc_rmse(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    r2_test = r2_score(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    adj_r2_test = adjusted_r_squared(r2_test, len(merged_test_data), X_tst.shape[1])\n",
        "\n",
        "    print(\"Test RMSE:\", test_rmse)\n",
        "    print(\"Test R-squared:\", r2_test)\n",
        "    print(\"Adjusted R-squared (Test):\", adj_r2_test)\n",
        "\n",
        "    # Save the predictions to a CSV file (optional)\n",
        "    predictions.to_csv('test_predictions.csv', index=False)\n",
        "    print(\"Predictions for test set saved to 'test_predictions.csv'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYwgA5Sms1kW",
        "outputId": "b3538e94-406d-43fb-f1e7-1b9d5a7356bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_trn shape (2051, 299)\n",
            "Removed feature: Lot_Area with coefficient: 1.6815505795190869e-06\n",
            "Removed feature: Misc_Val with coefficient: 6.8529609526047715e-06\n",
            "Removed feature: Mo_Sold with coefficient: 1.267671178909046e-05\n",
            "Removed feature: BsmtFin_SF_2 with coefficient: 1.871063878142956e-05\n",
            "Removed feature: Bsmt_Unf_SF with coefficient: 2.5633833178674623e-05\n",
            "Removed feature: Total_Bsmt_SF with coefficient: 1.675603785119002e-05\n",
            "Removed feature: Low_Qual_Fin_SF with coefficient: 2.8836632907011108e-05\n",
            "Removed feature: Second_Flr_SF with coefficient: 8.764679551269944e-06\n",
            "Removed feature: Garage_Area with coefficient: 4.140640363089076e-05\n",
            "Removed feature: Mas_Vnr_Area with coefficient: 5.7845672158041756e-05\n",
            "Removed feature: Open_Porch_SF with coefficient: 6.368243331475089e-05\n",
            "Removed feature: Wood_Deck_SF with coefficient: 6.734616526796381e-05\n",
            "Removed feature: First_Flr_SF with coefficient: 6.646231253084586e-05\n",
            "Removed feature: Garage_Finish_RFn with coefficient: 8.5326032206746e-05\n",
            "X_tst shape (879, 285)\n",
            "y_trn_pred shape (2051,)\n",
            "Training R-squared: 0.9335322988210455\n",
            "Adjusted R-squared (Training): 0.9227995538714693\n",
            "Training RMSE: 0.10661296331081088\n",
            "y_tst_pred shape (879,)\n",
            "Test RMSE: 0.23217566165992307\n",
            "Test R-squared: 0.6506502293778396\n",
            "Adjusted R-squared (Test): 0.4827502553014218\n",
            "Predictions for test set saved to 'test_predictions.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def preprocess_data(X_trn):\n",
        "    # Initialize the StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit the scaler on the training data and transform it\n",
        "    X_trn_scaled = scaler.fit_transform(X_trn)\n",
        "\n",
        "    # Return the scaled training data and the scaler object\n",
        "    return X_trn_scaled, scaler\n"
      ],
      "metadata": {
        "id": "jqodiRK1J_8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Load the train, test, and test_y datasets\n",
        "    train_file_path = 'fold8/train.csv'\n",
        "    test_file_path = 'fold8/test.csv'\n",
        "    test_y_file_path = 'fold8/test_y.csv'\n",
        "    housing_data_train = pd.read_csv(train_file_path)\n",
        "    housing_data_test = pd.read_csv(test_file_path)\n",
        "    test_y_data = pd.read_csv(test_y_file_path)  # Load the actual Sale Price for the test set\n",
        "\n",
        "    # Ensure that test_y_data and housing_data_test have matching PIDs\n",
        "    # This step aligns test data with only those entries in test_y_data\n",
        "    merged_test_data = pd.merge(housing_data_test, test_y_data, on='PID', how='inner')\n",
        "\n",
        "    # Identify categorical columns\n",
        "    categorical_columns = ['MS_SubClass', 'MS_Zoning', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities',\n",
        "                           'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type',\n",
        "                           'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style', 'Roof_Matl', 'Exterior_1st',\n",
        "                           'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual', 'Exter_Cond', 'Foundation', 'Bsmt_Qual',\n",
        "                           'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating', 'Heating_QC',\n",
        "                           'Central_Air', 'Electrical', 'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type',\n",
        "                           'Garage_Finish', 'Garage_Qual', 'Garage_Cond', 'Paved_Drive', 'Pool_QC', 'Fence',\n",
        "                           'Misc_Feature', 'Sale_Type', 'Sale_Condition']\n",
        "\n",
        "    # Perform dummy encoding on both train and test datasets\n",
        "    housing_data_encoded_train = pd.get_dummies(housing_data_train, columns=categorical_columns, drop_first=True)\n",
        "    housing_data_encoded_test = pd.get_dummies(merged_test_data, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "    # Handle missing values by imputing them in train and test data (median imputation) for numeric columns\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    numeric_columns = housing_data_encoded_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numeric_columns = [col for col in numeric_columns if col != 'Sale_Price']  # Exclude 'Sale_Price'\n",
        "\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_train.copy()\n",
        "    housing_data_encoded_imputed_train[numeric_columns] = imputer.fit_transform(housing_data_encoded_train[numeric_columns])\n",
        "\n",
        "    # For test data, apply the same imputation strategy\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_test.copy()\n",
        "    housing_data_encoded_imputed_test[numeric_columns] = imputer.transform(housing_data_encoded_test[numeric_columns])\n",
        "\n",
        "    # Remove 'PID' from train and test datasets\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_imputed_train.drop(columns=['PID'])\n",
        "    if 'PID' in housing_data_encoded_imputed_test.columns:\n",
        "        housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.drop(columns=['PID'])\n",
        "\n",
        "    # Align the columns of the test dataset to match the train dataset\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.reindex(columns=housing_data_encoded_imputed_train.columns.drop('Sale_Price'), fill_value=0)\n",
        "\n",
        "    # Separate predictors (X) and target variable (y) for training\n",
        "    X_trn = housing_data_encoded_imputed_train.drop(columns='Sale_Price')\n",
        "    y_trn = np.log(housing_data_encoded_imputed_train['Sale_Price'])\n",
        "\n",
        "    # Apply backward elimination using Ridge regression based on small coefficients\n",
        "    model_ridge, final_features_trn, removed_features = backward_elimination_ridge_aic2(X_trn, y_trn, alpha=1.0)\n",
        "    print(\"Final features after backward elimination:\", final_features_trn.columns)\n",
        "    print(\"Removed features:\", removed_features)\n",
        "\n",
        "    # Remove the same features from the test dataset\n",
        "    X_tst = housing_data_encoded_imputed_test.drop(columns=removed_features, errors='ignore')\n",
        "    print(\"X_tst shape\", X_tst.shape)\n",
        "\n",
        "    # Predict on the training set\n",
        "    y_trn_pred = model_ridge.predict(final_features_trn)\n",
        "    print(\"y_trn_pred shape\", y_trn_pred.shape)\n",
        "\n",
        "    # Calculate R-squared and adjusted R-squared for the training data\n",
        "    r2_train = r2_score(y_trn, y_trn_pred)\n",
        "    adj_r2_train = adjusted_r_squared(r2_train, len(y_trn), final_features_trn.shape[1])\n",
        "\n",
        "    # Calculate RMSE for the training data\n",
        "    train_rmse = calc_rmse(y_trn, y_trn_pred)\n",
        "\n",
        "    print(\"Training R-squared:\", r2_train)\n",
        "    print(\"Adjusted R-squared (Training):\", adj_r2_train)\n",
        "    print(\"Training RMSE:\", train_rmse)\n",
        "\n",
        "    # Predict on the test set using the trained Ridge model\n",
        "    y_tst_pred_log = model_ridge.predict(X_tst)\n",
        "\n",
        "    # Reverse the log-transformation to get predictions in the original scale\n",
        "    y_tst_pred = np.exp(y_tst_pred_log)\n",
        "    print(\"y_tst_pred shape\", y_tst_pred.shape)\n",
        "\n",
        "    # Merge the predictions with actual sale prices from test_y.csv using 'PID'\n",
        "    predictions = pd.DataFrame({\n",
        "        'PID': merged_test_data['PID'],\n",
        "        'Predicted_Sale_Price': y_tst_pred\n",
        "    })\n",
        "    merged_test_data = pd.merge(test_y_data, predictions, on='PID', how='inner')\n",
        "\n",
        "    # Calculate RMSE and R-squared for the test set using actual sale prices\n",
        "    test_rmse = calc_rmse(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    r2_test = r2_score(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    adj_r2_test = adjusted_r_squared(r2_test, len(merged_test_data), X_tst.shape[1])\n",
        "\n",
        "    print(\"Test RMSE:\", test_rmse)\n",
        "    print(\"Test R-squared:\", r2_test)\n",
        "    print(\"Adjusted R-squared (Test):\", adj_r2_test)\n",
        "\n",
        "    # Save the predictions to a CSV file\n",
        "    predictions.to_csv('test_predictions.csv', index=False)\n",
        "    print(\"Predictions for test set saved to 'test_predictions.csv'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PH4kSN0oE_CP",
        "outputId": "e51b0532-c4be-4894-ff4b-9cff58220fb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed feature: Lot_Area with coefficient: 1.6815505795190869e-06\n",
            "Removed feature: Misc_Val with coefficient: 6.8529609526047715e-06\n",
            "Removed feature: Mo_Sold with coefficient: 1.267671178909046e-05\n",
            "Removed feature: BsmtFin_SF_2 with coefficient: 1.871063878142956e-05\n",
            "Removed feature: Bsmt_Unf_SF with coefficient: 2.5633833178674623e-05\n",
            "Removed feature: Total_Bsmt_SF with coefficient: 1.675603785119002e-05\n",
            "Removed feature: Low_Qual_Fin_SF with coefficient: 2.8836632907011108e-05\n",
            "Removed feature: Second_Flr_SF with coefficient: 8.764679551269944e-06\n",
            "Removed feature: Garage_Area with coefficient: 4.140640363089076e-05\n",
            "Removed feature: Mas_Vnr_Area with coefficient: 5.7845672158041756e-05\n",
            "Removed feature: Open_Porch_SF with coefficient: 6.368243331475089e-05\n",
            "Removed feature: Wood_Deck_SF with coefficient: 6.734616526796381e-05\n",
            "Removed feature: First_Flr_SF with coefficient: 6.646231253084586e-05\n",
            "Removed feature: Garage_Finish_RFn with coefficient: 8.5326032206746e-05\n",
            "Final features after backward elimination: Index(['Lot_Frontage', 'Year_Built', 'Year_Remod_Add', 'BsmtFin_SF_1',\n",
            "       'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath',\n",
            "       'Half_Bath', 'Bedroom_AbvGr',\n",
            "       ...\n",
            "       'Sale_Type_ConLw', 'Sale_Type_New', 'Sale_Type_Oth', 'Sale_Type_VWD',\n",
            "       'Sale_Type_WD ', 'Sale_Condition_AdjLand', 'Sale_Condition_Alloca',\n",
            "       'Sale_Condition_Family', 'Sale_Condition_Normal',\n",
            "       'Sale_Condition_Partial'],\n",
            "      dtype='object', length=285)\n",
            "Removed features: ['Lot_Area', 'Misc_Val', 'Mo_Sold', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Low_Qual_Fin_SF', 'Second_Flr_SF', 'Garage_Area', 'Mas_Vnr_Area', 'Open_Porch_SF', 'Wood_Deck_SF', 'First_Flr_SF', 'Garage_Finish_RFn']\n",
            "X_tst shape (879, 285)\n",
            "y_trn_pred shape (2051,)\n",
            "Training R-squared: 0.9335322988210455\n",
            "Adjusted R-squared (Training): 0.9227995538714693\n",
            "Training RMSE: 0.10661296331081088\n",
            "y_tst_pred shape (879,)\n",
            "Test RMSE: 0.23217566165992307\n",
            "Test R-squared: 0.6506502293778396\n",
            "Adjusted R-squared (Test): 0.4827502553014218\n",
            "Predictions for test set saved to 'test_predictions.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def preprocess_data(X_trn):\n",
        "    # Initialize the StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit the scaler on the training data and transform it\n",
        "    X_trn_scaled = scaler.fit_transform(X_trn)\n",
        "\n",
        "    # Return the scaled training data as a DataFrame (to preserve column names) and the scaler object\n",
        "    X_trn_scaled_df = pd.DataFrame(X_trn_scaled, columns=X_trn.columns)\n",
        "\n",
        "    return X_trn_scaled_df, scaler\n",
        "\n",
        "def main():\n",
        "    # Load the train, test, and test_y datasets\n",
        "    train_file_path = 'fold8/train.csv'\n",
        "    test_file_path = 'fold8/test.csv'\n",
        "    test_y_file_path = 'fold8/test_y.csv'\n",
        "    housing_data_train = pd.read_csv(train_file_path)\n",
        "    housing_data_test = pd.read_csv(test_file_path)\n",
        "    test_y_data = pd.read_csv(test_y_file_path)  # Load the actual Sale Price for the test set\n",
        "\n",
        "    # Ensure that test_y_data and housing_data_test have matching PIDs\n",
        "    merged_test_data = pd.merge(housing_data_test, test_y_data, on='PID', how='inner')\n",
        "\n",
        "    # Identify categorical columns\n",
        "    categorical_columns = ['MS_SubClass', 'MS_Zoning', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities',\n",
        "                           'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type',\n",
        "                           'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style', 'Roof_Matl', 'Exterior_1st',\n",
        "                           'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual', 'Exter_Cond', 'Foundation', 'Bsmt_Qual',\n",
        "                           'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating', 'Heating_QC',\n",
        "                           'Central_Air', 'Electrical', 'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type',\n",
        "                           'Garage_Finish', 'Garage_Qual', 'Garage_Cond', 'Paved_Drive', 'Pool_QC', 'Fence',\n",
        "                           'Misc_Feature', 'Sale_Type', 'Sale_Condition']\n",
        "\n",
        "    # Perform dummy encoding on both train and test datasets\n",
        "    housing_data_encoded_train = pd.get_dummies(housing_data_train, columns=categorical_columns, drop_first=True)\n",
        "    housing_data_encoded_test = pd.get_dummies(merged_test_data, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "    # Align columns of train and test after dummy encoding:\n",
        "    # Get the union of all columns from train and test\n",
        "    all_columns = list(set(housing_data_encoded_train.columns) | set(housing_data_encoded_test.columns))\n",
        "    all_columns.remove('Sale_Price')  # Remove 'Sale_Price' if it's in all_columns\n",
        "\n",
        "    # Reindex both DataFrames to include all columns, filling missing values with 0\n",
        "    housing_data_encoded_train = housing_data_encoded_train.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    housing_data_encoded_test = housing_data_encoded_test.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    print(\"housing_data_encoded_train shape\", housing_data_encoded_train.shape)\n",
        "    print(\"housing_data_encoded_test shape\", housing_data_encoded_test.shape)\n",
        "    # Handle missing values by imputing them in train and test data (median imputation) for numeric columns\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    numeric_columns = housing_data_encoded_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numeric_columns = [col for col in numeric_columns if col != 'Sale_Price']  # Exclude 'Sale_Price'\n",
        "\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_train.copy()\n",
        "    housing_data_encoded_imputed_train[numeric_columns] = imputer.fit_transform(housing_data_encoded_train[numeric_columns])\n",
        "\n",
        "    # For test data, apply the same imputation strategy\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_test.copy()\n",
        "    housing_data_encoded_imputed_test[numeric_columns] = imputer.transform(housing_data_encoded_test[numeric_columns])\n",
        "    print(\"housing_data_encoded_imputed_train shape\", housing_data_encoded_imputed_train.shape)\n",
        "    print(\"housing_data_encoded_imputed_test shape\", housing_data_encoded_imputed_test.shape)\n",
        "    # Remove 'PID' from train and test datasets\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_imputed_train.drop(columns=['PID'])\n",
        "    if 'PID' in housing_data_encoded_imputed_test.columns:\n",
        "        housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.drop(columns=['PID'])\n",
        "\n",
        "    # Separate predictors (X) and target variable (y) for training\n",
        "    X_trn = housing_data_encoded_imputed_train.drop(columns='Sale_Price')\n",
        "    y_trn = np.log(housing_data_encoded_imputed_train['Sale_Price'])\n",
        "\n",
        "    # Scale the training data using preprocess_data function (returns DataFrame with column names)\n",
        "    X_trn_scaled, scaler = preprocess_data(X_trn)\n",
        "\n",
        "    # Apply backward elimination using Ridge regression based on small coefficients\n",
        "    model_ridge, final_features_trn, removed_features = backward_elimination_ridge_aic2(X_trn_scaled, y_trn, alpha=1.0)\n",
        "    print(\"Final features after backward elimination:\", final_features_trn.columns)\n",
        "    print(\"Removed features:\", removed_features)\n",
        "\n",
        "    # Remove the same features from the test dataset after aligning columns\n",
        "    housing_data_encoded_imputed_test_scaled = housing_data_encoded_imputed_test.drop(columns='Sale_Price')\n",
        "    print(\"housing_data_encoded_imputed_test_scaled shape\", housing_data_encoded_imputed_test_scaled.shape)\n",
        "    X_tst = housing_data_encoded_imputed_test.drop(columns=removed_features + ['Sale_Price'], errors='ignore')\n",
        "    print(\"X_tst shape\", X_tst.shape)\n",
        "    # Scale the test data using the scaler fitted on the training data\n",
        "    X_tst_scaled = pd.DataFrame(scaler.transform(housing_data_encoded_imputed_test_scaled), columns=housing_data_encoded_imputed_test_scaled.columns)\n",
        "    print(\"X_tst_scaled shape\", X_tst_scaled.shape)\n",
        "    X_tst_scaled_removed_features = X_tst_scaled.drop(columns=removed_features)\n",
        "    print(\"X_tst_scaled_removed_features shape\", X_tst_scaled_removed_features.shape)\n",
        "    # Predict on the training set\n",
        "    y_trn_pred = model_ridge.predict(final_features_trn)\n",
        "    print(\"y_trn_pred shape\", y_trn_pred.shape)\n",
        "\n",
        "    # Calculate R-squared and adjusted R-squared for the training data\n",
        "    r2_train = r2_score(y_trn, y_trn_pred)\n",
        "    adj_r2_train = adjusted_r_squared(r2_train, len(y_trn), final_features_trn.shape[1])\n",
        "\n",
        "    # Calculate RMSE for the training data\n",
        "    train_rmse = calc_rmse(y_trn, y_trn_pred)\n",
        "\n",
        "    print(\"Training R-squared:\", r2_train)\n",
        "    print(\"Adjusted R-squared (Training):\", adj_r2_train)\n",
        "    print(\"Training RMSE:\", train_rmse)\n",
        "\n",
        "    # Predict on the test set using the trained Ridge model\n",
        "    y_tst_pred_log = model_ridge.predict(X_tst_scaled_removed_features)\n",
        "\n",
        "    # Reverse the log-transformation to get predictions in the original scale\n",
        "    y_tst_pred = np.exp(y_tst_pred_log)\n",
        "    print(\"y_tst_pred shape\", y_tst_pred.shape)\n",
        "\n",
        "    # Merge the predictions with actual sale prices from test_y.csv using 'PID'\n",
        "    predictions = pd.DataFrame({\n",
        "        'PID': merged_test_data['PID'],\n",
        "        'Predicted_Sale_Price': y_tst_pred\n",
        "    })\n",
        "    merged_test_data = pd.merge(test_y_data, predictions, on='PID', how='inner')\n",
        "\n",
        "    # Calculate RMSE and R-squared for the test set using actual sale prices\n",
        "    test_rmse = calc_rmse(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    r2_test = r2_score(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    adj_r2_test = adjusted_r_squared(r2_test, len(merged_test_data), X_tst_scaled.shape[1])\n",
        "\n",
        "    print(\"Test RMSE:\", test_rmse)\n",
        "    print(\"Test R-squared:\", r2_test)\n",
        "    print(\"Adjusted R-squared (Test):\", adj_r2_test)\n",
        "\n",
        "    # Save the predictions to a CSV file\n",
        "    predictions.to_csv('test_predictions.csv', index=False)\n",
        "    print(\"Predictions for test set saved to 'test_predictions.csv'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kISEw09OZfZu",
        "outputId": "0cb847d0-a005-4d11-fec9-35701bd087cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "housing_data_encoded_train shape (2051, 307)\n",
            "housing_data_encoded_test shape (879, 307)\n",
            "housing_data_encoded_imputed_train shape (2051, 307)\n",
            "housing_data_encoded_imputed_test shape (879, 307)\n",
            "Removed feature: Misc_Feature_Gar2 with coefficient: 0.0\n",
            "Removed feature: Functional_Sev with coefficient: 0.0\n",
            "Removed feature: Condition_2_RRAe with coefficient: 0.0\n",
            "Removed feature: Exter_Cond_Poor with coefficient: 0.0\n",
            "Removed feature: Utilities_NoSewr with coefficient: 0.0\n",
            "Removed feature: Roof_Matl_Metal with coefficient: 0.0\n",
            "Removed feature: Electrical_FuseP with coefficient: 1.9235755174853126e-05\n",
            "Removed feature: House_Style_One_and_Half_Unf with coefficient: 7.672548145606089e-05\n",
            "Final features after backward elimination: Index(['Overall_Qual_Very_Poor', 'Sale_Condition_Family',\n",
            "       'Garage_Type_CarPort', 'Exterior_1st_Plywood', 'Overall_Cond_Excellent',\n",
            "       'Heating_QC_Fair', 'Neighborhood_Northridge_Heights',\n",
            "       'Overall_Qual_Poor', 'Pool_QC_Fair', 'BsmtFin_Type_2_BLQ',\n",
            "       ...\n",
            "       'Exterior_2nd_BrkFace', 'Exterior_1st_Stucco',\n",
            "       'House_Style_Two_and_Half_Fin', 'Exterior_2nd_Plywood', 'Lot_Area',\n",
            "       'Mas_Vnr_Type_CBlock', 'Neighborhood_College_Creek',\n",
            "       'Exterior_2nd_Stone', 'Bsmt_Cond_Fair', 'Electrical_FuseF'],\n",
            "      dtype='object', length=297)\n",
            "Removed features: ['Misc_Feature_Gar2', 'Functional_Sev', 'Condition_2_RRAe', 'Exter_Cond_Poor', 'Utilities_NoSewr', 'Roof_Matl_Metal', 'Electrical_FuseP', 'House_Style_One_and_Half_Unf']\n",
            "housing_data_encoded_imputed_test_scaled shape (879, 305)\n",
            "X_tst shape (879, 297)\n",
            "X_tst_scaled shape (879, 305)\n",
            "X_tst_scaled_removed_features shape (879, 297)\n",
            "y_trn_pred shape (2051,)\n",
            "Training R-squared: 0.9443669786584533\n",
            "Adjusted R-squared (Training): 0.9349414182828462\n",
            "Training RMSE: 0.0975373627035905\n",
            "y_tst_pred shape (879,)\n",
            "Test RMSE: 2.546572282456613\n",
            "Test R-squared: -41.02803587131323\n",
            "Adjusted R-squared (Test): -63.39897992148869\n",
            "Predictions for test set saved to 'test_predictions.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def preprocess_data(X_trn):\n",
        "    # Initialize the StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit the scaler on the training data and transform it\n",
        "    X_trn_scaled = scaler.fit_transform(X_trn)\n",
        "\n",
        "    # Return the scaled training data as a DataFrame (to preserve column names) and the scaler object\n",
        "    X_trn_scaled_df = pd.DataFrame(X_trn_scaled, columns=X_trn.columns)\n",
        "\n",
        "    return X_trn_scaled_df, scaler\n",
        "\n",
        "def main():\n",
        "    # Load the train, test, and test_y datasets\n",
        "    train_file_path = 'fold8/train.csv'\n",
        "    test_file_path = 'fold8/test.csv'\n",
        "    test_y_file_path = 'fold8/test_y.csv'\n",
        "    housing_data_train = pd.read_csv(train_file_path)\n",
        "    housing_data_test = pd.read_csv(test_file_path)\n",
        "    test_y_data = pd.read_csv(test_y_file_path)  # Load the actual Sale Price for the test set\n",
        "\n",
        "    # Ensure that test_y_data and housing_data_test have matching PIDs\n",
        "    merged_test_data = pd.merge(housing_data_test, test_y_data, on='PID', how='inner')\n",
        "\n",
        "    # Identify categorical columns\n",
        "    categorical_columns = ['MS_SubClass', 'MS_Zoning', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities',\n",
        "                           'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type',\n",
        "                           'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style', 'Roof_Matl', 'Exterior_1st',\n",
        "                           'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual', 'Exter_Cond', 'Foundation', 'Bsmt_Qual',\n",
        "                           'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating', 'Heating_QC',\n",
        "                           'Central_Air', 'Electrical', 'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type',\n",
        "                           'Garage_Finish', 'Garage_Qual', 'Garage_Cond', 'Paved_Drive', 'Pool_QC', 'Fence',\n",
        "                           'Misc_Feature', 'Sale_Type', 'Sale_Condition']\n",
        "\n",
        "    # Perform dummy encoding on both train and test datasets\n",
        "    housing_data_encoded_train = pd.get_dummies(housing_data_train, columns=categorical_columns, drop_first=True)\n",
        "    housing_data_encoded_test = pd.get_dummies(merged_test_data, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "    # Align columns of train and test after dummy encoding:\n",
        "    all_columns = list(set(housing_data_encoded_train.columns) | set(housing_data_encoded_test.columns))\n",
        "    all_columns.remove('Sale_Price')  # Remove 'Sale_Price' if it's in all_columns\n",
        "\n",
        "    housing_data_encoded_train = housing_data_encoded_train.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    housing_data_encoded_test = housing_data_encoded_test.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    print(\"housing_data_encoded_train shape\", housing_data_encoded_train.shape)\n",
        "    print(\"housing_data_encoded_test shape\", housing_data_encoded_test.shape)\n",
        "\n",
        "    # Handle missing values by imputing them in train and test data\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    numeric_columns = housing_data_encoded_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numeric_columns = [col for col in numeric_columns if col != 'Sale_Price']\n",
        "\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_train.copy()\n",
        "    housing_data_encoded_imputed_train[numeric_columns] = imputer.fit_transform(housing_data_encoded_train[numeric_columns])\n",
        "\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_test.copy()\n",
        "    housing_data_encoded_imputed_test[numeric_columns] = imputer.transform(housing_data_encoded_test[numeric_columns])\n",
        "    print(\"housing_data_encoded_imputed_train shape\", housing_data_encoded_imputed_train.shape)\n",
        "    print(\"housing_data_encoded_imputed_test shape\", housing_data_encoded_imputed_test.shape)\n",
        "\n",
        "    # Remove 'PID' from train and test datasets\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_imputed_train.drop(columns=['PID'])\n",
        "    if 'PID' in housing_data_encoded_imputed_test.columns:\n",
        "        housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.drop(columns=['PID'])\n",
        "\n",
        "    # Separate predictors (X) and target variable (y) for training\n",
        "    X_trn = housing_data_encoded_imputed_train.drop(columns='Sale_Price')\n",
        "    y_trn = np.log(housing_data_encoded_imputed_train['Sale_Price'])\n",
        "\n",
        "    # Scale the training data using preprocess_data function (returns DataFrame with column names)\n",
        "    X_trn_scaled, scaler = preprocess_data(X_trn)\n",
        "\n",
        "    # Apply backward elimination using Ridge regression based on small coefficients\n",
        "    model_ridge, final_features_trn, removed_features = backward_elimination_ridge_aic1(X_trn_scaled, y_trn, alpha=1.0)\n",
        "    print(\"Final features after backward elimination:\", final_features_trn.columns)\n",
        "    print(\"Removed features:\", removed_features)\n",
        "\n",
        "    # Remove the same features from the test dataset after aligning columns\n",
        "    X_tst_final = housing_data_encoded_imputed_test.drop(columns=removed_features + ['Sale_Price'], errors='ignore')\n",
        "    print(\"X_tst_final shape\", X_tst_final.shape)\n",
        "\n",
        "    # Scale the test data using the scaler fitted on the training data\n",
        "    X_tst_scaled = pd.DataFrame(scaler.transform(X_tst_final), columns=X_tst_final.columns)\n",
        "    print(\"X_tst_scaled shape\", X_tst_scaled.shape)\n",
        "\n",
        "    # Train the model on the scaled training data\n",
        "    y_trn_pred = model_ridge.predict(final_features_trn)\n",
        "    print(\"y_trn_pred shape\", y_trn_pred.shape)\n",
        "\n",
        "    # Calculate R-squared and adjusted R-squared for the training data\n",
        "    r2_train = r2_score(y_trn, y_trn_pred)\n",
        "    adj_r2_train = adjusted_r_squared(r2_train, len(y_trn), final_features_trn.shape[1])\n",
        "\n",
        "    # Calculate RMSE for the training data\n",
        "    train_rmse = calc_rmse(y_trn, y_trn_pred)\n",
        "\n",
        "    print(\"Training R-squared:\", r2_train)\n",
        "    print(\"Adjusted R-squared (Training):\", adj_r2_train)\n",
        "    print(\"Training RMSE:\", train_rmse)\n",
        "\n",
        "    # Predict on the test set using the trained Ridge model\n",
        "    y_tst_pred_log = model_ridge.predict(X_tst_scaled)\n",
        "\n",
        "    # Reverse the log-transformation to get predictions in the original scale\n",
        "    y_tst_pred = np.exp(y_tst_pred_log)\n",
        "    print(\"y_tst_pred shape\", y_tst_pred.shape)\n",
        "\n",
        "    # Merge the predictions with actual sale prices from test_y.csv using 'PID'\n",
        "    predictions = pd.DataFrame({\n",
        "        'PID': merged_test_data['PID'],\n",
        "        'Predicted_Sale_Price': y_tst_pred\n",
        "    })\n",
        "    merged_test_data = pd.merge(test_y_data, predictions, on='PID', how='inner')\n",
        "\n",
        "    # Calculate RMSE and R-squared for the test set using actual sale prices\n",
        "    test_rmse = calc_rmse(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    r2_test = r2_score(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    adj_r2_test = adjusted_r_squared(r2_test, len(merged_test_data), X_tst_scaled.shape[1])\n",
        "\n",
        "    print(\"Test RMSE:\", test_rmse)\n",
        "    print(\"Test R-squared:\", r2_test)\n",
        "    print(\"Adjusted R-squared (Test):\", adj_r2_test)\n",
        "\n",
        "    # Save the predictions to a CSV file\n",
        "    predictions.to_csv('test_predictions.csv', index=False)\n",
        "    print(\"Predictions for test set saved to 'test_predictions.csv'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v32U7xeiY1h",
        "outputId": "fc983d5f-ca84-45b2-cc71-81d79062185f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "housing_data_encoded_train shape (2051, 307)\n",
            "housing_data_encoded_test shape (879, 307)\n",
            "housing_data_encoded_imputed_train shape (2051, 307)\n",
            "housing_data_encoded_imputed_test shape (879, 307)\n",
            "removed_features []\n",
            "Final features after backward elimination: Index(['Overall_Qual_Very_Poor', 'Sale_Condition_Family',\n",
            "       'Garage_Type_CarPort', 'Exterior_1st_Plywood', 'Overall_Cond_Excellent',\n",
            "       'Heating_QC_Fair', 'Neighborhood_Northridge_Heights',\n",
            "       'Overall_Qual_Poor', 'Pool_QC_Fair', 'BsmtFin_Type_2_BLQ',\n",
            "       ...\n",
            "       'House_Style_Two_and_Half_Fin', 'Exterior_2nd_Plywood', 'Lot_Area',\n",
            "       'Utilities_NoSewr', 'Mas_Vnr_Type_CBlock', 'Neighborhood_College_Creek',\n",
            "       'Exterior_2nd_Stone', 'Bsmt_Cond_Fair', 'Electrical_FuseF',\n",
            "       'Roof_Matl_Metal'],\n",
            "      dtype='object', length=305)\n",
            "Removed features: []\n",
            "X_tst_final shape (879, 305)\n",
            "X_tst_scaled shape (879, 305)\n",
            "y_trn_pred shape (2051,)\n",
            "Training R-squared: 0.9443669766417784\n",
            "Adjusted R-squared (Training): 0.9346431530748686\n",
            "Training RMSE: 0.09753736447143588\n",
            "y_tst_pred shape (879,)\n",
            "Test RMSE: 2.5465692336394667\n",
            "Test R-squared: -41.02793523743719\n",
            "Adjusted R-squared (Test): -63.39882572158788\n",
            "Predictions for test set saved to 'test_predictions.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Helper functions\n",
        "def preprocess_data(X_trn):\n",
        "    # Initialize the StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit the scaler on the training data and transform it\n",
        "    X_trn_scaled = scaler.fit_transform(X_trn)\n",
        "\n",
        "    # Return the scaled training data as a DataFrame (to preserve column names) and the scaler object\n",
        "    X_trn_scaled_df = pd.DataFrame(X_trn_scaled, columns=X_trn.columns)\n",
        "\n",
        "    return X_trn_scaled_df, scaler\n",
        "\n",
        "def calc_rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def adjusted_r_squared(r2, n, p):\n",
        "    # Calculate adjusted R-squared\n",
        "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "\n",
        "def main():\n",
        "    # Load the train, test, and test_y datasets\n",
        "    train_file_path = 'fold8/train.csv'\n",
        "    test_file_path = 'fold8/test.csv'\n",
        "    test_y_file_path = 'fold8/test_y.csv'\n",
        "    housing_data_train = pd.read_csv(train_file_path)\n",
        "    housing_data_test = pd.read_csv(test_file_path)\n",
        "    test_y_data = pd.read_csv(test_y_file_path)  # Load the actual Sale Price for the test set\n",
        "\n",
        "    # Ensure that test_y_data and housing_data_test have matching PIDs\n",
        "    merged_test_data = pd.merge(housing_data_test, test_y_data, on='PID', how='inner')\n",
        "\n",
        "    # Identify categorical columns\n",
        "    categorical_columns = ['MS_SubClass', 'MS_Zoning', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities',\n",
        "                           'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type',\n",
        "                           'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style', 'Roof_Matl', 'Exterior_1st',\n",
        "                           'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual', 'Exter_Cond', 'Foundation', 'Bsmt_Qual',\n",
        "                           'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating', 'Heating_QC',\n",
        "                           'Central_Air', 'Electrical', 'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type',\n",
        "                           'Garage_Finish', 'Garage_Qual', 'Garage_Cond', 'Paved_Drive', 'Pool_QC', 'Fence',\n",
        "                           'Misc_Feature', 'Sale_Type', 'Sale_Condition']\n",
        "\n",
        "    # Perform dummy encoding on both train and test datasets\n",
        "    housing_data_encoded_train = pd.get_dummies(housing_data_train, columns=categorical_columns, drop_first=True)\n",
        "    housing_data_encoded_test = pd.get_dummies(merged_test_data, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "    # Align columns of train and test after dummy encoding:\n",
        "    all_columns = list(set(housing_data_encoded_train.columns) | set(housing_data_encoded_test.columns))\n",
        "    all_columns.remove('Sale_Price')  # Remove 'Sale_Price' if it's in all_columns\n",
        "\n",
        "    housing_data_encoded_train = housing_data_encoded_train.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    housing_data_encoded_test = housing_data_encoded_test.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    print(\"housing_data_encoded_train shape\", housing_data_encoded_train.shape)\n",
        "    print(\"housing_data_encoded_test shape\", housing_data_encoded_test.shape)\n",
        "\n",
        "    # Handle missing values by imputing them in train and test data\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    numeric_columns = housing_data_encoded_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numeric_columns = [col for col in numeric_columns if col != 'Sale_Price']\n",
        "\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_train.copy()\n",
        "    housing_data_encoded_imputed_train[numeric_columns] = imputer.fit_transform(housing_data_encoded_train[numeric_columns])\n",
        "\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_test.copy()\n",
        "    housing_data_encoded_imputed_test[numeric_columns] = imputer.transform(housing_data_encoded_test[numeric_columns])\n",
        "    print(\"housing_data_encoded_imputed_train shape\", housing_data_encoded_imputed_train.shape)\n",
        "    print(\"housing_data_encoded_imputed_test shape\", housing_data_encoded_imputed_test.shape)\n",
        "\n",
        "    # Remove 'PID' from train and test datasets\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_imputed_train.drop(columns=['PID'])\n",
        "    if 'PID' in housing_data_encoded_imputed_test.columns:\n",
        "        housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.drop(columns=['PID'])\n",
        "\n",
        "    # Separate predictors (X) and target variable (y) for training\n",
        "    X_trn = housing_data_encoded_imputed_train.drop(columns='Sale_Price')\n",
        "    y_trn = np.log(housing_data_encoded_imputed_train['Sale_Price'])\n",
        "\n",
        "    # Scale the training data using preprocess_data function (returns DataFrame with column names)\n",
        "    X_trn_scaled, scaler = preprocess_data(X_trn)\n",
        "\n",
        "    # 1. Use Lasso regression for feature selection\n",
        "    lasso_model = Lasso(alpha=0.01)  # Adjust the alpha value as necessary\n",
        "    lasso_model.fit(X_trn_scaled, y_trn)\n",
        "\n",
        "    # Select the features with non-zero coefficients\n",
        "    selected_features = X_trn_scaled.columns[lasso_model.coef_ != 0]\n",
        "    print(f\"Selected features after Lasso: {len(selected_features)} features\")\n",
        "\n",
        "    # Scale the test data using the same scaler\n",
        "    X_tst_scaled = pd.DataFrame(scaler.transform(housing_data_encoded_imputed_test.drop(columns=['Sale_Price'])), columns=housing_data_encoded_imputed_test.drop(columns=['Sale_Price']).columns)\n",
        "    print(\"X_tst_scaled shape\", X_tst_scaled.shape)\n",
        "\n",
        "\n",
        "\n",
        "    # 2. Grid Search for Ridge Regression to find the best alpha\n",
        "    param_grid = {'alpha': [0.01, 0.1, 1.0, 10, 100]}\n",
        "    ridge_model = Ridge()\n",
        "    grid_search = GridSearchCV(ridge_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "    grid_search.fit(X_trn_scaled[selected_features], y_trn)\n",
        "\n",
        "    # Get the best alpha value from grid search\n",
        "    best_alpha = grid_search.best_params_['alpha']\n",
        "    print(\"Best alpha for Ridge regression:\", best_alpha)\n",
        "\n",
        "    # 3. Train the final Ridge model with the best alpha\n",
        "    final_ridge_model = Ridge(alpha=best_alpha)\n",
        "    final_ridge_model.fit(X_trn_scaled[selected_features], y_trn)\n",
        "\n",
        "    # Predict on the training set\n",
        "    y_trn_pred = final_ridge_model.predict(X_trn_scaled[selected_features])\n",
        "\n",
        "    # Calculate R-squared and RMSE for training data\n",
        "    r2_train = r2_score(y_trn, y_trn_pred)\n",
        "    adj_r2_train = adjusted_r_squared(r2_train, len(y_trn), X_trn_scaled[selected_features].shape[1])\n",
        "    train_rmse = calc_rmse(y_trn, y_trn_pred)\n",
        "    print(f\"Training R-squared: {r2_train}\")\n",
        "    print(f\"Adjusted R-squared (Training): {adj_r2_train}\")\n",
        "    print(f\"Training RMSE: {train_rmse}\")\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_tst_pred_log = final_ridge_model.predict(X_tst_scaled[selected_features])\n",
        "\n",
        "    # Reverse the log-transformation to get predictions in the original scale\n",
        "    y_tst_pred = np.exp(y_tst_pred_log)\n",
        "\n",
        "    # Merge the predictions with actual sale prices from test_y.csv using 'PID'\n",
        "    predictions = pd.DataFrame({\n",
        "        'PID': merged_test_data['PID'],\n",
        "        'Predicted_Sale_Price': y_tst_pred\n",
        "    })\n",
        "    merged_test_data = pd.merge(test_y_data, predictions, on='PID', how='inner')\n",
        "\n",
        "    # Calculate RMSE and R-squared for the test set using actual sale prices\n",
        "    test_rmse = calc_rmse(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    r2_test = r2_score(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    adj_r2_test = adjusted_r_squared(r2_test, len(merged_test_data), X_tst_scaled.shape[1])\n",
        "    print(f\"Test RMSE: {test_rmse}\")\n",
        "    print(f\"Test R-squared: {r2_test}\")\n",
        "    print(f\"Adjusted R-squared (Test): {adj_r2_test}\")\n",
        "\n",
        "    # Save the predictions to a CSV file\n",
        "    predictions.to_csv('test_predictions.csv', index=False)\n",
        "    print(\"Predictions for test set saved to 'test_predictions.csv'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXq0OTtEm829",
        "outputId": "c0f90cf5-f616-4364-8233-7907ab1c670d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "housing_data_encoded_train shape (2051, 307)\n",
            "housing_data_encoded_test shape (879, 307)\n",
            "housing_data_encoded_imputed_train shape (2051, 307)\n",
            "housing_data_encoded_imputed_test shape (879, 307)\n",
            "Selected features after Lasso: 69 features\n",
            "X_tst_scaled shape (879, 305)\n",
            "Best alpha for Ridge regression: 100\n",
            "Training R-squared: 0.9167256386576015\n",
            "Adjusted R-squared (Training): 0.9138251182473918\n",
            "Training RMSE: 0.11933294006933579\n",
            "Test RMSE: 0.1316017987470567\n",
            "Test R-squared: 0.887759206256692\n",
            "Adjusted R-squared (Test): 0.8280149792205508\n",
            "Predictions for test set saved to 'test_predictions.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above uses\n",
        "he model is using the Ridge regression to calculate the training and testing RMSE. Here's why:\n",
        "\n",
        "In the code, you first use Lasso regression to perform feature selection by setting alpha=0.01 and fitting the Lasso model to the training data (X_trn_scaled).\n",
        "\n",
        "After the feature selection, only the features with non-zero coefficients are retained (selected_features).\n",
        "\n",
        "Once the Lasso model has selected the features, you proceed to use Ridge regression for training and testing:\n",
        "\n",
        "A grid search is performed to find the best alpha for the Ridge regression model:\n",
        "\n",
        "The Ridge model is then trained using the selected features from the Lasso model:\n",
        "Finally, predictions on both the training and test sets are made using the Ridge model, and the training and test RMSE are calculated based on those predictions.\n",
        "\n",
        "Summary:\n",
        "Lasso regression is used only for feature selection.\n",
        "Ridge regression is used to actually fit the model and calculate the training and testing"
      ],
      "metadata": {
        "id": "g8BLiKtSTsol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below code is for lasso model only skipping ridge model"
      ],
      "metadata": {
        "id": "rRcKyIn7T9vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Helper functions\n",
        "def preprocess_data(X_trn):\n",
        "    # Initialize the StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit the scaler on the training data and transform it\n",
        "    X_trn_scaled = scaler.fit_transform(X_trn)\n",
        "\n",
        "    # Return the scaled training data as a DataFrame (to preserve column names) and the scaler object\n",
        "    X_trn_scaled_df = pd.DataFrame(X_trn_scaled, columns=X_trn.columns)\n",
        "\n",
        "    return X_trn_scaled_df, scaler\n",
        "\n",
        "def calc_rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def adjusted_r_squared(r2, n, p):\n",
        "    # Calculate adjusted R-squared\n",
        "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "\n",
        "def main():\n",
        "    # Load the train, test, and test_y datasets\n",
        "    train_file_path = 'fold8/train.csv'\n",
        "    test_file_path = 'fold8/test.csv'\n",
        "    test_y_file_path = 'fold8/test_y.csv'\n",
        "    housing_data_train = pd.read_csv(train_file_path)\n",
        "    housing_data_test = pd.read_csv(test_file_path)\n",
        "    test_y_data = pd.read_csv(test_y_file_path)  # Load the actual Sale Price for the test set\n",
        "\n",
        "    # Ensure that test_y_data and housing_data_test have matching PIDs\n",
        "    merged_test_data = pd.merge(housing_data_test, test_y_data, on='PID', how='inner')\n",
        "\n",
        "    # Identify categorical columns\n",
        "    categorical_columns = ['MS_SubClass', 'MS_Zoning', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities',\n",
        "                           'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type',\n",
        "                           'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style', 'Roof_Matl', 'Exterior_1st',\n",
        "                           'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual', 'Exter_Cond', 'Foundation', 'Bsmt_Qual',\n",
        "                           'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating', 'Heating_QC',\n",
        "                           'Central_Air', 'Electrical', 'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type',\n",
        "                           'Garage_Finish', 'Garage_Qual', 'Garage_Cond', 'Paved_Drive', 'Pool_QC', 'Fence',\n",
        "                           'Misc_Feature', 'Sale_Type', 'Sale_Condition']\n",
        "\n",
        "    # Perform dummy encoding on both train and test datasets\n",
        "    housing_data_encoded_train = pd.get_dummies(housing_data_train, columns=categorical_columns, drop_first=True)\n",
        "    housing_data_encoded_test = pd.get_dummies(merged_test_data, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "    # Align columns of train and test after dummy encoding:\n",
        "    all_columns = list(set(housing_data_encoded_train.columns) | set(housing_data_encoded_test.columns))\n",
        "    all_columns.remove('Sale_Price')  # Remove 'Sale_Price' if it's in all_columns\n",
        "\n",
        "    housing_data_encoded_train = housing_data_encoded_train.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    housing_data_encoded_test = housing_data_encoded_test.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    print(\"housing_data_encoded_train shape\", housing_data_encoded_train.shape)\n",
        "    print(\"housing_data_encoded_test shape\", housing_data_encoded_test.shape)\n",
        "\n",
        "    # Handle missing values by imputing them in train and test data\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    numeric_columns = housing_data_encoded_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numeric_columns = [col for col in numeric_columns if col != 'Sale_Price']\n",
        "\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_train.copy()\n",
        "    housing_data_encoded_imputed_train[numeric_columns] = imputer.fit_transform(housing_data_encoded_train[numeric_columns])\n",
        "\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_test.copy()\n",
        "    housing_data_encoded_imputed_test[numeric_columns] = imputer.transform(housing_data_encoded_test[numeric_columns])\n",
        "    print(\"housing_data_encoded_imputed_train shape\", housing_data_encoded_imputed_train.shape)\n",
        "    print(\"housing_data_encoded_imputed_test shape\", housing_data_encoded_imputed_test.shape)\n",
        "\n",
        "    # Remove 'PID' from train and test datasets\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_imputed_train.drop(columns=['PID'])\n",
        "    if 'PID' in housing_data_encoded_imputed_test.columns:\n",
        "        housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.drop(columns=['PID'])\n",
        "\n",
        "    # Separate predictors (X) and target variable (y) for training\n",
        "    X_trn = housing_data_encoded_imputed_train.drop(columns='Sale_Price')\n",
        "    y_trn = np.log(housing_data_encoded_imputed_train['Sale_Price'])\n",
        "\n",
        "    # Scale the training data using preprocess_data function (returns DataFrame with column names)\n",
        "    X_trn_scaled, scaler = preprocess_data(X_trn)\n",
        "\n",
        "    # Tune Lasso alpha\n",
        "    lasso_param_grid = {'alpha': [0.001, 0.01, 0.1, 1.0]}\n",
        "    lasso_grid_search = GridSearchCV(Lasso(), lasso_param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "    lasso_grid_search.fit(X_trn_scaled, y_trn)\n",
        "\n",
        "    best_lasso_alpha = lasso_grid_search.best_params_['alpha']\n",
        "    print(\"Best Lasso alpha:\", best_lasso_alpha)\n",
        "\n",
        "    # Train the final Lasso model with the best alpha\n",
        "    lasso_model = Lasso(alpha=best_lasso_alpha)\n",
        "    lasso_model.fit(X_trn_scaled, y_trn)\n",
        "\n",
        "    # Select the features with non-zero coefficients\n",
        "    selected_features = X_trn_scaled.columns[lasso_model.coef_ != 0]\n",
        "    print(f\"Selected features after Lasso: {len(selected_features)} features\")\n",
        "\n",
        "    # Use the selected features for training\n",
        "    X_trn_scaled_selected = X_trn_scaled[selected_features]\n",
        "    lasso_model.fit(X_trn_scaled_selected, y_trn)\n",
        "    # Make predictions on the training set using the Lasso model\n",
        "    y_trn_pred_log = lasso_model.predict(X_trn_scaled_selected)\n",
        "\n",
        "    # Calculate RMSE for the training set\n",
        "    train_rmse = calc_rmse(y_trn, y_trn_pred_log)\n",
        "    print(f\"Training RMSE: {train_rmse}\")\n",
        "\n",
        "    # Scale the test data using the same scaler and select only the features used by the Lasso model\n",
        "    X_tst_scaled = pd.DataFrame(scaler.transform(housing_data_encoded_imputed_test.drop(columns=['Sale_Price'])),\n",
        "                                columns=housing_data_encoded_imputed_test.drop(columns=['Sale_Price']).columns)\n",
        "    X_tst_scaled_selected = X_tst_scaled[selected_features]\n",
        "    print(\"X_tst_scaled_selected shape\", X_tst_scaled_selected.shape)\n",
        "\n",
        "    # Make predictions on the test set using the Lasso model\n",
        "    y_tst_pred_log = lasso_model.predict(X_tst_scaled_selected)\n",
        "\n",
        "    # Reverse the log-transformation to get predictions in the original scale\n",
        "    y_tst_pred = np.exp(y_tst_pred_log)\n",
        "\n",
        "    # Merge the predictions with actual sale prices from test_y.csv using 'PID'\n",
        "    predictions = pd.DataFrame({\n",
        "        'PID': merged_test_data['PID'],\n",
        "        'Predicted_Sale_Price': y_tst_pred\n",
        "    })\n",
        "    merged_test_data = pd.merge(test_y_data, predictions, on='PID', how='inner')\n",
        "\n",
        "    # Calculate RMSE and R-squared for the test set using actual sale prices\n",
        "    test_rmse = calc_rmse(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    r2_test = r2_score(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    adj_r2_test = adjusted_r_squared(r2_test, len(merged_test_data), X_tst_scaled_selected.shape[1])\n",
        "    print(f\"Test RMSE: {test_rmse}\")\n",
        "    print(f\"Test R-squared: {r2_test}\")\n",
        "    print(f\"Adjusted R-squared (Test): {adj_r2_test}\")\n",
        "\n",
        "    # Save the predictions to a CSV file\n",
        "    predictions.to_csv('test_predictions.csv', index=False)\n",
        "    print(\"Predictions for test set saved to 'test_predictions.csv'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coV9F3q6UCY6",
        "outputId": "f11dfd58-2a1e-4696-f45d-13dc73f959c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "housing_data_encoded_train shape (2051, 307)\n",
            "housing_data_encoded_test shape (879, 307)\n",
            "housing_data_encoded_imputed_train shape (2051, 307)\n",
            "housing_data_encoded_imputed_test shape (879, 307)\n",
            "Best Lasso alpha: 0.01\n",
            "Selected features after Lasso: 69 features\n",
            "Training RMSE: 0.13135307975132254\n",
            "X_tst_scaled_selected shape (879, 69)\n",
            "Test RMSE: 0.1489133313827432\n",
            "Test R-squared: 0.8562876015142857\n",
            "Adjusted R-squared (Test): 0.8440303017670493\n",
            "Predictions for test set saved to 'test_predictions.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rCNJVZrX043k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Helper functions\n",
        "def preprocess_data(X_trn):\n",
        "    # Initialize the StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit the scaler on the training data and transform it\n",
        "    X_trn_scaled = scaler.fit_transform(X_trn)\n",
        "\n",
        "    # Return the scaled training data as a DataFrame (to preserve column names) and the scaler object\n",
        "    X_trn_scaled_df = pd.DataFrame(X_trn_scaled, columns=X_trn.columns)\n",
        "\n",
        "    return X_trn_scaled_df, scaler\n",
        "\n",
        "def calc_rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def adjusted_r_squared(r2, n, p):\n",
        "    # Calculate adjusted R-squared\n",
        "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "\n",
        "def main():\n",
        "    # Load the train, test, and test_y datasets\n",
        "    train_file_path = 'fold8/train.csv'\n",
        "    test_file_path = 'fold8/test.csv'\n",
        "    test_y_file_path = 'fold8/test_y.csv'\n",
        "    housing_data_train = pd.read_csv(train_file_path)\n",
        "    housing_data_test = pd.read_csv(test_file_path)\n",
        "    test_y_data = pd.read_csv(test_y_file_path)  # Load the actual Sale Price for the test set\n",
        "\n",
        "    # Ensure that test_y_data and housing_data_test have matching PIDs\n",
        "    merged_test_data = pd.merge(housing_data_test, test_y_data, on='PID', how='inner')\n",
        "\n",
        "    # Identify categorical columns\n",
        "    categorical_columns = ['MS_SubClass', 'MS_Zoning', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities',\n",
        "                           'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type',\n",
        "                           'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style', 'Roof_Matl', 'Exterior_1st',\n",
        "                           'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual', 'Exter_Cond', 'Foundation', 'Bsmt_Qual',\n",
        "                           'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating', 'Heating_QC',\n",
        "                           'Central_Air', 'Electrical', 'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type',\n",
        "                           'Garage_Finish', 'Garage_Qual', 'Garage_Cond', 'Paved_Drive', 'Pool_QC', 'Fence',\n",
        "                           'Misc_Feature', 'Sale_Type', 'Sale_Condition']\n",
        "\n",
        "    # Perform dummy encoding on both train and test datasets\n",
        "    housing_data_encoded_train = pd.get_dummies(housing_data_train, columns=categorical_columns, drop_first=True)\n",
        "    housing_data_encoded_test = pd.get_dummies(merged_test_data, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "    # Align columns of train and test after dummy encoding:\n",
        "    all_columns = list(set(housing_data_encoded_train.columns) | set(housing_data_encoded_test.columns))\n",
        "    all_columns.remove('Sale_Price')  # Remove 'Sale_Price' if it's in all_columns\n",
        "\n",
        "    housing_data_encoded_train = housing_data_encoded_train.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    housing_data_encoded_test = housing_data_encoded_test.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    print(\"housing_data_encoded_train shape\", housing_data_encoded_train.shape)\n",
        "    print(\"housing_data_encoded_test shape\", housing_data_encoded_test.shape)\n",
        "\n",
        "    # Handle missing values by imputing them in train and test data\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    numeric_columns = housing_data_encoded_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numeric_columns = [col for col in numeric_columns if col != 'Sale_Price']\n",
        "\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_train.copy()\n",
        "    print(\"housing_data_encoded_imputed_train shape\", housing_data_encoded_imputed_train.shape)\n",
        "    housing_data_encoded_imputed_train[numeric_columns] = imputer.fit_transform(housing_data_encoded_train[numeric_columns])\n",
        "\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_test.copy()\n",
        "    housing_data_encoded_imputed_test[numeric_columns] = imputer.transform(housing_data_encoded_test[numeric_columns])\n",
        "    print(\"housing_data_encoded_imputed_train shape\", housing_data_encoded_imputed_train.shape)\n",
        "    print(\"housing_data_encoded_imputed_test shape\", housing_data_encoded_imputed_test.shape)\n",
        "\n",
        "    # Remove 'PID' from train and test datasets\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_imputed_train.drop(columns=['PID'])\n",
        "    #if 'PID' in housing_data_encoded_imputed_test.columns:\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.drop(columns=['PID'])\n",
        "\n",
        "    # Separate predictors (X) and target variable (y) for training\n",
        "    X_trn = housing_data_encoded_imputed_train.drop(columns='Sale_Price')\n",
        "    y_trn = np.log(housing_data_encoded_imputed_train['Sale_Price'])\n",
        "    print(\"X_trn shape\", X_trn.shape)\n",
        "    print(\"y_trn shape\", y_trn.shape)\n",
        "    # Scale the training data using preprocess_data function (returns DataFrame with column names)\n",
        "    X_trn_scaled, scaler = preprocess_data(X_trn)\n",
        "\n",
        "    # Tune Lasso alpha\n",
        "    lasso_param_grid = {'alpha': [0.001, 0.01, 0.1, 1.0]}\n",
        "    lasso_grid_search = GridSearchCV(Lasso(), lasso_param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "    lasso_grid_search.fit(X_trn_scaled, y_trn)\n",
        "\n",
        "    best_lasso_alpha = lasso_grid_search.best_params_['alpha']\n",
        "    print(\"Best Lasso alpha:\", best_lasso_alpha)\n",
        "\n",
        "    lasso_model = Lasso(alpha=best_lasso_alpha)\n",
        "    lasso_model.fit(X_trn_scaled, y_trn)\n",
        "\n",
        "    selected_features = X_trn_scaled.columns[lasso_model.coef_ != 0]\n",
        "    print(f\"Selected features after tuned Lasso: {len(selected_features)} features\")\n",
        "    print(\"housing_data_encoded_imputed_test\", housing_data_encoded_imputed_test.shape)\n",
        "    print(\"housing_data_encoded_imputed_test.drop(columns=['Sale_Price'])\", housing_data_encoded_imputed_test.drop(columns=['Sale_Price']).shape)\n",
        "    # Scale the test data using the same scaler\n",
        "    X_tst_scaled = pd.DataFrame(scaler.transform(housing_data_encoded_imputed_test.drop(columns=['Sale_Price'])), columns=housing_data_encoded_imputed_test.drop(columns=['Sale_Price']).columns)\n",
        "    print(\"X_tst_scaled shape\", X_tst_scaled.shape)\n",
        "\n",
        "    # Make predictions on the test set using the Lasso model\n",
        "    y_tst_pred_log = lasso_model.predict(X_tst_scaled)\n",
        "\n",
        "    # Reverse the log-transformation to get predictions in the original scale\n",
        "    y_tst_pred = np.exp(y_tst_pred_log)\n",
        "\n",
        "    # Merge the predictions with actual sale prices from test_y.csv using 'PID'\n",
        "    predictions = pd.DataFrame({\n",
        "        'PID': merged_test_data['PID'],\n",
        "        'Predicted_Sale_Price': y_tst_pred\n",
        "    })\n",
        "    merged_test_data = pd.merge(test_y_data, predictions, on='PID', how='inner')\n",
        "\n",
        "    # Calculate RMSE and R-squared for the test set using actual sale prices\n",
        "    test_rmse = calc_rmse(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    r2_test = r2_score(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    adj_r2_test = adjusted_r_squared(r2_test, len(merged_test_data), X_tst_scaled.shape[1])\n",
        "    print(f\"Test RMSE: {test_rmse}\")\n",
        "    print(f\"Test R-squared: {r2_test}\")\n",
        "    print(f\"Adjusted R-squared (Test): {adj_r2_test}\")\n",
        "\n",
        "    # Save the predictions to a CSV file\n",
        "    predictions.to_csv('test_predictions.csv', index=False)\n",
        "    print(\"Predictions for test set saved to 'test_predictions.csv'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7afef4c7-2d3e-420e-c93f-90a749261f17",
        "id": "SgRHAGeS05XE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "housing_data_encoded_train shape (2051, 307)\n",
            "housing_data_encoded_test shape (879, 307)\n",
            "housing_data_encoded_imputed_train shape (2051, 307)\n",
            "housing_data_encoded_imputed_train shape (2051, 307)\n",
            "housing_data_encoded_imputed_test shape (879, 307)\n",
            "X_trn shape (2051, 305)\n",
            "y_trn shape (2051,)\n",
            "Best Lasso alpha: 0.01\n",
            "Selected features after tuned Lasso: 69 features\n",
            "housing_data_encoded_imputed_test (879, 306)\n",
            "housing_data_encoded_imputed_test.drop(columns=['Sale_Price']) (879, 305)\n",
            "X_tst_scaled shape (879, 305)\n",
            "Test RMSE: 0.14891342282154155\n",
            "Test R-squared: 0.8562874250238005\n",
            "Adjusted R-squared (Test): 0.7797912027415302\n",
            "Predictions for test set saved to 'test_predictions.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Helper functions\n",
        "def preprocess_data(X_trn):\n",
        "    # Initialize the StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit the scaler on the training data and transform it\n",
        "    X_trn_scaled = scaler.fit_transform(X_trn)\n",
        "\n",
        "    # Return the scaled training data as a DataFrame (to preserve column names) and the scaler object\n",
        "    X_trn_scaled_df = pd.DataFrame(X_trn_scaled, columns=X_trn.columns)\n",
        "\n",
        "    return X_trn_scaled_df, scaler\n",
        "\n",
        "def calc_rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def adjusted_r_squared(r2, n, p):\n",
        "    # Calculate adjusted R-squared\n",
        "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "\n",
        "def main():\n",
        "    # Load the train, test, and test_y datasets\n",
        "    train_file_path = 'fold8/train.csv'\n",
        "    test_file_path = 'fold8/test.csv'\n",
        "    test_y_file_path = 'fold8/test_y.csv'\n",
        "    housing_data_train = pd.read_csv(train_file_path)\n",
        "    housing_data_test = pd.read_csv(test_file_path)\n",
        "    test_y_data = pd.read_csv(test_y_file_path)  # Load the actual Sale Price for the test set\n",
        "\n",
        "    # Ensure that test_y_data and housing_data_test have matching PIDs\n",
        "    merged_test_data = pd.merge(housing_data_test, test_y_data, on='PID', how='inner')\n",
        "\n",
        "    # Identify categorical columns\n",
        "    categorical_columns = ['MS_SubClass', 'MS_Zoning', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities',\n",
        "                           'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type',\n",
        "                           'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style', 'Roof_Matl', 'Exterior_1st',\n",
        "                           'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual', 'Exter_Cond', 'Foundation', 'Bsmt_Qual',\n",
        "                           'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating', 'Heating_QC',\n",
        "                           'Central_Air', 'Electrical', 'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type',\n",
        "                           'Garage_Finish', 'Garage_Qual', 'Garage_Cond', 'Paved_Drive', 'Pool_QC', 'Fence',\n",
        "                           'Misc_Feature', 'Sale_Type', 'Sale_Condition']\n",
        "\n",
        "    # Perform dummy encoding on both train and test datasets\n",
        "    housing_data_encoded_train = pd.get_dummies(housing_data_train, columns=categorical_columns, drop_first=True)\n",
        "    housing_data_encoded_test = pd.get_dummies(merged_test_data, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "    # Align columns of train and test after dummy encoding:\n",
        "    all_columns = list(set(housing_data_encoded_train.columns) | set(housing_data_encoded_test.columns))\n",
        "    all_columns.remove('Sale_Price')  # Remove 'Sale_Price' if it's in all_columns\n",
        "\n",
        "    housing_data_encoded_train = housing_data_encoded_train.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    housing_data_encoded_test = housing_data_encoded_test.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    print(\"housing_data_encoded_train shape\", housing_data_encoded_train.shape)\n",
        "    print(\"housing_data_encoded_test shape\", housing_data_encoded_test.shape)\n",
        "\n",
        "    # Handle missing values by imputing them in train and test data\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    numeric_columns = housing_data_encoded_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numeric_columns = [col for col in numeric_columns if col != 'Sale_Price']\n",
        "\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_train.copy()\n",
        "    housing_data_encoded_imputed_train[numeric_columns] = imputer.fit_transform(housing_data_encoded_train[numeric_columns])\n",
        "\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_test.copy()\n",
        "    housing_data_encoded_imputed_test[numeric_columns] = imputer.transform(housing_data_encoded_test[numeric_columns])\n",
        "    print(\"housing_data_encoded_imputed_train shape\", housing_data_encoded_imputed_train.shape)\n",
        "    print(\"housing_data_encoded_imputed_test shape\", housing_data_encoded_imputed_test.shape)\n",
        "\n",
        "    # Remove 'PID' from train and test datasets\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_imputed_train.drop(columns=['PID'])\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.drop(columns=['PID'])\n",
        "\n",
        "    # Separate predictors (X) and target variable (y) for training\n",
        "    X_trn = housing_data_encoded_imputed_train.drop(columns='Sale_Price')\n",
        "    y_trn = np.log(housing_data_encoded_imputed_train['Sale_Price'])\n",
        "    print(\"X_trn shape\", X_trn.shape)\n",
        "    print(\"y_trn shape\", y_trn.shape)\n",
        "\n",
        "    # Scale the training data using preprocess_data function (returns DataFrame with column names)\n",
        "    X_trn_scaled, scaler = preprocess_data(X_trn)\n",
        "\n",
        "    # Tune Lasso alpha\n",
        "    lasso_param_grid = {'alpha': [0.001, 0.01, 0.1, 1.0]}\n",
        "    lasso_grid_search = GridSearchCV(Lasso(), lasso_param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "    lasso_grid_search.fit(X_trn_scaled, y_trn)\n",
        "\n",
        "    best_lasso_alpha = lasso_grid_search.best_params_['alpha']\n",
        "    print(\"Best Lasso alpha:\", best_lasso_alpha)\n",
        "\n",
        "    lasso_model = Lasso(alpha=best_lasso_alpha)\n",
        "    lasso_model.fit(X_trn_scaled, y_trn)\n",
        "\n",
        "    selected_features = X_trn_scaled.columns[lasso_model.coef_ != 0]\n",
        "    print(f\"Selected features after tuned Lasso: {len(selected_features)} features\")\n",
        "\n",
        "    # **Only keep selected features for both training and testing**\n",
        "    X_trn_scaled_selected = X_trn_scaled[selected_features]\n",
        "    print(\"X_trn_scaled_selected shape:\", X_trn_scaled_selected.shape)\n",
        "\n",
        "    lasso_model.fit(X_trn_scaled_selected, y_trn)\n",
        "\n",
        "    # Make predictions on the training set using the Lasso model\n",
        "    y_trn_pred_log = lasso_model.predict(X_trn_scaled_selected)\n",
        "\n",
        "    # Calculate RMSE for the training set\n",
        "    train_rmse = calc_rmse(y_trn, y_trn_pred_log)\n",
        "    print(f\"Training RMSE: {train_rmse}\")\n",
        "\n",
        "    # Scale the test data using the same scaler and select only the features used by the Lasso model\n",
        "    X_tst_scaled = pd.DataFrame(scaler.transform(housing_data_encoded_imputed_test.drop(columns=['Sale_Price'])),\n",
        "                                columns=housing_data_encoded_imputed_test.drop(columns=['Sale_Price']).columns)\n",
        "    X_tst_scaled_selected = X_tst_scaled[selected_features]\n",
        "    print(\"X_tst_scaled_selected shape\", X_tst_scaled_selected.shape)\n",
        "\n",
        "    # Make predictions on the test set using the Lasso model\n",
        "    y_tst_pred_log = lasso_model.predict(X_tst_scaled_selected)\n",
        "\n",
        "    # Reverse the log-transformation to get predictions in the original scale\n",
        "    y_tst_pred = np.exp(y_tst_pred_log)\n",
        "\n",
        "    # Merge the predictions with actual sale prices from test_y.csv using 'PID'\n",
        "    predictions = pd.DataFrame({\n",
        "        'PID': merged_test_data['PID'],\n",
        "        'Predicted_Sale_Price': y_tst_pred\n",
        "    })\n",
        "    merged_test_data = pd.merge(test_y_data, predictions, on='PID', how='inner')\n",
        "\n",
        "    # Calculate RMSE and R-squared for the test set using actual sale prices\n",
        "    test_rmse = calc_rmse(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    r2_test = r2_score(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    adj_r2_test = adjusted_r_squared(r2_test, len(merged_test_data), X_tst_scaled_selected.shape[1])\n",
        "    print(f\"Test RMSE: {test_rmse}\")\n",
        "    print(f\"Test R-squared: {r2_test}\")\n",
        "    print(f\"Adjusted R-squared (Test): {adj_r2_test}\")\n",
        "\n",
        "    # Save the predictions to a CSV file\n",
        "    predictions.to_csv('test_predictions.csv', index=False)\n",
        "    print(\"Predictions for test set saved to 'test_predictions.csv'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAyLEpV0DsW3",
        "outputId": "a2f5ad15-bea2-4134-a59e-e7af00d9ad9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "housing_data_encoded_train shape (2051, 307)\n",
            "housing_data_encoded_test shape (879, 307)\n",
            "housing_data_encoded_imputed_train shape (2051, 307)\n",
            "housing_data_encoded_imputed_test shape (879, 307)\n",
            "X_trn shape (2051, 305)\n",
            "y_trn shape (2051,)\n",
            "Best Lasso alpha: 0.01\n",
            "Selected features after tuned Lasso: 69 features\n",
            "X_trn_scaled_selected shape: (2051, 69)\n",
            "Training RMSE: 0.13135307975132254\n",
            "X_tst_scaled_selected shape (879, 69)\n",
            "Test RMSE: 0.1489133313827432\n",
            "Test R-squared: 0.8562876015142857\n",
            "Adjusted R-squared (Test): 0.8440303017670493\n",
            "Predictions for test set saved to 'test_predictions.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Helper functions\n",
        "def preprocess_data(X_trn):\n",
        "    # Initialize the StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit the scaler on the training data and transform it\n",
        "    X_trn_scaled = scaler.fit_transform(X_trn)\n",
        "\n",
        "    # Return the scaled training data as a DataFrame (to preserve column names) and the scaler object\n",
        "    X_trn_scaled_df = pd.DataFrame(X_trn_scaled, columns=X_trn.columns)\n",
        "\n",
        "    return X_trn_scaled_df, scaler\n",
        "\n",
        "def calc_rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def adjusted_r_squared(r2, n, p):\n",
        "    # Calculate adjusted R-squared\n",
        "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "\n",
        "def main():\n",
        "    # Load the train, test, and test_y datasets\n",
        "    train_file_path = 'fold8/train.csv'\n",
        "    test_file_path = 'fold8/test.csv'\n",
        "    test_y_file_path = 'fold8/test_y.csv'\n",
        "    housing_data_train = pd.read_csv(train_file_path)\n",
        "    housing_data_test = pd.read_csv(test_file_path)\n",
        "    test_y_data = pd.read_csv(test_y_file_path)  # Load the actual Sale Price for the test set\n",
        "\n",
        "    # Ensure that test_y_data and housing_data_test have matching PIDs\n",
        "    merged_test_data = pd.merge(housing_data_test, test_y_data, on='PID', how='inner')\n",
        "\n",
        "    # Identify categorical columns\n",
        "    categorical_columns = ['MS_SubClass', 'MS_Zoning', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities',\n",
        "                           'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type',\n",
        "                           'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style', 'Roof_Matl', 'Exterior_1st',\n",
        "                           'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual', 'Exter_Cond', 'Foundation', 'Bsmt_Qual',\n",
        "                           'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating', 'Heating_QC',\n",
        "                           'Central_Air', 'Electrical', 'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type',\n",
        "                           'Garage_Finish', 'Garage_Qual', 'Garage_Cond', 'Paved_Drive', 'Pool_QC', 'Fence',\n",
        "                           'Misc_Feature', 'Sale_Type', 'Sale_Condition']\n",
        "\n",
        "    # Perform dummy encoding on both train and test datasets\n",
        "    housing_data_encoded_train = pd.get_dummies(housing_data_train, columns=categorical_columns, drop_first=True)\n",
        "    housing_data_encoded_test = pd.get_dummies(merged_test_data, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "    # Align columns of train and test after dummy encoding:\n",
        "    all_columns = list(set(housing_data_encoded_train.columns) | set(housing_data_encoded_test.columns))\n",
        "    all_columns.remove('Sale_Price')  # Remove 'Sale_Price' if it's in all_columns\n",
        "\n",
        "    housing_data_encoded_train = housing_data_encoded_train.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    housing_data_encoded_test = housing_data_encoded_test.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    print(\"housing_data_encoded_train shape\", housing_data_encoded_train.shape)\n",
        "    print(\"housing_data_encoded_test shape\", housing_data_encoded_test.shape)\n",
        "\n",
        "    # Handle missing values by imputing them in train and test data\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    numeric_columns = housing_data_encoded_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numeric_columns = [col for col in numeric_columns if col != 'Sale_Price']\n",
        "\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_train.copy()\n",
        "    housing_data_encoded_imputed_train[numeric_columns] = imputer.fit_transform(housing_data_encoded_train[numeric_columns])\n",
        "\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_test.copy()\n",
        "    housing_data_encoded_imputed_test[numeric_columns] = imputer.transform(housing_data_encoded_test[numeric_columns])\n",
        "    print(\"housing_data_encoded_imputed_train shape\", housing_data_encoded_imputed_train.shape)\n",
        "    print(\"housing_data_encoded_imputed_test shape\", housing_data_encoded_imputed_test.shape)\n",
        "\n",
        "    # Remove 'PID' from train and test datasets\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_imputed_train.drop(columns=['PID'])\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.drop(columns=['PID'])\n",
        "\n",
        "    # Separate predictors (X) and target variable (y) for training\n",
        "    X_trn = housing_data_encoded_imputed_train.drop(columns='Sale_Price')\n",
        "    y_trn = np.log(housing_data_encoded_imputed_train['Sale_Price'])\n",
        "    print(\"X_trn shape\", X_trn.shape)\n",
        "    print(\"y_trn shape\", y_trn.shape)\n",
        "\n",
        "    # Scale the training data using preprocess_data function (returns DataFrame with column names)\n",
        "    X_trn_scaled, scaler = preprocess_data(X_trn)\n",
        "\n",
        "    # Tune Lasso alpha\n",
        "    lasso_param_grid = {'alpha': [0.001, 0.01, 0.1, 1.0]}\n",
        "    lasso_grid_search = GridSearchCV(Lasso(), lasso_param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "    lasso_grid_search.fit(X_trn_scaled, y_trn)\n",
        "\n",
        "    best_lasso_alpha = lasso_grid_search.best_params_['alpha']\n",
        "    print(\"Best Lasso alpha:\", best_lasso_alpha)\n",
        "\n",
        "    lasso_model = Lasso(alpha=best_lasso_alpha)\n",
        "    lasso_model.fit(X_trn_scaled, y_trn)\n",
        "\n",
        "    # Select features based on non-zero coefficients\n",
        "    selected_features = X_trn_scaled.columns[lasso_model.coef_ != 0]\n",
        "    print(f\"Selected features after tuned Lasso: {len(selected_features)} features\")\n",
        "\n",
        "    # **Retrain the Lasso model on the selected features**\n",
        "    X_trn_scaled_selected = X_trn_scaled[selected_features]\n",
        "    lasso_model.fit(X_trn_scaled_selected, y_trn)\n",
        "\n",
        "    # Make predictions on the training set using the Lasso model\n",
        "    y_trn_pred_log = lasso_model.predict(X_trn_scaled_selected)\n",
        "\n",
        "    # Calculate RMSE for the training set\n",
        "    train_rmse = calc_rmse(y_trn, y_trn_pred_log)\n",
        "    print(f\"Training RMSE: {train_rmse}\")\n",
        "\n",
        "    # Scale the test data using the same scaler and select only the features used by the Lasso model\n",
        "    X_tst_scaled = pd.DataFrame(scaler.transform(housing_data_encoded_imputed_test.drop(columns=['Sale_Price'])),\n",
        "                                columns=housing_data_encoded_imputed_test.drop(columns=['Sale_Price']).columns)\n",
        "    X_tst_scaled_selected = X_tst_scaled[selected_features]\n",
        "    print(\"X_tst_scaled_selected shape\", X_tst_scaled_selected.shape)\n",
        "\n",
        "    # Make predictions on the test set using the retrained Lasso model\n",
        "    y_tst_pred_log = lasso_model.predict(X_tst_scaled_selected)\n",
        "\n",
        "    # Reverse the log-transformation to get predictions in the original scale\n",
        "    y_tst_pred = np.exp(y_tst_pred_log)\n",
        "\n",
        "    # Merge the predictions with actual sale prices from test_y.csv using 'PID'\n",
        "    predictions = pd.DataFrame({\n",
        "        'PID': merged_test_data['PID'],\n",
        "        'Predicted_Sale_Price': y_tst_pred\n",
        "    })\n",
        "    merged_test_data = pd.merge(test_y_data, predictions, on='PID', how='inner')\n",
        "\n",
        "    # Calculate RMSE and R-squared for the test set using actual sale prices\n",
        "    test_rmse = calc_rmse(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    r2_test = r2_score(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    adj_r2_test = adjusted_r_squared(r2_test, len(merged_test_data), X_tst_scaled_selected.shape[1])\n",
        "    print(f\"Test RMSE: {test_rmse}\")\n",
        "    print(f\"Test R-squared: {r2_test}\")\n",
        "    print(f\"Adjusted R-squared (Test): {adj_r2_test}\")\n",
        "\n",
        "    # Save the predictions to a CSV file\n",
        "    predictions.to_csv('test_predictions.csv', index=False)\n",
        "    print(\"Predictions for test set saved to 'test_predictions.csv'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MS5ZtupPJLne",
        "outputId": "e94925b5-38d1-47bb-ce7b-88ad00629212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "housing_data_encoded_train shape (2051, 307)\n",
            "housing_data_encoded_test shape (879, 307)\n",
            "housing_data_encoded_imputed_train shape (2051, 307)\n",
            "housing_data_encoded_imputed_test shape (879, 307)\n",
            "X_trn shape (2051, 305)\n",
            "y_trn shape (2051,)\n",
            "Best Lasso alpha: 0.01\n",
            "Selected features after tuned Lasso: 69 features\n",
            "Training RMSE: 0.13135307975132254\n",
            "X_tst_scaled_selected shape (879, 69)\n",
            "Test RMSE: 0.1489133313827432\n",
            "Test R-squared: 0.8562876015142857\n",
            "Adjusted R-squared (Test): 0.8440303017670493\n",
            "Predictions for test set saved to 'test_predictions.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jp81c_NXL12N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Helper functions\n",
        "def preprocess_data(X_trn):\n",
        "    # Initialize the StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit the scaler on the training data and transform it\n",
        "    X_trn_scaled = scaler.fit_transform(X_trn)\n",
        "\n",
        "    # Return the scaled training data as a DataFrame (to preserve column names) and the scaler object\n",
        "    X_trn_scaled_df = pd.DataFrame(X_trn_scaled, columns=X_trn.columns)\n",
        "\n",
        "    return X_trn_scaled_df, scaler\n",
        "\n",
        "def calc_rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def adjusted_r_squared(r2, n, p):\n",
        "    # Calculate adjusted R-squared\n",
        "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "\n",
        "def main():\n",
        "    # Load the train, test, and test_y datasets\n",
        "    train_file_path = 'fold8/train.csv'\n",
        "    test_file_path = 'fold8/test.csv'\n",
        "    test_y_file_path = 'fold8/test_y.csv'\n",
        "    housing_data_train = pd.read_csv(train_file_path)\n",
        "    housing_data_test = pd.read_csv(test_file_path)\n",
        "    test_y_data = pd.read_csv(test_y_file_path)  # Load the actual Sale Price for the test set\n",
        "\n",
        "    # Ensure that test_y_data and housing_data_test have matching PIDs\n",
        "    merged_test_data = pd.merge(housing_data_test, test_y_data, on='PID', how='inner')\n",
        "\n",
        "    # Identify categorical columns\n",
        "    categorical_columns = ['MS_SubClass', 'MS_Zoning', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities',\n",
        "                           'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type',\n",
        "                           'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style', 'Roof_Matl', 'Exterior_1st',\n",
        "                           'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual', 'Exter_Cond', 'Foundation', 'Bsmt_Qual',\n",
        "                           'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating', 'Heating_QC',\n",
        "                           'Central_Air', 'Electrical', 'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type',\n",
        "                           'Garage_Finish', 'Garage_Qual', 'Garage_Cond', 'Paved_Drive', 'Pool_QC', 'Fence',\n",
        "                           'Misc_Feature', 'Sale_Type', 'Sale_Condition']\n",
        "\n",
        "    # Perform dummy encoding on both train and test datasets\n",
        "    housing_data_encoded_train = pd.get_dummies(housing_data_train, columns=categorical_columns, drop_first=True)\n",
        "    housing_data_encoded_test = pd.get_dummies(merged_test_data, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "    # Align columns of train and test after dummy encoding:\n",
        "    all_columns = list(set(housing_data_encoded_train.columns) | set(housing_data_encoded_test.columns))\n",
        "    all_columns.remove('Sale_Price')  # Remove 'Sale_Price' if it's in all_columns\n",
        "\n",
        "    housing_data_encoded_train = housing_data_encoded_train.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    housing_data_encoded_test = housing_data_encoded_test.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    print(\"housing_data_encoded_train shape\", housing_data_encoded_train.shape)\n",
        "    print(\"housing_data_encoded_test shape\", housing_data_encoded_test.shape)\n",
        "\n",
        "    # Handle missing values by imputing them in train and test data\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    numeric_columns = housing_data_encoded_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numeric_columns = [col for col in numeric_columns if col != 'Sale_Price']\n",
        "\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_train.copy()\n",
        "    housing_data_encoded_imputed_train[numeric_columns] = imputer.fit_transform(housing_data_encoded_train[numeric_columns])\n",
        "\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_test.copy()\n",
        "    housing_data_encoded_imputed_test[numeric_columns] = imputer.transform(housing_data_encoded_test[numeric_columns])\n",
        "    print(\"housing_data_encoded_imputed_train shape\", housing_data_encoded_imputed_train.shape)\n",
        "    print(\"housing_data_encoded_imputed_test shape\", housing_data_encoded_imputed_test.shape)\n",
        "\n",
        "    # Remove 'PID' from train and test datasets\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_imputed_train.drop(columns=['PID'])\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.drop(columns=['PID'])\n",
        "\n",
        "    # Separate predictors (X) and target variable (y) for training\n",
        "    X_trn = housing_data_encoded_imputed_train.drop(columns='Sale_Price')\n",
        "    y_trn = np.log(housing_data_encoded_imputed_train['Sale_Price'])\n",
        "    print(\"X_trn shape\", X_trn.shape)\n",
        "    print(\"y_trn shape\", y_trn.shape)\n",
        "\n",
        "    # Scale the training data using preprocess_data function (returns DataFrame with column names)\n",
        "    X_trn_scaled, scaler = preprocess_data(X_trn)\n",
        "\n",
        "    # Tune Lasso alpha\n",
        "    lasso_param_grid = {'alpha': [0.001, 0.01, 0.1, 1.0]}\n",
        "    lasso_grid_search = GridSearchCV(Lasso(), lasso_param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "    lasso_grid_search.fit(X_trn_scaled, y_trn)\n",
        "\n",
        "    best_lasso_alpha = lasso_grid_search.best_params_['alpha']\n",
        "    print(\"Best Lasso alpha:\", best_lasso_alpha)\n",
        "\n",
        "    lasso_model = Lasso(alpha=best_lasso_alpha)\n",
        "    lasso_model.fit(X_trn_scaled, y_trn)\n",
        "\n",
        "    # Select features based on non-zero coefficients\n",
        "    selected_features = X_trn_scaled.columns[lasso_model.coef_ != 0]\n",
        "    print(f\"Selected features after tuned Lasso: {len(selected_features)} features\")\n",
        "\n",
        "    # **Retrain the Lasso model on the selected features**\n",
        "    X_trn_scaled_selected = X_trn_scaled[selected_features]\n",
        "    lasso_model.fit(X_trn_scaled_selected, y_trn)\n",
        "\n",
        "    # Make predictions on the training set using the Lasso model\n",
        "    y_trn_pred_log = lasso_model.predict(X_trn_scaled_selected)\n",
        "\n",
        "    # Calculate RMSE for the training set\n",
        "    train_rmse = calc_rmse(y_trn, y_trn_pred_log)\n",
        "    print(f\"Training RMSE: {train_rmse}\")\n",
        "\n",
        "    # Scale the test data using the same scaler and select only the features used by the Lasso model\n",
        "    X_tst_scaled = pd.DataFrame(scaler.transform(housing_data_encoded_imputed_test.drop(columns=['Sale_Price'])),\n",
        "                                columns=housing_data_encoded_imputed_test.drop(columns=['Sale_Price']).columns)\n",
        "    X_tst_scaled_selected = X_tst_scaled[selected_features]\n",
        "    print(\"X_tst_scaled_selected shape\", X_tst_scaled_selected.shape)\n",
        "\n",
        "    # Make predictions on the test set using the retrained Lasso model\n",
        "    y_tst_pred_log = lasso_model.predict(X_tst_scaled_selected)\n",
        "\n",
        "    # Reverse the log-transformation to get predictions in the original scale\n",
        "    y_tst_pred = np.exp(y_tst_pred_log)\n",
        "\n",
        "    # Merge the predictions with actual sale prices from test_y.csv using 'PID'\n",
        "    predictions = pd.DataFrame({\n",
        "        'PID': merged_test_data['PID'],\n",
        "        'Predicted_Sale_Price': y_tst_pred\n",
        "    })\n",
        "    merged_test_data = pd.merge(test_y_data, predictions, on='PID', how='inner')\n",
        "\n",
        "    # Calculate RMSE and R-squared for the test set using actual sale prices\n",
        "    test_rmse = calc_rmse(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    r2_test = r2_score(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    adj_r2_test = adjusted_r_squared(r2_test, len(merged_test_data), X_tst_scaled_selected.shape[1])\n",
        "    print(f\"Test RMSE: {test_rmse}\")\n",
        "    print(f\"Test R-squared: {r2_test}\")\n",
        "    print(f\"Adjusted R-squared (Test): {adj_r2_test}\")\n",
        "\n",
        "    # Save the predictions to a CSV file\n",
        "    predictions.to_csv('test_predictions.csv', index=False)\n",
        "    print(\"Predictions for test set saved to 'test_predictions.csv'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa46a1cc-97d0-468f-85a1-30613dc4e0e7",
        "id": "ZrvVXG9jL2Vo"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "housing_data_encoded_train shape (2051, 307)\n",
            "housing_data_encoded_test shape (879, 307)\n",
            "housing_data_encoded_imputed_train shape (2051, 307)\n",
            "housing_data_encoded_imputed_test shape (879, 307)\n",
            "X_trn shape (2051, 305)\n",
            "y_trn shape (2051,)\n",
            "Best Lasso alpha: 0.01\n",
            "Selected features after tuned Lasso: 69 features\n",
            "Training RMSE: 0.13135307975132254\n",
            "X_tst_scaled_selected shape (879, 69)\n",
            "Test RMSE: 0.1489133313827432\n",
            "Test R-squared: 0.8562876015142857\n",
            "Adjusted R-squared (Test): 0.8440303017670493\n",
            "Predictions for test set saved to 'test_predictions.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy import stats  # Importing for z-score calculation\n",
        "\n",
        "# Helper functions\n",
        "def preprocess_data(X_trn):\n",
        "    # Initialize the StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit the scaler on the training data and transform it\n",
        "    X_trn_scaled = scaler.fit_transform(X_trn)\n",
        "\n",
        "    # Return the scaled training data as a DataFrame (to preserve column names) and the scaler object\n",
        "    X_trn_scaled_df = pd.DataFrame(X_trn_scaled, columns=X_trn.columns)\n",
        "\n",
        "    return X_trn_scaled_df, scaler\n",
        "\n",
        "def calc_rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def adjusted_r_squared(r2, n, p):\n",
        "    # Calculate adjusted R-squared\n",
        "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "\n",
        "def main():\n",
        "    # Load the train, test, and test_y datasets\n",
        "    train_file_path = 'fold3/train.csv'\n",
        "    test_file_path = 'fold3/test.csv'\n",
        "    test_y_file_path = 'fold3/test_y.csv'\n",
        "    housing_data_train = pd.read_csv(train_file_path)\n",
        "    housing_data_test = pd.read_csv(test_file_path)\n",
        "    test_y_data = pd.read_csv(test_y_file_path)  # Load the actual Sale Price for the test set\n",
        "\n",
        "    # Ensure that test_y_data and housing_data_test have matching PIDs\n",
        "    merged_test_data = pd.merge(housing_data_test, test_y_data, on='PID', how='inner')\n",
        "\n",
        "    # Identify categorical columns\n",
        "    categorical_columns = ['MS_SubClass', 'MS_Zoning', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities',\n",
        "                           'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type',\n",
        "                           'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style', 'Roof_Matl', 'Exterior_1st',\n",
        "                           'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual', 'Exter_Cond', 'Foundation', 'Bsmt_Qual',\n",
        "                           'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating', 'Heating_QC',\n",
        "                           'Central_Air', 'Electrical', 'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type',\n",
        "                           'Garage_Finish', 'Garage_Qual', 'Garage_Cond', 'Paved_Drive', 'Pool_QC', 'Fence',\n",
        "                           'Misc_Feature', 'Sale_Type', 'Sale_Condition']\n",
        "\n",
        "    # Perform dummy encoding on both train and test datasets\n",
        "    housing_data_encoded_train = pd.get_dummies(housing_data_train, columns=categorical_columns, drop_first=True)\n",
        "    housing_data_encoded_test = pd.get_dummies(merged_test_data, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "    # Align columns of train and test after dummy encoding:\n",
        "    all_columns = list(set(housing_data_encoded_train.columns) | set(housing_data_encoded_test.columns))\n",
        "    all_columns.remove('Sale_Price')  # Remove 'Sale_Price' if it's in all_columns\n",
        "\n",
        "    housing_data_encoded_train = housing_data_encoded_train.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    housing_data_encoded_test = housing_data_encoded_test.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    print(\"housing_data_encoded_train shape\", housing_data_encoded_train.shape)\n",
        "    print(\"housing_data_encoded_test shape\", housing_data_encoded_test.shape)\n",
        "\n",
        "    # Handle missing values by imputing them in train and test data\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    numeric_columns = housing_data_encoded_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    #numeric_columns = [col for col in numeric_columns if col != 'Sale_Price']\n",
        "    # Ensure 'Sale_Price' and 'PID' are excluded from numeric_columns\n",
        "    numeric_columns = [col for col in numeric_columns if col not in ['Sale_Price', 'PID']]\n",
        "\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_train.copy()\n",
        "    housing_data_encoded_imputed_train[numeric_columns] = imputer.fit_transform(housing_data_encoded_train[numeric_columns])\n",
        "\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_test.copy()\n",
        "    housing_data_encoded_imputed_test[numeric_columns] = imputer.transform(housing_data_encoded_test[numeric_columns])\n",
        "    print(\"housing_data_encoded_imputed_train shape\", housing_data_encoded_imputed_train.shape)\n",
        "    print(\"housing_data_encoded_imputed_test shape\", housing_data_encoded_imputed_test.shape)\n",
        "\n",
        "    # Safely drop 'PID' if it exists\n",
        "    if 'PID' in housing_data_encoded_imputed_train.columns:\n",
        "        housing_data_encoded_imputed_train = housing_data_encoded_imputed_train.drop(columns=['PID'])\n",
        "    if 'PID' in housing_data_encoded_imputed_test.columns:\n",
        "        housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.drop(columns=['PID'])\n",
        "\n",
        "    # **Handling outliers using z-scores**\n",
        "    z_scores = np.abs(stats.zscore(housing_data_encoded_imputed_train[numeric_columns]))\n",
        "\n",
        "    # Check the distribution of z-scores before filtering\n",
        "    print(\"Max z-score:\", np.max(z_scores))\n",
        "    print(\"Mean z-score:\", np.mean(z_scores))\n",
        "    # Convert the z-scores to a DataFrame for easier analysis\n",
        "    z_scores_df = pd.DataFrame(z_scores, columns=numeric_columns)\n",
        "    \"\"\"\n",
        "    # Find rows where any feature has a z-score greater than a threshold (e.g., 3)\n",
        "    outliers = z_scores_df > 3  # You can adjust this threshold as necessary\n",
        "\n",
        "    # Count how many outliers each feature has\n",
        "    outlier_counts = outliers.sum(axis=0).sort_values(ascending=False)\n",
        "    print(\"Number of outliers per feature:\")\n",
        "    print(outlier_counts)\n",
        "\n",
        "    # Select the top features with the highest number of outliers\n",
        "    top_outlier_features = outlier_counts[outlier_counts > 0].index.tolist()\n",
        "\n",
        "    print(f\"Top features contributing to outliers: {top_outlier_features}\")\n",
        "\n",
        "    # Investigate the rows where the z-scores are extreme for these features\n",
        "\n",
        "\n",
        "    extreme_outliers = z_scores_df[top_outlier_features][z_scores_df[top_outlier_features] > 3].dropna()\n",
        "    print(\"Extreme outliers in top contributing features:\")\n",
        "    print(extreme_outliers)\n",
        "\n",
        "    # Get the indices of rows with the highest z-scores (outliers)\n",
        "    outlier_rows = extreme_outliers.index\n",
        "\n",
        "    # Investigate the original data for these outlier rows\n",
        "    outlier_data = housing_data_encoded_imputed_train.loc[outlier_rows, top_outlier_features]\n",
        "    print(\"Rows with high z-scores (potential outliers):\")\n",
        "    print(outlier_data)\n",
        "    \"\"\"\n",
        "\n",
        "    # Adjust the threshold for filtering outliers, use a more relaxed threshold of 4\n",
        "    \"\"\"\n",
        "    threshold = 5\n",
        "    housing_data_cleaned = housing_data_encoded_imputed_train[(z_scores < threshold).all(axis=1)]\n",
        "    print(f\"Data shape after outlier removal1: {housing_data_cleaned.shape}\")\n",
        "    #housing_data_cleaned = housing_data_encoded_imputed_train[(z_scores < 3).all(axis=1)]\n",
        "    #print(f\"Data shape after outlier removal: {housing_data_cleaned.shape}\")\n",
        "    \"\"\"\n",
        "    corr_matrix = housing_data_encoded_imputed_train.corr()\n",
        "    sale_price_corr = corr_matrix['Sale_Price'].abs().sort_values(ascending=False)\n",
        "    print(\"sale_price_corr\",sale_price_corr)\n",
        "\n",
        "    high_outlier_rows = z_scores_df[z_scores_df > 9].dropna(how='all').index\n",
        "    housing_data_cleaned = housing_data_encoded_imputed_train.drop(high_outlier_rows)\n",
        "    print(f\"Data shape after outlier removal2: {housing_data_cleaned.shape}\")\n",
        "    # Separate predictors (X) and target variable (y) for training (after cleaning)\n",
        "    X_trn = housing_data_cleaned.drop(columns='Sale_Price')\n",
        "    y_trn = np.log(housing_data_cleaned['Sale_Price'])\n",
        "    print(\"X_trn shape\", X_trn.shape)\n",
        "    print(\"y_trn shape\", y_trn.shape)\n",
        "\n",
        "    # Scale the training data using preprocess_data function (returns DataFrame with column names)\n",
        "    X_trn_scaled, scaler = preprocess_data(X_trn)\n",
        "\n",
        "    # Tune Lasso alpha\n",
        "    lasso_param_grid = {'alpha': [0.0009, 0.001, 0.01, 0.1, 1.0]}\n",
        "    lasso_grid_search = GridSearchCV(Lasso(), lasso_param_grid, cv=8, scoring='neg_mean_squared_error')\n",
        "    lasso_grid_search.fit(X_trn_scaled, y_trn)\n",
        "\n",
        "    best_lasso_alpha = lasso_grid_search.best_params_['alpha']\n",
        "    print(\"Best Lasso alpha:\", best_lasso_alpha)\n",
        "\n",
        "    lasso_model = Lasso(alpha=best_lasso_alpha)\n",
        "    lasso_model.fit(X_trn_scaled, y_trn)\n",
        "    print(\"lasso_model.coef_\", lasso_model.coef_)\n",
        "    threshold = 1e-01 # Adjust this threshold as needed\n",
        "    # Select features based on non-zero coefficients\n",
        "    selected_features = X_trn_scaled.columns[lasso_model.coef_ != 0  ]\n",
        "    print(f\"Selected features after tuned Lasso: {len(selected_features)} features\")\n",
        "\n",
        "    # **Retrain the Lasso model on the selected features**\n",
        "    X_trn_scaled_selected = X_trn_scaled[selected_features]\n",
        "    lasso_model.fit(X_trn_scaled_selected, y_trn)\n",
        "\n",
        "    # Make predictions on the training set using the Lasso model\n",
        "    y_trn_pred_log = lasso_model.predict(X_trn_scaled_selected)\n",
        "\n",
        "    # Calculate RMSE for the training set\n",
        "    train_rmse = calc_rmse(y_trn, y_trn_pred_log)\n",
        "    print(f\"Training RMSE: {train_rmse}\")\n",
        "\n",
        "    # Scale the test data using the same scaler and select only the features used by the Lasso model\n",
        "    X_tst_scaled = pd.DataFrame(scaler.transform(housing_data_encoded_imputed_test.drop(columns=['Sale_Price'])),\n",
        "                                columns=housing_data_encoded_imputed_test.drop(columns=['Sale_Price']).columns)\n",
        "    X_tst_scaled_selected = X_tst_scaled[selected_features]\n",
        "    print(\"X_tst_scaled_selected shape\", X_tst_scaled_selected.shape)\n",
        "\n",
        "    # Make predictions on the test set using the retrained Lasso model\n",
        "    y_tst_pred_log = lasso_model.predict(X_tst_scaled_selected)\n",
        "\n",
        "    # Reverse the log-transformation to get predictions in the original scale\n",
        "    y_tst_pred = np.exp(y_tst_pred_log)\n",
        "\n",
        "    # Merge the predictions with actual sale prices from test_y.csv using 'PID'\n",
        "    predictions = pd.DataFrame({\n",
        "        'PID': merged_test_data['PID'],\n",
        "        'Predicted_Sale_Price': y_tst_pred\n",
        "    })\n",
        "    merged_test_data = pd.merge(test_y_data, predictions, on='PID', how='inner')\n",
        "    print(\"predictions\", predictions)\n",
        "    print(\"merged_test_data\", merged_test_data)\n",
        "    # Calculate RMSE and R-squared for the test set using actual sale prices\n",
        "    test_rmse = calc_rmse(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    r2_test = r2_score(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    adj_r2_test = adjusted_r_squared(r2_test, len(merged_test_data), X_tst_scaled_selected.shape[1])\n",
        "    print(f\"Test RMSE: {test_rmse}\")\n",
        "    print(f\"Test R-squared: {r2_test}\")\n",
        "    print(f\"Adjusted R-squared (Test): {adj_r2_test}\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jXeV9-ZMV28",
        "outputId": "1c2805dd-6767-420e-fb95-6096c781d6ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "housing_data_encoded_train shape (2051, 307)\n",
            "housing_data_encoded_test shape (879, 307)\n",
            "housing_data_encoded_imputed_train shape (2051, 307)\n",
            "housing_data_encoded_imputed_test shape (879, 307)\n",
            "Max z-score: 35.19411986086601\n",
            "Mean z-score: 0.6935495391681232\n",
            "sale_price_corr Sale_Price           1.000000\n",
            "Gr_Liv_Area          0.711530\n",
            "Garage_Cars          0.656445\n",
            "Total_Bsmt_SF        0.655571\n",
            "Garage_Area          0.655343\n",
            "                       ...   \n",
            "Misc_Feature_Gar2         NaN\n",
            "Functional_Sev            NaN\n",
            "Condition_2_RRAe          NaN\n",
            "Utilities_NoSewr          NaN\n",
            "Roof_Matl_Metal           NaN\n",
            "Name: Sale_Price, Length: 306, dtype: float64\n",
            "Data shape after outlier removal2: (2014, 306)\n",
            "X_trn shape (2014, 305)\n",
            "y_trn shape (2014,)\n",
            "Best Lasso alpha: 0.001\n",
            "lasso_model.coef_ [-1.55906679e-02 -0.00000000e+00 -3.18244405e-03 -0.00000000e+00\n",
            "  6.88630775e-03 -3.41084691e-03  1.55923561e-02 -3.18776247e-02\n",
            "  0.00000000e+00 -0.00000000e+00  3.47607830e-03  2.72810454e-03\n",
            " -0.00000000e+00 -7.47457756e-03 -6.16076594e-04 -4.68062602e-04\n",
            " -2.37008765e-02  5.03136695e-02 -3.56062567e-04  6.21732451e-03\n",
            " -2.93334647e-03 -6.76615996e-03  1.29172448e-03 -1.10118806e-02\n",
            "  2.08105709e-02 -2.11747611e-03  2.47849716e-04 -1.13803193e-03\n",
            " -2.34105554e-03 -5.82309814e-03  1.58361045e-02 -1.29713640e-04\n",
            " -1.30259159e-03  2.15964642e-03  0.00000000e+00 -9.40476364e-03\n",
            " -5.41163055e-03  1.07778835e-02  2.93331332e-04  3.54959451e-03\n",
            " -1.16686579e-02  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
            "  0.00000000e+00  4.83634856e-03  1.17278439e-02 -0.00000000e+00\n",
            " -5.11899056e-04  0.00000000e+00  3.55352650e-03 -0.00000000e+00\n",
            "  2.79499148e-02  0.00000000e+00  0.00000000e+00  9.19170754e-03\n",
            " -3.97766367e-03 -1.27499980e-03  4.73367224e-03  0.00000000e+00\n",
            " -0.00000000e+00 -8.02464718e-03 -2.17806241e-03 -1.57883185e-03\n",
            " -2.65316862e-02 -0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00 -6.95985794e-03  3.88269721e-03  0.00000000e+00\n",
            "  5.14514944e-03  7.36112922e-04 -1.22232620e-03  0.00000000e+00\n",
            "  0.00000000e+00  1.87456945e-02  0.00000000e+00 -4.84189782e-04\n",
            " -2.13780226e-04  0.00000000e+00 -1.61617641e-02  9.15129669e-03\n",
            " -1.33909478e-03 -1.38627392e-02  0.00000000e+00  3.12274849e-03\n",
            " -1.99225415e-03  3.60960451e-03  5.18082634e-03  0.00000000e+00\n",
            "  9.64960687e-03  1.54350768e-18  0.00000000e+00  3.02244301e-04\n",
            " -0.00000000e+00  0.00000000e+00 -1.80288726e-03  0.00000000e+00\n",
            " -0.00000000e+00 -7.72894835e-04  2.78636386e-03 -1.12930872e-03\n",
            " -0.00000000e+00 -1.00790967e-03  0.00000000e+00  9.40372935e-04\n",
            "  6.50584051e-03  3.82149152e-03 -6.87048324e-04  0.00000000e+00\n",
            "  1.31539645e-02  2.84810805e-05  1.32727059e-02  2.06462847e-02\n",
            "  0.00000000e+00  0.00000000e+00 -4.40279175e-05  8.99463240e-04\n",
            " -1.13429695e-02  0.00000000e+00  8.46710200e-04  1.55359204e-03\n",
            "  3.27585652e-03  0.00000000e+00  1.75598341e-02  3.34529468e-03\n",
            " -2.03781116e-02  0.00000000e+00  4.23031590e-04 -1.47067467e-04\n",
            "  2.49022269e-04 -0.00000000e+00 -7.27473798e-03 -1.94829963e-03\n",
            " -6.19262158e-03 -0.00000000e+00  2.91594894e-03 -5.67280760e-04\n",
            " -4.26674262e-03 -1.18886441e-03 -1.32539835e-02  0.00000000e+00\n",
            " -0.00000000e+00 -2.00225492e-04 -0.00000000e+00 -0.00000000e+00\n",
            "  0.00000000e+00  7.13182514e-05  5.81430337e-05 -0.00000000e+00\n",
            " -6.98331714e-04 -1.36656144e-03  1.32240678e-02  2.78106715e-03\n",
            "  0.00000000e+00 -6.52624360e-03  0.00000000e+00  4.11145499e-03\n",
            "  0.00000000e+00 -2.24718048e-03  1.11799769e-02  0.00000000e+00\n",
            "  0.00000000e+00  7.88588793e-03  3.57746692e-03 -1.12914068e-03\n",
            " -0.00000000e+00  0.00000000e+00  1.76897237e-03 -0.00000000e+00\n",
            " -1.48164280e-02  1.69156883e-02 -0.00000000e+00 -2.83388741e-03\n",
            "  0.00000000e+00 -3.84897190e-03 -0.00000000e+00 -4.09027965e-03\n",
            "  4.73205943e-05  5.39624190e-03  7.37552506e-04 -2.65554194e-03\n",
            "  1.73603163e-02  1.03112227e-02 -0.00000000e+00  4.94697809e-03\n",
            "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -1.11248129e-03\n",
            "  4.13801645e-02  0.00000000e+00  1.98839698e-02  2.63756760e-03\n",
            "  1.65290967e-03  1.09091990e-02 -1.23098778e-02  1.11602854e-03\n",
            " -1.79836787e-02 -1.82284668e-03 -0.00000000e+00  2.54442888e-03\n",
            " -1.19519985e-04  3.60662264e-03  1.29260288e-02 -3.30435970e-03\n",
            "  0.00000000e+00 -4.63180901e-03  9.24810493e-03  1.05408101e-02\n",
            "  3.07597210e-03  3.37693823e-03  2.52425926e-02 -7.12851778e-04\n",
            "  0.00000000e+00  1.86023636e-02  0.00000000e+00 -0.00000000e+00\n",
            " -3.85371785e-03 -1.85377312e-03  2.40650917e-03 -0.00000000e+00\n",
            "  0.00000000e+00 -4.53812546e-04 -7.12883940e-03 -1.80645029e-03\n",
            "  0.00000000e+00 -2.48403499e-04 -0.00000000e+00 -0.00000000e+00\n",
            "  0.00000000e+00 -0.00000000e+00 -1.46891482e-02 -3.31310153e-03\n",
            "  3.73466425e-03  1.12195092e-03 -2.44438149e-02 -8.34548457e-04\n",
            "  4.40774816e-03  1.06589674e-01  7.76239075e-03 -1.34703699e-02\n",
            " -1.88866370e-02  1.47867076e-02  5.19261083e-03  1.20690737e-02\n",
            "  5.77917873e-03  0.00000000e+00  2.43822182e-02  2.71629535e-03\n",
            " -4.84452926e-03 -3.54233403e-03  3.02080170e-03 -5.80362561e-06\n",
            " -5.86272268e-03  1.33594019e-03 -3.83416426e-04  0.00000000e+00\n",
            " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00  5.30420030e-03\n",
            " -4.46554142e-03  1.97109963e-03 -0.00000000e+00 -7.30256527e-03\n",
            " -9.92048540e-04 -7.37030277e-04  1.42149075e-03  0.00000000e+00\n",
            " -1.89384959e-03  4.62233433e-03  5.64682865e-02  0.00000000e+00\n",
            " -2.47168688e-03  0.00000000e+00 -1.94229036e-03 -7.79297294e-03\n",
            " -2.86016393e-03  4.73068375e-03 -0.00000000e+00  4.85520828e-04\n",
            "  5.97831837e-03  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
            " -2.00373741e-02 -1.95635382e-03 -1.26515381e-03 -1.02266904e-05\n",
            "  1.68374358e-02 -3.37110539e-03  0.00000000e+00  0.00000000e+00\n",
            " -2.82025996e-03  1.88607671e-02  0.00000000e+00 -4.35081030e-03\n",
            " -0.00000000e+00 -0.00000000e+00 -2.51599787e-03 -9.66106540e-04\n",
            "  0.00000000e+00]\n",
            "Selected features after tuned Lasso: 209 features\n",
            "Training RMSE: 0.0994989373511673\n",
            "X_tst_scaled_selected shape (879, 209)\n",
            "predictions            PID  Predicted_Sale_Price\n",
            "0    534152120         162155.112984\n",
            "1    907202220         132742.187231\n",
            "2    534430090         130108.361606\n",
            "3    535126100         119695.886924\n",
            "4    528445070         242103.895269\n",
            "..         ...                   ...\n",
            "874  527402150         138259.134098\n",
            "875  535456070         149433.258171\n",
            "876  908102260         149568.967053\n",
            "877  923228370          92916.912684\n",
            "878  528250040         185335.091705\n",
            "\n",
            "[879 rows x 2 columns]\n",
            "merged_test_data            PID  Sale_Price  Predicted_Sale_Price\n",
            "0    534152120      167900         162155.112984\n",
            "1    907202220      120000         132742.187231\n",
            "2    534430090      124000         130108.361606\n",
            "3    535126100      110000         119695.886924\n",
            "4    528445070      221300         242103.895269\n",
            "..         ...         ...                   ...\n",
            "874  527402150      143250         138259.134098\n",
            "875  535456070      150000         149433.258171\n",
            "876  908102260      147500         149568.967053\n",
            "877  923228370       85500          92916.912684\n",
            "878  528250040      178000         185335.091705\n",
            "\n",
            "[879 rows x 3 columns]\n",
            "Test RMSE: 0.14120110544706202\n",
            "Test R-squared: 0.8643951265928276\n",
            "Adjusted R-squared (Test): 0.8220312722698095\n",
            "X_train shape (2014, 209)\n",
            "y_train shape (2014,)\n",
            "X_test shape (879, 209)\n",
            "y_test shape (879,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "loop through folders 1 to 10"
      ],
      "metadata": {
        "id": "1iG7lFcI1Ism"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy import stats  # Importing for z-score calculation\n",
        "\n",
        "# Helper functions\n",
        "def preprocess_data(X_trn):\n",
        "    # Initialize the StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit the scaler on the training data and transform it\n",
        "    X_trn_scaled = scaler.fit_transform(X_trn)\n",
        "\n",
        "    # Return the scaled training data as a DataFrame (to preserve column names) and the scaler object\n",
        "    X_trn_scaled_df = pd.DataFrame(X_trn_scaled, columns=X_trn.columns)\n",
        "\n",
        "    return X_trn_scaled_df, scaler\n",
        "\n",
        "def calc_rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def adjusted_r_squared(r2, n, p):\n",
        "    # Calculate adjusted R-squared\n",
        "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "\n",
        "def main():\n",
        "    # Loop over folds 1 to 10\n",
        "    for fold_num in range(1, 11):\n",
        "        print(f\"\\nProcessing fold{fold_num}...\")\n",
        "\n",
        "        # Load the train, test, and test_y datasets\n",
        "        train_file_path = f'fold{fold_num}/train.csv'\n",
        "        test_file_path = f'fold{fold_num}/test.csv'\n",
        "        test_y_file_path = f'fold{fold_num}/test_y.csv'\n",
        "        housing_data_train = pd.read_csv(train_file_path)\n",
        "        housing_data_test = pd.read_csv(test_file_path)\n",
        "        test_y_data = pd.read_csv(test_y_file_path)  # Load the actual Sale Price for the test set\n",
        "\n",
        "        # Ensure that test_y_data and housing_data_test have matching PIDs\n",
        "        merged_test_data = pd.merge(housing_data_test, test_y_data, on='PID', how='inner')\n",
        "\n",
        "        # Identify categorical columns\n",
        "        categorical_columns = ['MS_SubClass', 'MS_Zoning', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities',\n",
        "                               'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type',\n",
        "                               'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style', 'Roof_Matl', 'Exterior_1st',\n",
        "                               'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual', 'Exter_Cond', 'Foundation', 'Bsmt_Qual',\n",
        "                               'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating', 'Heating_QC',\n",
        "                               'Central_Air', 'Electrical', 'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type',\n",
        "                               'Garage_Finish', 'Garage_Qual', 'Garage_Cond', 'Paved_Drive', 'Pool_QC', 'Fence',\n",
        "                               'Misc_Feature', 'Sale_Type', 'Sale_Condition']\n",
        "\n",
        "        # Perform dummy encoding on both train and test datasets\n",
        "        housing_data_encoded_train = pd.get_dummies(housing_data_train, columns=categorical_columns, drop_first=True)\n",
        "        housing_data_encoded_test = pd.get_dummies(merged_test_data, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "        # Align columns of train and test after dummy encoding:\n",
        "        all_columns = list(set(housing_data_encoded_train.columns) | set(housing_data_encoded_test.columns))\n",
        "        if 'Sale_Price' in all_columns:\n",
        "            all_columns.remove('Sale_Price')  # Remove 'Sale_Price' if it's in all_columns\n",
        "\n",
        "        housing_data_encoded_train = housing_data_encoded_train.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "        housing_data_encoded_test = housing_data_encoded_test.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "        print(\"housing_data_encoded_train shape\", housing_data_encoded_train.shape)\n",
        "        print(\"housing_data_encoded_test shape\", housing_data_encoded_test.shape)\n",
        "\n",
        "        # Handle missing values by imputing them in train and test data\n",
        "        imputer = SimpleImputer(strategy='median')\n",
        "        numeric_columns = housing_data_encoded_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        # Ensure 'Sale_Price' and 'PID' are excluded from numeric_columns\n",
        "        numeric_columns = [col for col in numeric_columns if col not in ['Sale_Price', 'PID']]\n",
        "\n",
        "        housing_data_encoded_imputed_train = housing_data_encoded_train.copy()\n",
        "        housing_data_encoded_imputed_train[numeric_columns] = imputer.fit_transform(housing_data_encoded_train[numeric_columns])\n",
        "\n",
        "        housing_data_encoded_imputed_test = housing_data_encoded_test.copy()\n",
        "        housing_data_encoded_imputed_test[numeric_columns] = imputer.transform(housing_data_encoded_test[numeric_columns])\n",
        "        print(\"housing_data_encoded_imputed_train shape\", housing_data_encoded_imputed_train.shape)\n",
        "        print(\"housing_data_encoded_imputed_test shape\", housing_data_encoded_imputed_test.shape)\n",
        "\n",
        "        # Safely drop 'PID' if it exists\n",
        "        if 'PID' in housing_data_encoded_imputed_train.columns:\n",
        "            housing_data_encoded_imputed_train = housing_data_encoded_imputed_train.drop(columns=['PID'])\n",
        "        if 'PID' in housing_data_encoded_imputed_test.columns:\n",
        "            housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.drop(columns=['PID'])\n",
        "\n",
        "        # **Handling outliers using z-scores**\n",
        "        z_scores = np.abs(stats.zscore(housing_data_encoded_imputed_train[numeric_columns]))\n",
        "\n",
        "        # Check the distribution of z-scores before filtering\n",
        "        print(\"Max z-score:\", np.max(z_scores))\n",
        "        print(\"Mean z-score:\", np.mean(z_scores))\n",
        "        # Convert the z-scores to a DataFrame for easier analysis\n",
        "        z_scores_df = pd.DataFrame(z_scores, columns=numeric_columns)\n",
        "\n",
        "        # Adjust the threshold for filtering outliers\n",
        "        high_outlier_rows = z_scores_df[z_scores_df > 9].dropna(how='all').index\n",
        "        housing_data_cleaned = housing_data_encoded_imputed_train.drop(high_outlier_rows)\n",
        "        print(f\"Data shape after outlier removal: {housing_data_cleaned.shape}\")\n",
        "\n",
        "        # Separate predictors (X) and target variable (y) for training (after cleaning)\n",
        "        X_trn = housing_data_cleaned.drop(columns='Sale_Price')\n",
        "        y_trn = np.log(housing_data_cleaned['Sale_Price'])\n",
        "        print(\"X_trn shape\", X_trn.shape)\n",
        "        print(\"y_trn shape\", y_trn.shape)\n",
        "\n",
        "        # Scale the training data using preprocess_data function (returns DataFrame with column names)\n",
        "        X_trn_scaled, scaler = preprocess_data(X_trn)\n",
        "\n",
        "        # Tune Lasso alpha\n",
        "        #lasso_param_grid = {'alpha': [0.0009, 0.001, 0.01, 0.1, 1.0]}\n",
        "\n",
        "        # Generate 100 alpha values using np.exp and np.linspace\n",
        "        alpha_values = np.exp(np.linspace(-5, 5, 100))\n",
        "\n",
        "        # Set the alpha values in the parameter grid for Lasso\n",
        "        lasso_param_grid = {'alpha': alpha_values}\n",
        "\n",
        "\n",
        "        lasso_grid_search = GridSearchCV(Lasso(), lasso_param_grid, cv=8, scoring='neg_mean_squared_error')\n",
        "        lasso_grid_search.fit(X_trn_scaled, y_trn)\n",
        "\n",
        "        best_lasso_alpha = lasso_grid_search.best_params_['alpha']\n",
        "        print(\"Best Lasso alpha:\", best_lasso_alpha)\n",
        "\n",
        "        lasso_model = Lasso(alpha=best_lasso_alpha)\n",
        "        lasso_model.fit(X_trn_scaled, y_trn)\n",
        "        threshold = 1e-01  # Adjust this threshold as needed\n",
        "\n",
        "        # Select features based on non-zero coefficients\n",
        "        selected_features = X_trn_scaled.columns[lasso_model.coef_ != 0]\n",
        "        print(f\"Selected features after tuned Lasso: {len(selected_features)} features\")\n",
        "\n",
        "        # **Retrain the Lasso model on the selected features**\n",
        "        X_trn_scaled_selected = X_trn_scaled[selected_features]\n",
        "        lasso_model.fit(X_trn_scaled_selected, y_trn)\n",
        "\n",
        "        # Make predictions on the training set using the Lasso model\n",
        "        y_trn_pred_log = lasso_model.predict(X_trn_scaled_selected)\n",
        "\n",
        "        # Calculate RMSE for the training set\n",
        "        train_rmse = calc_rmse(y_trn, y_trn_pred_log)\n",
        "        print(f\"Training RMSE: {train_rmse}\")\n",
        "\n",
        "        # Scale the test data using the same scaler and select only the features used by the Lasso model\n",
        "        X_tst_scaled = pd.DataFrame(scaler.transform(housing_data_encoded_imputed_test.drop(columns=['Sale_Price'])),\n",
        "                                    columns=housing_data_encoded_imputed_test.drop(columns=['Sale_Price']).columns)\n",
        "        X_tst_scaled_selected = X_tst_scaled[selected_features]\n",
        "        print(\"X_tst_scaled_selected shape\", X_tst_scaled_selected.shape)\n",
        "\n",
        "        # Make predictions on the test set using the retrained Lasso model\n",
        "        y_tst_pred_log = lasso_model.predict(X_tst_scaled_selected)\n",
        "\n",
        "        # Reverse the log-transformation to get predictions in the original scale\n",
        "        y_tst_pred = np.exp(y_tst_pred_log)\n",
        "\n",
        "        # Merge the predictions with actual sale prices from test_y.csv using 'PID'\n",
        "        predictions = pd.DataFrame({\n",
        "            'PID': merged_test_data['PID'],\n",
        "            'Predicted_Sale_Price': y_tst_pred\n",
        "        })\n",
        "        merged_test_data = pd.merge(test_y_data, predictions, on='PID', how='inner')\n",
        "\n",
        "        # Calculate RMSE and R-squared for the test set using actual sale prices\n",
        "        test_rmse = calc_rmse(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "        r2_test = r2_score(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "        adj_r2_test = adjusted_r_squared(r2_test, len(merged_test_data), X_tst_scaled_selected.shape[1])\n",
        "        print(f\"Test RMSE: {test_rmse}\")\n",
        "        print(f\"Test R-squared: {r2_test}\")\n",
        "        print(f\"Adjusted R-squared (Test): {adj_r2_test}\")\n",
        "\n",
        "        print(f\"Finished processing fold{fold_num}.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cLc52PMY1H19",
        "outputId": "5b0c9fb0-1bea-4c6c-c02d-ce4a801054a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing fold1...\n",
            "housing_data_encoded_train shape (2051, 307)\n",
            "housing_data_encoded_test shape (879, 307)\n",
            "housing_data_encoded_imputed_train shape (2051, 307)\n",
            "housing_data_encoded_imputed_test shape (879, 307)\n",
            "Max z-score: 34.517747117391295\n",
            "Mean z-score: 0.6924108037936196\n",
            "Data shape after outlier removal: (2018, 306)\n",
            "X_trn shape (2018, 305)\n",
            "y_trn shape (2018,)\n",
            "Best Lasso alpha: 0.006737946999085467\n",
            "Selected features after tuned Lasso: 76 features\n",
            "Training RMSE: 0.11357932470984648\n",
            "X_tst_scaled_selected shape (879, 76)\n",
            "Test RMSE: 0.1602642424774064\n",
            "Test R-squared: 0.8271227125929487\n",
            "Adjusted R-squared (Test): 0.8107403262551234\n",
            "Finished processing fold1.\n",
            "\n",
            "Processing fold2...\n",
            "housing_data_encoded_train shape (2051, 307)\n",
            "housing_data_encoded_test shape (879, 307)\n",
            "housing_data_encoded_imputed_train shape (2051, 307)\n",
            "housing_data_encoded_imputed_test shape (879, 307)\n",
            "Max z-score: 29.346142622362727\n",
            "Mean z-score: 0.6888063405058934\n",
            "Data shape after outlier removal: (2013, 306)\n",
            "X_trn shape (2013, 305)\n",
            "y_trn shape (2013,)\n",
            "Best Lasso alpha: 0.006737946999085467\n",
            "Selected features after tuned Lasso: 77 features\n",
            "Training RMSE: 0.11120920437005115\n",
            "X_tst_scaled_selected shape (879, 77)\n",
            "Test RMSE: 0.1424887982430395\n",
            "Test R-squared: 0.8721991219153535\n",
            "Adjusted R-squared (Test): 0.8599136442467921\n",
            "Finished processing fold2.\n",
            "\n",
            "Processing fold3...\n",
            "housing_data_encoded_train shape (2051, 307)\n",
            "housing_data_encoded_test shape (879, 307)\n",
            "housing_data_encoded_imputed_train shape (2051, 307)\n",
            "housing_data_encoded_imputed_test shape (879, 307)\n",
            "Max z-score: 35.19411986086601\n",
            "Mean z-score: 0.6935495391681231\n",
            "Data shape after outlier removal: (2014, 306)\n",
            "X_trn shape (2014, 305)\n",
            "y_trn shape (2014,)\n",
            "Best Lasso alpha: 0.006737946999085467\n",
            "Selected features after tuned Lasso: 84 features\n",
            "Training RMSE: 0.11454289259438138\n",
            "X_tst_scaled_selected shape (879, 84)\n",
            "Test RMSE: 0.14230173641736793\n",
            "Test R-squared: 0.8622728682483127\n",
            "Adjusted R-squared (Test): 0.8477022397002753\n",
            "Finished processing fold3.\n",
            "\n",
            "Processing fold4...\n",
            "housing_data_encoded_train shape (2051, 306)\n",
            "housing_data_encoded_test shape (879, 306)\n",
            "housing_data_encoded_imputed_train shape (2051, 306)\n",
            "housing_data_encoded_imputed_test shape (879, 306)\n",
            "Max z-score: 28.548258742761885\n",
            "Mean z-score: 0.6927870893808528\n",
            "Data shape after outlier removal: (2016, 305)\n",
            "X_trn shape (2016, 304)\n",
            "y_trn shape (2016,)\n",
            "Best Lasso alpha: 0.006737946999085467\n",
            "Selected features after tuned Lasso: 83 features\n",
            "Training RMSE: 0.11446289970138745\n",
            "X_tst_scaled_selected shape (879, 83)\n",
            "Test RMSE: 0.15742840290523555\n",
            "Test R-squared: 0.8501629180392627\n",
            "Adjusted R-squared (Test): 0.8345195497339278\n",
            "Finished processing fold4.\n",
            "\n",
            "Processing fold5...\n",
            "housing_data_encoded_train shape (2051, 307)\n",
            "housing_data_encoded_test shape (879, 307)\n",
            "housing_data_encoded_imputed_train shape (2051, 307)\n",
            "housing_data_encoded_imputed_test shape (879, 307)\n",
            "Max z-score: 30.22393751958268\n",
            "Mean z-score: 0.6919397746498371\n",
            "Data shape after outlier removal: (2020, 306)\n",
            "X_trn shape (2020, 305)\n",
            "y_trn shape (2020,)\n",
            "Best Lasso alpha: 0.006737946999085467\n",
            "Selected features after tuned Lasso: 81 features\n",
            "Training RMSE: 0.1144446801533149\n",
            "X_tst_scaled_selected shape (879, 81)\n",
            "Test RMSE: 0.13213521420581578\n",
            "Test R-squared: 0.8815724476535931\n",
            "Adjusted R-squared (Test): 0.8695365232620511\n",
            "Finished processing fold5.\n",
            "\n",
            "Processing fold6...\n",
            "housing_data_encoded_train shape (2051, 307)\n",
            "housing_data_encoded_test shape (879, 307)\n",
            "housing_data_encoded_imputed_train shape (2051, 307)\n",
            "housing_data_encoded_imputed_test shape (879, 307)\n",
            "Max z-score: 34.5177471173913\n",
            "Mean z-score: 0.6924797045657644\n",
            "Data shape after outlier removal: (2018, 306)\n",
            "X_trn shape (2018, 305)\n",
            "y_trn shape (2018,)\n",
            "Best Lasso alpha: 0.006737946999085467\n",
            "Selected features after tuned Lasso: 73 features\n",
            "Training RMSE: 0.11376954835037209\n",
            "X_tst_scaled_selected shape (879, 73)\n",
            "Test RMSE: 0.17097312982773308\n",
            "Test R-squared: 0.812349560917097\n",
            "Adjusted R-squared (Test): 0.7953328130251069\n",
            "Finished processing fold6.\n",
            "\n",
            "Processing fold7...\n",
            "housing_data_encoded_train shape (2051, 307)\n",
            "housing_data_encoded_test shape (879, 307)\n",
            "housing_data_encoded_imputed_train shape (2051, 307)\n",
            "housing_data_encoded_imputed_test shape (879, 307)\n",
            "Max z-score: 29.346142622362727\n",
            "Mean z-score: 0.6887910554962138\n",
            "Data shape after outlier removal: (2013, 306)\n",
            "X_trn shape (2013, 305)\n",
            "y_trn shape (2013,)\n",
            "Best Lasso alpha: 0.006737946999085467\n",
            "Selected features after tuned Lasso: 74 features\n",
            "Training RMSE: 0.1104162079508371\n",
            "X_tst_scaled_selected shape (879, 74)\n",
            "Test RMSE: 0.1552085665232889\n",
            "Test R-squared: 0.8549800058397776\n",
            "Adjusted R-squared (Test): 0.841632394436971\n",
            "Finished processing fold7.\n",
            "\n",
            "Processing fold8...\n",
            "housing_data_encoded_train shape (2051, 307)\n",
            "housing_data_encoded_test shape (879, 307)\n",
            "housing_data_encoded_imputed_train shape (2051, 307)\n",
            "housing_data_encoded_imputed_test shape (879, 307)\n",
            "Max z-score: 35.19411986086601\n",
            "Mean z-score: 0.6935865270287536\n",
            "Data shape after outlier removal: (2014, 306)\n",
            "X_trn shape (2014, 305)\n",
            "y_trn shape (2014,)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-43ad08078156>\u001b[0m in \u001b[0;36m<cell line: 177>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-43ad08078156>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mlasso_grid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLasso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlasso_param_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neg_mean_squared_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mlasso_grid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trn_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mbest_lasso_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlasso_grid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alpha'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    963\u001b[0m                     )\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    966\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    967\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         )\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    886\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m             \u001b[0mX_copied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m             X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    981\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1316\u001b[0m     )\n\u001b[1;32m   1317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1318\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;34m\"\"\"Isolated part of check_X_y dedicated to y validation\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m         y = check_array(\n\u001b[0m\u001b[1;32m   1329\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1010\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6294\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6296\u001b[0;31m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_can_hold_identifiers_and_holds_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5447\u001b[0m         if (\n\u001b[1;32m   5448\u001b[0m             \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5449\u001b[0;31m             \u001b[0;32mor\u001b[0m \u001b[0mis_string_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5450\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5451\u001b[0m         ):\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_string_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_is_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36m_is_dtype\u001b[0;34m(arr_or_dtype, condition)\u001b[0m\n\u001b[1;32m   1394\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1395\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1396\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mcondition\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mis_string_or_object_np_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "result = np.exp(np.linspace(-5, 5, 100))\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMW_dIPU2JVG",
        "outputId": "d54dfb6e-fe3f-4101-af08-0e84533946b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6.73794700e-03 7.45410867e-03 8.24638961e-03 9.12288035e-03\n",
            " 1.00925314e-02 1.11652445e-02 1.23519740e-02 1.36648383e-02\n",
            " 1.51172441e-02 1.67240230e-02 1.85015829e-02 2.04680757e-02\n",
            " 2.26435828e-02 2.50503198e-02 2.77128635e-02 3.06584033e-02\n",
            " 3.39170180e-02 3.75219838e-02 4.15101135e-02 4.59221329e-02\n",
            " 5.08030961e-02 5.62028462e-02 6.21765240e-02 6.87851310e-02\n",
            " 7.60961524e-02 8.41842462e-02 9.31320059e-02 1.03030803e-01\n",
            " 1.13981723e-01 1.26096591e-01 1.39499122e-01 1.54326179e-01\n",
            " 1.70729172e-01 1.88875603e-01 2.08950778e-01 2.31159700e-01\n",
            " 2.55729160e-01 2.82910054e-01 3.12979946e-01 3.46245901e-01\n",
            " 3.83047621e-01 4.23760917e-01 4.68801539e-01 5.18629431e-01\n",
            " 5.73753421e-01 6.34736419e-01 7.02201167e-01 7.76836596e-01\n",
            " 8.59404861e-01 9.50749127e-01 1.05180218e+00 1.16359593e+00\n",
            " 1.28727200e+00 1.42409333e+00 1.57545710e+00 1.74290900e+00\n",
            " 1.92815899e+00 2.13309880e+00 2.35982121e+00 2.61064146e+00\n",
            " 2.88812084e+00 3.19509289e+00 3.53469234e+00 3.91038707e+00\n",
            " 4.32601357e+00 4.78581611e+00 5.29449005e+00 5.85722984e+00\n",
            " 6.47978201e+00 7.16850389e+00 7.93042851e+00 8.77333644e+00\n",
            " 9.70583521e+00 1.07374472e+01 1.18787071e+01 1.31412689e+01\n",
            " 1.45380257e+01 1.60832407e+01 1.77926932e+01 1.96838397e+01\n",
            " 2.17759920e+01 2.40905147e+01 2.66510429e+01 2.94837240e+01\n",
            " 3.26174847e+01 3.60843259e+01 3.99196501e+01 4.41626226e+01\n",
            " 4.88565713e+01 5.40494295e+01 5.97942254e+01 6.61496230e+01\n",
            " 7.31805220e+01 8.09587199e+01 8.95636455e+01 9.90831698e+01\n",
            " 1.09614504e+02 1.21265190e+02 1.34154202e+02 1.48413159e+02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(PSL) Random Forest for Regression\n"
      ],
      "metadata": {
        "id": "gTFbs6e-CJPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the train, test, and test_y datasets\n",
        "train_file_path = 'fold2/train.csv'\n",
        "test_file_path = 'fold2/test.csv'\n",
        "test_y_file_path = 'fold2/test_y.csv'\n",
        "housing_data_train = pd.read_csv(train_file_path)\n",
        "housing_data_test = pd.read_csv(test_file_path)\n",
        "test_y_data = pd.read_csv(test_y_file_path)  # Load the actual Sale Price for the test set\n",
        "\n",
        "# Create X_train by removing the SalePrice column from housing_data_train\n",
        "X_train = housing_data_train.drop('Sale_Price', axis=1)\n",
        "\n",
        "# Create Y_train by selecting only the SalePrice column from housing_data_train\n",
        "y_train = np.log(housing_data_train['Sale_Price'])\n",
        "print(\"X_train shape\", X_train.shape)\n",
        "print(\"y_train shape\", y_train.shape)\n",
        "\n",
        "# Create X_test by removing the SalePrice column from housing_data_test\n",
        "X_test = housing_data_test\n",
        "\n",
        "# Create y_test by selecting only the SalePrice column from the test_y_data\n",
        "y_test = test_y_data['Sale_Price']\n",
        "\n",
        "print(\"X_train shape\", X_train.shape)\n",
        "print(\"y_train shape\", y_train.shape)\n",
        "print(\"X_test shape\", X_test.shape)\n",
        "print(\"y_test shape\", y_test.shape)\n",
        "\n",
        "#print(\"X_train \", X_train)\n",
        "#print(\"y_train \", y_train)\n",
        "#print(\"X_test \", X_test)\n",
        "#print(\"y_test \", y_test)\n",
        "\n",
        "# Apply one-hot encoding to both X_train and X_test\n",
        "X_train = pd.get_dummies(X_train)\n",
        "X_test = pd.get_dummies(X_test)\n",
        "\n",
        "# Align the columns of X_train and X_test so they have the same structure\n",
        "X_train, X_test = X_train.align(X_test, join='left', axis=1)\n",
        "\n",
        "# Fill any missing columns in X_test (which may occur due to one-hot encoding) with 0\n",
        "X_test = X_test.fillna(0)\n",
        "\n",
        "\n",
        "\n",
        "print(\"X_train shape\", X_train.shape)\n",
        "print(\"y_train shape\", y_train.shape)\n",
        "print(\"X_test shape\", X_test.shape)\n",
        "print(\"y_test shape\", y_test.shape)\n",
        "\n",
        "#print(\"X_train \", X_train)\n",
        "#print(\"y_train \", y_train)\n",
        "#print(\"X_test \", X_test)\n",
        "#print(\"y_test \", y_test)\n",
        "\n",
        "rfModel = RandomForestRegressor(n_estimators=400, oob_score=True, max_features = 1.0/3)\n",
        "rfModel.fit(X_train, y_train);\n",
        "\n",
        "yhat = rfModel.predict(X_test)\n",
        "#print (\"yhat\", yhat)\n",
        "#print (\"y_test\", np.log(y_test))\n",
        "np.mean((yhat - np.log(y_test)) ** 2.0)\n",
        "\n",
        "yhat_train = rfModel.predict(X_train)\n",
        "np.mean((yhat_train - y_train) ** 2.0)\n",
        "\n",
        "yhat_train = rfModel.oob_prediction_\n",
        "np.mean((yhat_train - y_train) ** 2.0)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Calculate RMSE for test data\n",
        "yhat_test = rfModel.predict(X_test)\n",
        "test_rmse = np.sqrt(mean_squared_error(np.log(y_test), yhat_test))\n",
        "print(f\"Test RMSE: {test_rmse}\")\n",
        "\n",
        "# Calculate RMSE for training data (using the out-of-bag predictions)\n",
        "yhat_train_oob = rfModel.oob_prediction_\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, yhat_train_oob))\n",
        "print(f\"Train RMSE (OOB): {train_rmse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ayvm9wG2CM-N",
        "outputId": "a217306d-21d2-4644-d9f4-105514610afa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape (2051, 82)\n",
            "y_train shape (2051,)\n",
            "X_train shape (2051, 82)\n",
            "y_train shape (2051,)\n",
            "X_test shape (879, 82)\n",
            "y_test shape (879,)\n",
            "X_train shape (2051, 342)\n",
            "y_train shape (2051,)\n",
            "X_test shape (879, 342)\n",
            "y_test shape (879,)\n",
            "Test RMSE: 0.1348387141480778\n",
            "Train RMSE (OOB): 0.14112957489652594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_features(X, features_to_remove):\n",
        "    \"\"\"\n",
        "    Removes specified features from the DataFrame.\n",
        "\n",
        "    Args:\n",
        "    - X (pd.DataFrame): The original DataFrame.\n",
        "    - features_to_remove (list): List of column names to remove from X.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: DataFrame after removing specified features.\n",
        "    \"\"\"\n",
        "    return X.drop(columns=features_to_remove, errors='ignore')\n"
      ],
      "metadata": {
        "id": "bxKkX_k7IRmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Loop through each fold (from fold1 to fold10)\n",
        "for fold_num in range(1, 11):\n",
        "    print(f\"Processing fold{fold_num}...\")\n",
        "\n",
        "    # Set the paths for the current fold\n",
        "    folder = f'fold{fold_num}'\n",
        "    train_file_path = os.path.join(folder, 'train.csv')\n",
        "    test_file_path = os.path.join(folder, 'test.csv')\n",
        "    test_y_file_path = os.path.join(folder, 'test_y.csv')\n",
        "\n",
        "    # Load the train, test, and test_y datasets\n",
        "    housing_data_train = pd.read_csv(train_file_path)\n",
        "    housing_data_test = pd.read_csv(test_file_path)\n",
        "    test_y_data = pd.read_csv(test_y_file_path)  # Load the actual Sale Price for the test set\n",
        "\n",
        "    # EXperiment starts\n",
        "    # Define the list of features to remove (this is just an example, adjust as needed)\n",
        "    features_to_remove = [\"Longitude\", \"Latitude\", \"Street\", \"Utilities\", \"Condition_2\",\n",
        "                      \"Roof_Matl\", \"Heating\", \"Pool_QC\", \"Misc_Feature\", \"Misc_Val\",\n",
        "                        \"Low_Qual_Fin_SF\", \"Pool_Area\"]  # Specify the features you want to remove\n",
        "    # Call the remove_features function to remove unwanted features from X_trn\n",
        "    housing_data_train = remove_features(housing_data_train, features_to_remove)\n",
        "\n",
        "    # Create X_train by removing the SalePrice column from housing_data_train\n",
        "    X_train = housing_data_train.drop('Sale_Price', axis=1)\n",
        "\n",
        "    # Create y_train by selecting only the SalePrice column from housing_data_train\n",
        "    y_train = np.log(housing_data_train['Sale_Price'])\n",
        "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "\n",
        "    # Create X_test from the housing_data_test\n",
        "    X_test = housing_data_test\n",
        "\n",
        "    # Create y_test by selecting only the SalePrice column from the test_y_data\n",
        "    y_test = test_y_data['Sale_Price']\n",
        "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "    # Apply one-hot encoding to both X_train and X_test\n",
        "    X_train = pd.get_dummies(X_train)\n",
        "    X_test = pd.get_dummies(X_test)\n",
        "\n",
        "    # Align the columns of X_train and X_test so they have the same structure\n",
        "    X_train, X_test = X_train.align(X_test, join='left', axis=1)\n",
        "\n",
        "    # Fill any missing columns in X_test (which may occur due to one-hot encoding) with 0\n",
        "    X_test = X_test.fillna(0)\n",
        "\n",
        "    print(f\"Aligned X_train shape: {X_train.shape}, Aligned X_test shape: {X_test.shape}\")\n",
        "\n",
        "    # Train a RandomForestRegressor\n",
        "    rfModel = RandomForestRegressor(n_estimators=400, oob_score=True, max_features=1.0/3)\n",
        "    rfModel.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    yhat_test = rfModel.predict(X_test)\n",
        "\n",
        "    # Calculate Test RMSE\n",
        "    test_rmse = np.sqrt(mean_squared_error(np.log(y_test), yhat_test))\n",
        "    print(f\"Fold{fold_num} - Test RMSE: {test_rmse}\")\n",
        "\n",
        "    # Calculate Train RMSE (using the out-of-bag predictions)\n",
        "    yhat_train_oob = rfModel.oob_prediction_\n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train, yhat_train_oob))\n",
        "    print(f\"Fold{fold_num} - Train RMSE (OOB): {train_rmse}\")\n",
        "\n",
        "    print(f\"Finished processing fold{fold_num}.\\n\")\n",
        "\n",
        "print(\"Completed processing all folds.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lutFarIowlsz",
        "outputId": "181e12f8-b079-475e-81cf-b8f3eba766e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing fold1...\n",
            "X_train shape: (2051, 70), y_train shape: (2051,)\n",
            "X_test shape: (879, 82), y_test shape: (879,)\n",
            "Aligned X_train shape: (2051, 308), Aligned X_test shape: (879, 308)\n",
            "Fold1 - Test RMSE: 0.1332226049289803\n",
            "Fold1 - Train RMSE (OOB): 0.1420623362462036\n",
            "Finished processing fold1.\n",
            "\n",
            "Processing fold2...\n",
            "X_train shape: (2051, 70), y_train shape: (2051,)\n",
            "X_test shape: (879, 82), y_test shape: (879,)\n",
            "Aligned X_train shape: (2051, 303), Aligned X_test shape: (879, 303)\n",
            "Fold2 - Test RMSE: 0.13470380050557873\n",
            "Fold2 - Train RMSE (OOB): 0.14187081184708014\n",
            "Finished processing fold2.\n",
            "\n",
            "Processing fold3...\n",
            "X_train shape: (2051, 70), y_train shape: (2051,)\n",
            "X_test shape: (879, 82), y_test shape: (879,)\n",
            "Aligned X_train shape: (2051, 309), Aligned X_test shape: (879, 309)\n",
            "Fold3 - Test RMSE: 0.1300871905846696\n",
            "Fold3 - Train RMSE (OOB): 0.1446622900661869\n",
            "Finished processing fold3.\n",
            "\n",
            "Processing fold4...\n",
            "X_train shape: (2051, 70), y_train shape: (2051,)\n",
            "X_test shape: (879, 82), y_test shape: (879,)\n",
            "Aligned X_train shape: (2051, 304), Aligned X_test shape: (879, 304)\n",
            "Fold4 - Test RMSE: 0.13477299915454902\n",
            "Fold4 - Train RMSE (OOB): 0.14450344799355164\n",
            "Finished processing fold4.\n",
            "\n",
            "Processing fold5...\n",
            "X_train shape: (2051, 70), y_train shape: (2051,)\n",
            "X_test shape: (879, 82), y_test shape: (879,)\n",
            "Aligned X_train shape: (2051, 305), Aligned X_test shape: (879, 305)\n",
            "Fold5 - Test RMSE: 0.12500818017862855\n",
            "Fold5 - Train RMSE (OOB): 0.14581709118766986\n",
            "Finished processing fold5.\n",
            "\n",
            "Processing fold6...\n",
            "X_train shape: (2051, 70), y_train shape: (2051,)\n",
            "X_test shape: (879, 82), y_test shape: (879,)\n",
            "Aligned X_train shape: (2051, 307), Aligned X_test shape: (879, 307)\n",
            "Fold6 - Test RMSE: 0.14464737624391705\n",
            "Fold6 - Train RMSE (OOB): 0.13818299726964334\n",
            "Finished processing fold6.\n",
            "\n",
            "Processing fold7...\n",
            "X_train shape: (2051, 70), y_train shape: (2051,)\n",
            "X_test shape: (879, 82), y_test shape: (879,)\n",
            "Aligned X_train shape: (2051, 303), Aligned X_test shape: (879, 303)\n",
            "Fold7 - Test RMSE: 0.1477069395782404\n",
            "Fold7 - Train RMSE (OOB): 0.13671253654363094\n",
            "Finished processing fold7.\n",
            "\n",
            "Processing fold8...\n",
            "X_train shape: (2051, 70), y_train shape: (2051,)\n",
            "X_test shape: (879, 82), y_test shape: (879,)\n",
            "Aligned X_train shape: (2051, 308), Aligned X_test shape: (879, 308)\n",
            "Fold8 - Test RMSE: 0.14409094779376763\n",
            "Fold8 - Train RMSE (OOB): 0.1397232347800909\n",
            "Finished processing fold8.\n",
            "\n",
            "Processing fold9...\n",
            "X_train shape: (2051, 70), y_train shape: (2051,)\n",
            "X_test shape: (879, 82), y_test shape: (879,)\n",
            "Aligned X_train shape: (2051, 301), Aligned X_test shape: (879, 301)\n",
            "Fold9 - Test RMSE: 0.147586903368673\n",
            "Fold9 - Train RMSE (OOB): 0.14053792998098022\n",
            "Finished processing fold9.\n",
            "\n",
            "Processing fold10...\n",
            "X_train shape: (2051, 70), y_train shape: (2051,)\n",
            "X_test shape: (879, 82), y_test shape: (879,)\n",
            "Aligned X_train shape: (2051, 304), Aligned X_test shape: (879, 304)\n",
            "Fold10 - Test RMSE: 0.13608077308202288\n",
            "Fold10 - Train RMSE (OOB): 0.141629300409863\n",
            "Finished processing fold10.\n",
            "\n",
            "Completed processing all folds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding XGBoost below"
      ],
      "metadata": {
        "id": "AxaBZdzP9ZUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Loop through each fold (from fold1 to fold10)\n",
        "for fold_num in range(1, 11):\n",
        "    print(f\"Processing fold{fold_num}...\")\n",
        "    np.random.seed(1390)\n",
        "    # Set the paths for the current fold\n",
        "    folder = f'fold{fold_num}'\n",
        "    train_file_path = os.path.join(folder, 'train.csv')\n",
        "    test_file_path = os.path.join(folder, 'test.csv')\n",
        "    test_y_file_path = os.path.join(folder, 'test_y.csv')\n",
        "\n",
        "    # Load the train, test, and test_y datasets\n",
        "    housing_data_train = pd.read_csv(train_file_path)\n",
        "    housing_data_test = pd.read_csv(test_file_path)\n",
        "    test_y_data = pd.read_csv(test_y_file_path)  # Load the actual Sale Price for the test set\n",
        "\n",
        "    # EXperiment starts\n",
        "    # Define the list of features to remove (this is just an example, adjust as needed)\n",
        "    features_to_remove = [\"Longitude\", \"Latitude\", \"Street\", \"Utilities\", \"Condition_2\",\n",
        "                      \"Roof_Matl\", \"Heating\", \"Pool_QC\", \"Misc_Feature\", \"Misc_Val\",\n",
        "                        \"Low_Qual_Fin_SF\", \"Pool_Area\"]  # Specify the features you want to remove\n",
        "    # Call the remove_features function to remove unwanted features from X_trn\n",
        "    housing_data_train = remove_features(housing_data_train, features_to_remove)\n",
        "\n",
        "    # Create X_train by removing the SalePrice column from housing_data_train\n",
        "    X_train = housing_data_train.drop('Sale_Price', axis=1)\n",
        "\n",
        "    # Create y_train by selecting only the SalePrice column from housing_data_train\n",
        "    y_train = np.log(housing_data_train['Sale_Price'])\n",
        "    #print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "\n",
        "    # Create X_test from the housing_data_test\n",
        "    X_test = housing_data_test\n",
        "\n",
        "    # Create y_test by selecting only the SalePrice column from the test_y_data\n",
        "    y_test = test_y_data['Sale_Price']\n",
        "    #print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "    # Apply one-hot encoding to both X_train and X_test\n",
        "    X_train = pd.get_dummies(X_train)\n",
        "    X_test = pd.get_dummies(X_test)\n",
        "\n",
        "    # Align the columns of X_train and X_test so they have the same structure\n",
        "    X_train, X_test = X_train.align(X_test, join='left', axis=1)\n",
        "\n",
        "    # Fill any missing columns in X_test (which may occur due to one-hot encoding) with 0\n",
        "    X_test = X_test.fillna(0)\n",
        "\n",
        "    #print(f\"Aligned X_train shape: {X_train.shape}, Aligned X_test shape: {X_test.shape}\")\n",
        "\n",
        "    # Train a RandomForestRegressor\n",
        "    #rfModel = RandomForestRegressor(n_estimators=400, oob_score=True, max_features=1.0/3)\n",
        "    #rfModel.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    #yhat_test_rf = rfModel.predict(X_test)\n",
        "\n",
        "    # Calculate Test RMSE for Random Forest\n",
        "    #test_rmse_rf = np.sqrt(mean_squared_error(np.log(y_test), yhat_test_rf))\n",
        "    #print(f\"Fold{fold_num} - Random Forest Test RMSE: {test_rmse_rf}\")\n",
        "\n",
        "    # Calculate Train RMSE for Random Forest (using the out-of-bag predictions)\n",
        "    #yhat_train_oob_rf = rfModel.oob_prediction_\n",
        "    #train_rmse_rf = np.sqrt(mean_squared_error(y_train, yhat_train_oob_rf))\n",
        "    #print(f\"Fold{fold_num} - Random Forest Train RMSE (OOB): {train_rmse_rf}\")\n",
        "\n",
        "    \"\"\"\n",
        "    # Define the model\n",
        "    xgbModel = XGBRegressor(random_state=42)\n",
        "\n",
        "    # Define the parameter grid\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200, 300, 400, 500],  # Number of trees\n",
        "        'learning_rate': [0.01, 0.05, 0.1, 0.2],  # Step size shrinkage\n",
        "        'max_depth': [3, 4, 5, 6],  # Maximum depth of trees\n",
        "        'subsample': [0.6, 0.8, 1.0],  # Fraction of samples to train on\n",
        "        'colsample_bytree': [0.6, 0.8, 1.0],  # Fraction of features used per tree\n",
        "        'gamma': [0, 0.1, 0.2],  # Minimum loss reduction\n",
        "        'reg_alpha': [0, 0.01, 0.1],  # L1 regularization\n",
        "        'reg_lambda': [1, 0.1, 0.01]  # L2 regularization\n",
        "    }\n",
        "\n",
        "    # Randomized search with cross-validation\n",
        "    random_search = RandomizedSearchCV(xgbModel, param_distributions=param_grid,\n",
        "                                      n_iter=50, scoring='neg_root_mean_squared_error',\n",
        "                                      cv=5, verbose=1, random_state=42, n_jobs=-1)\n",
        "\n",
        "    # Fit the random search model\n",
        "    random_search.fit(X_train, y_train)\n",
        "\n",
        "    # Best hyperparameters and RMSE\n",
        "    print(\"Best parameters found: \", random_search.best_params_)\n",
        "    print(\"Best RMSE score: \", -random_search.best_score_)\n",
        "    \"\"\"\n",
        "    # Train an XGBRegressor\n",
        "    #xgbModel = XGBRegressor(n_estimators=400, learning_rate=0.05, max_depth=3, random_state=42)\n",
        "    # Replace with best parameters from RandomizedSearchCV\n",
        "    xgbModel = XGBRegressor(n_estimators=400,\n",
        "                            learning_rate=0.05,\n",
        "                            max_depth=4,\n",
        "                            subsample=0.8,\n",
        "                            colsample_bytree=0.6,\n",
        "                            reg_lambda=0.1,\n",
        "                            reg_alpha=0.01,\n",
        "                            gamma=0,\n",
        "                            random_state=42)\n",
        "    # Accessing best parameters directly from random_search.best_params_\n",
        "    #best_params = random_search.best_params_\n",
        "\n",
        "    # Train the XGBRegressor with the best parameters found from RandomizedSearchCV\n",
        "    #xgbModel = XGBRegressor(**best_params, random_state=42)\n",
        "\n",
        "    xgbModel.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the training set (in-sample predictions)\n",
        "    yhat_train_xgb = xgbModel.predict(X_train)\n",
        "\n",
        "    # Calculate Train RMSE for XGBoost\n",
        "    train_rmse_xgb = np.sqrt(mean_squared_error(y_train, yhat_train_xgb))\n",
        "    print(f\"Fold{fold_num} - XGBoost Train RMSE: {train_rmse_xgb}\")\n",
        "\n",
        "    # Predict on the test set using XGBoost\n",
        "    yhat_test_xgb = xgbModel.predict(X_test)\n",
        "\n",
        "    # Calculate Test RMSE for XGBoost\n",
        "    test_rmse_xgb = np.sqrt(mean_squared_error(np.log(y_test), yhat_test_xgb))\n",
        "    print(f\"Fold{fold_num} - XGBoost Test RMSE: {test_rmse_xgb}\")\n",
        "\n",
        "    #print(f\"Finished processing fold{fold_num}.\\n\")\n",
        "\n",
        "print(\"Completed processing all folds.\")\n"
      ],
      "metadata": {
        "id": "WBNnhqECCWe5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ee279cf-f1b6-4902-baad-1d428d9339da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing fold1...\n",
            "Fold1 - XGBoost Train RMSE: 0.05193366683570815\n",
            "Fold1 - XGBoost Test RMSE: 0.1153433061435926\n",
            "Processing fold2...\n",
            "Fold2 - XGBoost Train RMSE: 0.049750192640706646\n",
            "Fold2 - XGBoost Test RMSE: 0.11791694689180016\n",
            "Processing fold3...\n",
            "Fold3 - XGBoost Train RMSE: 0.051731474230395945\n",
            "Fold3 - XGBoost Test RMSE: 0.11613088454525501\n",
            "Processing fold4...\n",
            "Fold4 - XGBoost Train RMSE: 0.05140311661923465\n",
            "Fold4 - XGBoost Test RMSE: 0.10989938310462323\n",
            "Processing fold5...\n",
            "Fold5 - XGBoost Train RMSE: 0.05248526007996803\n",
            "Fold5 - XGBoost Test RMSE: 0.10775687038225883\n",
            "Processing fold6...\n",
            "Fold6 - XGBoost Train RMSE: 0.05140486676469129\n",
            "Fold6 - XGBoost Test RMSE: 0.12788898422166814\n",
            "Processing fold7...\n",
            "Fold7 - XGBoost Train RMSE: 0.05048320241997869\n",
            "Fold7 - XGBoost Test RMSE: 0.131695504551595\n",
            "Processing fold8...\n",
            "Fold8 - XGBoost Train RMSE: 0.051276160306289884\n",
            "Fold8 - XGBoost Test RMSE: 0.12434747652836972\n",
            "Processing fold9...\n",
            "Fold9 - XGBoost Train RMSE: 0.052266051753495434\n",
            "Fold9 - XGBoost Test RMSE: 0.129281780870555\n",
            "Processing fold10...\n",
            "Fold10 - XGBoost Train RMSE: 0.052322137961813196\n",
            "Fold10 - XGBoost Test RMSE: 0.12165029453727548\n",
            "Completed processing all folds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAYSM60yC_wz",
        "outputId": "588e2c1e-e923-43bc-b1e6-009a8a3729bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape (2051, 82)\n",
            "y_train shape (2051,)\n",
            "X_train shape (2051, 82)\n",
            "y_train shape (2051,)\n",
            "X_test shape (879, 82)\n",
            "y_test shape (879,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7nE6xcBJC6V",
        "outputId": "4b603d22-a99d-4db4-9205-a7bf8e6677c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape (2051, 341)\n",
            "y_train shape (2051,)\n",
            "X_test shape (879, 341)\n",
            "y_test shape (879,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rkaomAauH_XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0_-8NkFXHkKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6XiyJQlIkux",
        "outputId": "5991553c-2b7e-4370-d791-fab8c6d96efc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.01857196005649597"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7F1Qsrtm3iD",
        "outputId": "e3b83691-77a6-4d41-ba90-544c1b97e202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.002724804750938125"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AcpASs-m_Tk",
        "outputId": "f8a9968c-db07-42e4-ba2d-de42d6dbbe55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.020068197948494702"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ref: https://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html\n",
        "min_estimators = 20\n",
        "max_estimators = 400\n",
        "\n",
        "error_rate = []\n",
        "clf = RandomForestRegressor(oob_score=True)\n",
        "for i in range(min_estimators, max_estimators + 1):\n",
        "    clf.set_params(n_estimators=i, warm_start=True)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Record the OOB error for each `n_estimators=i` setting.\n",
        "    oob_error = np.mean((clf.oob_prediction_ - y_train) ** 2.0)\n",
        "    error_rate.append((i, oob_error))"
      ],
      "metadata": {
        "id": "P8h-tMaIpRQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "error_rate = np.array([*error_rate])\n",
        "\n",
        "plt.plot(error_rate[:, 0], error_rate[:, 1])\n",
        "plt.xlabel(\"Number of trees\")\n",
        "plt.ylabel(\"OOB error rate\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "8k54kv_jpb_e",
        "outputId": "eb26d47b-d2de-4745-f07c-4eb78a4ddc9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'OOB error rate')"
            ]
          },
          "metadata": {},
          "execution_count": 173
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBjFPbryrSRS",
        "outputId": "d0308fbb-8ece-4abd-e4a7-6d3ef5d140d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test RMSE: 0.1362789787769778\n",
            "Train RMSE (OOB): 0.14166226720088418\n"
          ]
        }
      ]
    }
  ]
}