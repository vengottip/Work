{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeLJmTXXZEp0S5W4mdLQ3v"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZFR_ImKxcM4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_features(X, features_to_remove):\n",
        "    \"\"\"\n",
        "    Removes specified features from the DataFrame.\n",
        "\n",
        "    Args:\n",
        "    - X (pd.DataFrame): The original DataFrame.\n",
        "    - features_to_remove (list): List of column names to remove from X.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: DataFrame after removing specified features.\n",
        "    \"\"\"\n",
        "    #return X.drop(columns=features_to_remove, errors='ignore')\n"
      ],
      "metadata": {
        "id": "NPbbInd53Yh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_data_remove_features2(df):\n",
        "    # Remove specific columns\n",
        "\n",
        "    cols_to_remove = [\"Longitude\", \"Latitude\", \"Street\", \"Utilities\", \"Condition_2\",\n",
        "                      \"Roof_Matl\", \"Heating\", \"Pool_QC\", \"Misc_Feature\", \"Low_Qual_Fin_SF\", \"Pool_Area\"]\n",
        "    X = df\n",
        "    #print(\"X  before remove features\", X.columns)\n",
        "    X = df.drop(columns=cols_to_remove)\n",
        "\n",
        "    #X = df\n",
        "    #print(\"X  after remove features\", X.columns)\n",
        "    # Create quadratic features\n",
        "    quad = X.select_dtypes(include=[np.number]) ** 2\n",
        "    quad.columns = [str(col) + \"_Q2\" for col in quad.columns]\n",
        "\n",
        "    # Create cubic features\n",
        "    cube = X.select_dtypes(include=[np.number]) ** 3\n",
        "    cube.columns = [str(col) + \"_C3\" for col in cube.columns]\n",
        "\n",
        "    # Combine original data with quadratic and cubic features\n",
        "    X = pd.concat([X, quad, cube], axis=1)\n",
        "\n",
        "    # One-hot encoding for categorical columns\n",
        "    X = pd.get_dummies(X, drop_first=True)\n",
        "    #print(\"X  after adding dummies\", X.columns)\n",
        "    return X\n"
      ],
      "metadata": {
        "id": "d_1uDY4YnF5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data_remove_features(df, categorical_columns):\n",
        "    \"\"\"\n",
        "    Preprocesses the dataframe by removing specific columns, creating quadratic\n",
        "    and cubic features for numeric columns, and encoding specified categorical columns.\n",
        "\n",
        "    Args:\n",
        "    - df (pd.DataFrame): The original dataframe.\n",
        "    - categorical_columns (list): List of categorical column names to be one-hot encoded.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: The processed dataframe with quadratic and cubic features and encoded categorical variables.\n",
        "    \"\"\"\n",
        "    # Remove specific columns\n",
        "    cols_to_remove = [\"Longitude\", \"Latitude\", \"Street\", \"Utilities\", \"Condition_2\",\n",
        "                      \"Roof_Matl\", \"Heating\", \"Pool_QC\", \"Misc_Feature\", \"Low_Qual_Fin_SF\", \"Pool_Area\"]\n",
        "    X = df.drop(columns=cols_to_remove, errors='ignore')\n",
        "\n",
        "    # Create quadratic and cubic features for numeric columns only\n",
        "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
        "    quad = X[numeric_cols] ** 2\n",
        "    quad.columns = [str(col) + \"_Q2\" for col in quad.columns]\n",
        "\n",
        "    cube = X[numeric_cols] ** 3\n",
        "    cube.columns = [str(col) + \"_C3\" for col in cube.columns]\n",
        "\n",
        "    # Combine original data with quadratic and cubic features\n",
        "    X_combined = pd.concat([X, quad, cube], axis=1)\n",
        "\n",
        "    # Apply one-hot encoding only to the specified categorical columns\n",
        "    X_encoded = pd.get_dummies(X_combined, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "    return X_encoded\n"
      ],
      "metadata": {
        "id": "cLX50dUa_xIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import mstats\n",
        "\n",
        "# To store saved quantiles across folds\n",
        "saved_quantiles = {}\n",
        "\n",
        "def preprocess_data_winsorize_selected(df, fold, to_winsorize, train=True):\n",
        "    global saved_quantiles\n",
        "\n",
        "    # Initialize result as a copy of the input dataframe\n",
        "    res = df.copy()\n",
        "\n",
        "    if fold not in saved_quantiles:\n",
        "        saved_quantiles[fold] = {}\n",
        "\n",
        "    for col in to_winsorize:\n",
        "        if col in res.columns:\n",
        "            if train:\n",
        "                # Calculate the 5th and 95th percentiles\n",
        "                lower_quantile = res[col].quantile(0.05)\n",
        "                upper_quantile = res[col].quantile(0.95)\n",
        "                saved_quantiles[fold][col] = (lower_quantile, upper_quantile)\n",
        "            else:\n",
        "                # Use previously saved quantiles\n",
        "                lower_quantile, upper_quantile = saved_quantiles[fold][col]\n",
        "\n",
        "            # Apply winsorization\n",
        "            res[col] = mstats.winsorize(res[col], limits=[0.05, 0.05])\n",
        "\n",
        "    return res\n"
      ],
      "metadata": {
        "id": "lSIE5lp9nZm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def numeric_columns(df):\n",
        "    # Select columns that are not categorical (i.e., numeric columns)\n",
        "    categorical_vars = df.select_dtypes(include=['object']).columns\n",
        "    df_numeric = df.drop(columns=categorical_vars)\n",
        "\n",
        "    return df_numeric\n"
      ],
      "metadata": {
        "id": "QCN1CT9RojPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def raise_pow(df, p):\n",
        "    return df ** p\n"
      ],
      "metadata": {
        "id": "sisJQkIBokJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def changename(col):\n",
        "    return col + \"_A2\"\n"
      ],
      "metadata": {
        "id": "dwHZ8qMoomnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example usage"
      ],
      "metadata": {
        "id": "3rqtwFpYou9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample DataFrame\n",
        "data = {'A': [1, 2, 3], 'B': [4, 5, 6], 'C': ['cat', 'dog', 'mouse']}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 1. Get only numeric columns\n",
        "numeric_df = numeric_columns(df)\n",
        "\n",
        "# 2. Raise numeric columns to the power of 2\n",
        "quad_df = raise_pow(numeric_df, 2)\n",
        "\n",
        "# 3. Change column names (append \"_A2\")\n",
        "quad_df.columns = [changename(col) for col in quad_df.columns]\n",
        "\n",
        "print(quad_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n33mmckooxSz",
        "outputId": "eea0d038-cbe2-4e4a-9fdf-855374b7619f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   A_A2  B_A2\n",
            "0     1    16\n",
            "1     4    25\n",
            "2     9    36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hhM__g7D3sMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_data_poly(df):\n",
        "    # Remove specific columns\n",
        "    cols_to_remove = [\"Longitude\", \"Latitude\", \"Street\", \"Utilities\", \"Condition_2\",\n",
        "                      \"Roof_Matl\", \"Heating\", \"Pool_QC\", \"Misc_Feature\", \"Low_Qual_Fin_SF\", \"Pool_Area\"]\n",
        "\n",
        "    X = df.drop(columns=cols_to_remove)\n",
        "    \"\"\"\n",
        "    # Create quadratic features\n",
        "    quad = X.select_dtypes(include=[np.number]) ** 2\n",
        "    quad.columns = [str(col) + \"_A2\" for col in quad.columns]\n",
        "\n",
        "    # Create cubic features\n",
        "    cube = X.select_dtypes(include=[np.number]) ** 3\n",
        "    cube.columns = [str(col) + \"_A3\" for col in cube.columns]\n",
        "\n",
        "    # Combine original data with quadratic and cubic features\n",
        "    X = pd.concat([X, quad, cube], axis=1)\n",
        "    \"\"\"\n",
        "    # One-hot encoding for categorical columns\n",
        "    X = pd.get_dummies(X, drop_first=True)\n",
        "    print (\"X\", X)\n",
        "    return X\n"
      ],
      "metadata": {
        "id": "iRQMSgKPtGaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import mstats\n",
        "\n",
        "# To store saved quantiles across folds\n",
        "saved_quantiles = {}\n",
        "\n",
        "def winsorize_selected(df, fold, to_winsorize, train=True):\n",
        "    global saved_quantiles\n",
        "\n",
        "    # Initialize result as a copy of the input dataframe\n",
        "    res = df.copy()\n",
        "\n",
        "    if fold not in saved_quantiles:\n",
        "        saved_quantiles[fold] = {}\n",
        "\n",
        "    for col in to_winsorize:\n",
        "        if col in res.columns:\n",
        "            if train:\n",
        "                # Calculate the 5th and 95th percentiles\n",
        "                lower_quantile = res[col].quantile(0.05)\n",
        "                upper_quantile = res[col].quantile(0.95)\n",
        "                saved_quantiles[fold][col] = (lower_quantile, upper_quantile)\n",
        "            else:\n",
        "                # Use previously saved quantiles\n",
        "                lower_quantile, upper_quantile = saved_quantiles[fold][col]\n",
        "\n",
        "            # Apply winsorization\n",
        "            res[col] = mstats.winsorize(res[col], limits=[0.05, 0.05])\n",
        "\n",
        "    return res\n",
        "\n"
      ],
      "metadata": {
        "id": "7FXwmoG0OTqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def numeric_columns(df):\n",
        "    # Select columns that are not categorical (i.e., numeric columns)\n",
        "    categorical_vars = df.select_dtypes(include=['object']).columns\n",
        "    df_numeric = df.drop(columns=categorical_vars)\n",
        "\n",
        "    return df_numeric"
      ],
      "metadata": {
        "id": "YeXDquU-sXd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy import stats  # Importing for z-score calculation\n",
        "\n",
        "# Helper functions\n",
        "def preprocess_data(X_trn):\n",
        "    # Initialize the StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit the scaler on the training data and transform it\n",
        "    X_trn_scaled = scaler.fit_transform(X_trn)\n",
        "\n",
        "    # Return the scaled training data as a DataFrame (to preserve column names) and the scaler object\n",
        "    X_trn_scaled_df = pd.DataFrame(X_trn_scaled, columns=X_trn.columns)\n",
        "\n",
        "    return X_trn_scaled_df, scaler\n",
        "\n",
        "def calc_rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def adjusted_r_squared(r2, n, p):\n",
        "    # Calculate adjusted R-squared\n",
        "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "\n",
        "def process_fold(fold_num):\n",
        "    print(f\"\\nProcessing fold{fold_num}...\")\n",
        "\n",
        "    # Load the train, test, and test_y datasets\n",
        "    train_file_path = f'fold{fold_num}/train.csv'\n",
        "    test_file_path = f'fold{fold_num}/test.csv'\n",
        "    test_y_file_path = f'fold{fold_num}/test_y.csv'\n",
        "\n",
        "    housing_data_train = pd.read_csv(train_file_path)\n",
        "    housing_data_test = pd.read_csv(test_file_path)\n",
        "    test_y_data = pd.read_csv(test_y_file_path)  # Load the actual Sale Price for the test set\n",
        "\n",
        "    # Ensure that test_y_data and housing_data_test have matching PIDs\n",
        "    merged_test_data = pd.merge(housing_data_test, test_y_data, on='PID', how='inner')\n",
        "\n",
        "\n",
        "    # Identify categorical columns\n",
        "    categorical_columns = ['MS_SubClass', 'MS_Zoning',  'Alley', 'Lot_Shape', 'Land_Contour',\n",
        "                           'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1',  'Bldg_Type',\n",
        "                           'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style',  'Exterior_1st',\n",
        "                           'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual', 'Exter_Cond', 'Foundation', 'Bsmt_Qual',\n",
        "                           'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2',  'Heating_QC',\n",
        "                           'Central_Air', 'Electrical', 'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type',\n",
        "                           'Garage_Finish', 'Garage_Qual', 'Garage_Cond', 'Paved_Drive',  'Fence',\n",
        "                           'Sale_Type', 'Sale_Condition']\n",
        "\n",
        "    # Perform dummy encoding on both train and test datasets\n",
        "    #housing_data_encoded_train = pd.get_dummies(housing_data_train, columns=categorical_columns, drop_first=True)\n",
        "    #housing_data_encoded_test = pd.get_dummies(merged_test_data, columns=categorical_columns, drop_first=True)\n",
        "    housing_data_encoded_train = preprocess_data_remove_features(housing_data_train, categorical_columns)\n",
        "    housing_data_encoded_test = preprocess_data_remove_features(merged_test_data, categorical_columns)\n",
        "    # Align columns of train and test after dummy encoding:\n",
        "    all_columns = list(set(housing_data_encoded_train.columns) | set(housing_data_encoded_test.columns))\n",
        "    all_columns.remove('Sale_Price')  # Remove 'Sale_Price' if it's in all_columns\n",
        "\n",
        "    housing_data_encoded_train = housing_data_encoded_train.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    housing_data_encoded_test = housing_data_encoded_test.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    #print(\"housing_data_encoded_train shape\", housing_data_encoded_train.shape)\n",
        "    #print(\"housing_data_encoded_test shape\", housing_data_encoded_test.shape)\n",
        "\n",
        "    # Handle missing values by imputing them in train and test data\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    numeric_columns = housing_data_encoded_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numeric_columns = [col for col in numeric_columns if col not in ['Sale_Price', 'PID']]\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_train.copy()\n",
        "    housing_data_encoded_imputed_train[numeric_columns] = imputer.fit_transform(housing_data_encoded_train[numeric_columns])\n",
        "\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_test.copy()\n",
        "    housing_data_encoded_imputed_test[numeric_columns] = imputer.transform(housing_data_encoded_test[numeric_columns])\n",
        "    #print(\"housing_data_encoded_imputed_train shape\", housing_data_encoded_imputed_train.shape)\n",
        "    #print(\"housing_data_encoded_imputed_test shape\", housing_data_encoded_imputed_test.shape)\n",
        "\n",
        "    # Remove 'PID' from train and test datasets\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_imputed_train.drop(columns=['PID'])\n",
        "    if 'PID' in housing_data_encoded_imputed_test.columns:\n",
        "        housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.drop(columns=['PID'])\n",
        "\n",
        "    # Separate predictors (X) and target variable (y) for training\n",
        "    X_trn = housing_data_encoded_imputed_train.drop(columns='Sale_Price')\n",
        "    y_trn = np.log(housing_data_encoded_imputed_train['Sale_Price'])\n",
        "\n",
        "    # Scale the training data using preprocess_data function (returns DataFrame with column names)\n",
        "    X_trn_scaled, scaler = preprocess_data(X_trn)\n",
        "\n",
        "    # 1. Use Lasso regression for feature selection\n",
        "    lasso_model = Lasso(alpha=0.01)  # Adjust the alpha value as necessary\n",
        "    lasso_model.fit(X_trn_scaled, y_trn)\n",
        "    selected_features = X_trn_scaled.columns[lasso_model.coef_ != 0]\n",
        "    #print(f\"Selected features after Lasso: {len(selected_features)} features\")\n",
        "\n",
        "    # Scale the test data using the same scaler\n",
        "    X_tst_scaled = pd.DataFrame(scaler.transform(housing_data_encoded_imputed_test.drop(columns=['Sale_Price'])), columns=housing_data_encoded_imputed_test.drop(columns=['Sale_Price']).columns)\n",
        "    #print(\"X_tst_scaled shape\", X_tst_scaled.shape)\n",
        "\n",
        "    # 2. Grid Search for Ridge Regression to find the best alpha\n",
        "    alpha_values = np.exp(np.linspace(-5, 5, 100))\n",
        "    param_grid = {'alpha': alpha_values}\n",
        "\n",
        "    ridge_model = Ridge()\n",
        "    grid_search = GridSearchCV(ridge_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "    grid_search.fit(X_trn_scaled[selected_features], y_trn)\n",
        "    best_alpha = grid_search.best_params_['alpha']\n",
        "    #print(\"Best alpha for Ridge regression:\", best_alpha)\n",
        "\n",
        "    # 3. Train the final Ridge model with the best alpha\n",
        "    final_ridge_model = Ridge(alpha=best_alpha)\n",
        "    final_ridge_model.fit(X_trn_scaled[selected_features], y_trn)\n",
        "\n",
        "    # Predict on the training set\n",
        "    y_trn_pred = final_ridge_model.predict(X_trn_scaled[selected_features])\n",
        "    r2_train = r2_score(y_trn, y_trn_pred)\n",
        "    adj_r2_train = adjusted_r_squared(r2_train, len(y_trn), X_trn_scaled[selected_features].shape[1])\n",
        "    train_rmse = calc_rmse(y_trn, y_trn_pred)\n",
        "    #print(f\"Training R-squared: {r2_train}\")\n",
        "    #print(f\"Adjusted R-squared (Training): {adj_r2_train}\")\n",
        "    print(f\"Training RMSE: {train_rmse}\")\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_tst_pred_log = final_ridge_model.predict(X_tst_scaled[selected_features])\n",
        "\n",
        "    # Reverse the log-transformation to get predictions in the original scale\n",
        "    y_tst_pred = np.exp(y_tst_pred_log)\n",
        "\n",
        "    # Merge the predictions with actual sale prices from test_y.csv using 'PID'\n",
        "    predictions = pd.DataFrame({\n",
        "        'PID': merged_test_data['PID'],\n",
        "        'Predicted_Sale_Price': y_tst_pred\n",
        "    })\n",
        "    merged_test_data = pd.merge(test_y_data, predictions, on='PID', how='inner')\n",
        "\n",
        "    # Calculate RMSE and R-squared for the test set using actual sale prices\n",
        "    test_rmse = calc_rmse(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    r2_test = r2_score(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    adj_r2_test = adjusted_r_squared(r2_test, len(merged_test_data), X_tst_scaled.shape[1])\n",
        "    print(f\"Test RMSE: {test_rmse}\")\n",
        "    #print(f\"Test R-squared: {r2_test}\")\n",
        "    #print(f\"Adjusted R-squared (Test): {adj_r2_test}\")\n",
        "\n",
        "    # Save the predictions to a CSV file\n",
        "    predictions_file = f'fold{fold_num}_test_predictions.csv'\n",
        "    predictions.to_csv(predictions_file, index=False)\n",
        "    #print(f\"Predictions for fold{fold_num} saved to '{predictions_file}'.\")\n",
        "\n",
        "def main():\n",
        "    # Loop over folds 1 to 10\n",
        "    for fold_num in range(1, 11):\n",
        "        process_fold(fold_num)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85719461-5736-417a-ae9d-f532b34d2617",
        "id": "SaxEkgs_3s7J"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing fold1...\n",
            "Training RMSE: 0.09594796418915669\n",
            "Test RMSE: 0.10470095736772288\n",
            "\n",
            "Processing fold2...\n",
            "Training RMSE: 0.09582772225902751\n",
            "Test RMSE: 0.10919573341706115\n",
            "\n",
            "Processing fold3...\n",
            "Training RMSE: 0.09899734279687067\n",
            "Test RMSE: 0.11067459176582921\n",
            "\n",
            "Processing fold4...\n",
            "Training RMSE: 0.09706450792910584\n",
            "Test RMSE: 0.10137524410487105\n",
            "\n",
            "Processing fold5...\n",
            "Training RMSE: 0.0983135931549487\n",
            "Test RMSE: 0.10311582332938206\n",
            "\n",
            "Processing fold6...\n",
            "Training RMSE: 0.09632112502074647\n",
            "Test RMSE: 0.114918089189173\n",
            "\n",
            "Processing fold7...\n",
            "Training RMSE: 0.09489886531676978\n",
            "Test RMSE: 0.1215892691168203\n",
            "\n",
            "Processing fold8...\n",
            "Training RMSE: 0.09892128410565307\n",
            "Test RMSE: 0.10394924096076101\n",
            "\n",
            "Processing fold9...\n",
            "Training RMSE: 0.0982606024647681\n",
            "Test RMSE: 0.11315701014386603\n",
            "\n",
            "Processing fold10...\n",
            "Training RMSE: 0.09836567892003986\n",
            "Test RMSE: 0.11474662950842018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MnSt2tXje8w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy import stats  # Importing for z-score calculation\n",
        "\n",
        "# Helper functions\n",
        "def preprocess_data(X_trn):\n",
        "    # Initialize the StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit the scaler on the training data and transform it\n",
        "    X_trn_scaled = scaler.fit_transform(X_trn)\n",
        "\n",
        "    # Return the scaled training data as a DataFrame (to preserve column names) and the scaler object\n",
        "    X_trn_scaled_df = pd.DataFrame(X_trn_scaled, columns=X_trn.columns)\n",
        "\n",
        "    return X_trn_scaled_df, scaler\n",
        "\n",
        "def calc_rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def adjusted_r_squared(r2, n, p):\n",
        "    # Calculate adjusted R-squared\n",
        "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "\n",
        "def process_fold(fold_num):\n",
        "    print(f\"\\nProcessing fold{fold_num}...\")\n",
        "\n",
        "    # Load the train, test, and test_y datasets\n",
        "    train_file_path = f'fold{fold_num}/train.csv'\n",
        "    test_file_path = f'fold{fold_num}/test.csv'\n",
        "    test_y_file_path = f'fold{fold_num}/test_y.csv'\n",
        "\n",
        "    housing_data_train = pd.read_csv(train_file_path)\n",
        "    housing_data_test = pd.read_csv(test_file_path)\n",
        "    test_y_data = pd.read_csv(test_y_file_path)  # Load the actual Sale Price for the test set\n",
        "\n",
        "    # Ensure that test_y_data and housing_data_test have matching PIDs\n",
        "    merged_test_data = pd.merge(housing_data_test, test_y_data, on='PID', how='inner')\n",
        "\n",
        "\n",
        "    # Identify categorical columns\n",
        "    categorical_columns = ['MS_SubClass', 'MS_Zoning',  'Alley', 'Lot_Shape', 'Land_Contour',\n",
        "                           'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1',  'Bldg_Type',\n",
        "                           'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style',  'Exterior_1st',\n",
        "                           'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual', 'Exter_Cond', 'Foundation', 'Bsmt_Qual',\n",
        "                           'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2',  'Heating_QC',\n",
        "                           'Central_Air', 'Electrical', 'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type',\n",
        "                           'Garage_Finish', 'Garage_Qual', 'Garage_Cond', 'Paved_Drive',  'Fence',\n",
        "                           'Sale_Type', 'Sale_Condition']\n",
        "\n",
        "    # Perform dummy encoding on both train and test datasets\n",
        "    #housing_data_encoded_train = pd.get_dummies(housing_data_train, columns=categorical_columns, drop_first=True)\n",
        "    #housing_data_encoded_test = pd.get_dummies(merged_test_data, columns=categorical_columns, drop_first=True)\n",
        "    housing_data_encoded_train = preprocess_data_remove_features(housing_data_train, categorical_columns)\n",
        "    housing_data_encoded_test = preprocess_data_remove_features(merged_test_data, categorical_columns)\n",
        "    # Align columns of train and test after dummy encoding:\n",
        "    all_columns = list(set(housing_data_encoded_train.columns) | set(housing_data_encoded_test.columns))\n",
        "    all_columns.remove('Sale_Price')  # Remove 'Sale_Price' if it's in all_columns\n",
        "\n",
        "    housing_data_encoded_train = housing_data_encoded_train.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    housing_data_encoded_test = housing_data_encoded_test.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    #print(\"housing_data_encoded_train shape\", housing_data_encoded_train.shape)\n",
        "    #print(\"housing_data_encoded_test shape\", housing_data_encoded_test.shape)\n",
        "\n",
        "    # Handle missing values by imputing them in train and test data\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    numeric_columns = housing_data_encoded_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numeric_columns = [col for col in numeric_columns if col not in ['Sale_Price', 'PID']]\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_train.copy()\n",
        "    housing_data_encoded_imputed_train[numeric_columns] = imputer.fit_transform(housing_data_encoded_train[numeric_columns])\n",
        "\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_test.copy()\n",
        "    housing_data_encoded_imputed_test[numeric_columns] = imputer.transform(housing_data_encoded_test[numeric_columns])\n",
        "    #print(\"housing_data_encoded_imputed_train shape\", housing_data_encoded_imputed_train.shape)\n",
        "    #print(\"housing_data_encoded_imputed_test shape\", housing_data_encoded_imputed_test.shape)\n",
        "\n",
        "    # Remove 'PID' from train and test datasets\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_imputed_train.drop(columns=['PID'])\n",
        "    if 'PID' in housing_data_encoded_imputed_test.columns:\n",
        "        housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.drop(columns=['PID'])\n",
        "\n",
        "    # Separate predictors (X) and target variable (y) for training\n",
        "    X_trn = housing_data_encoded_imputed_train.drop(columns='Sale_Price')\n",
        "    y_trn = np.log(housing_data_encoded_imputed_train['Sale_Price'])\n",
        "\n",
        "    # Scale the training data using preprocess_data function (returns DataFrame with column names)\n",
        "    X_trn_scaled, scaler = preprocess_data(X_trn)\n",
        "\n",
        "    # 1. Use Lasso regression for feature selection\n",
        "    lasso_model = Lasso(alpha=0.01)  # Adjust the alpha value as necessary\n",
        "    lasso_model.fit(X_trn_scaled, y_trn)\n",
        "    selected_features = X_trn_scaled.columns[lasso_model.coef_ != 0]\n",
        "    #print(f\"Selected features after Lasso: {len(selected_features)} features\")\n",
        "\n",
        "    # Scale the test data using the same scaler\n",
        "    X_tst_scaled = pd.DataFrame(scaler.transform(housing_data_encoded_imputed_test.drop(columns=['Sale_Price'])), columns=housing_data_encoded_imputed_test.drop(columns=['Sale_Price']).columns)\n",
        "    #print(\"X_tst_scaled shape\", X_tst_scaled.shape)\n",
        "\n",
        "    # 2. Grid Search for Ridge Regression to find the best alpha\n",
        "    alpha_values = np.exp(np.linspace(-5, 5, 100))\n",
        "    param_grid = {'alpha': alpha_values}\n",
        "\n",
        "    ridge_model = Ridge()\n",
        "    grid_search = GridSearchCV(ridge_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "    grid_search.fit(X_trn_scaled[selected_features], y_trn)\n",
        "    best_alpha = grid_search.best_params_['alpha']\n",
        "    #print(\"Best alpha for Ridge regression:\", best_alpha)\n",
        "\n",
        "    # 3. Train the final Ridge model with the best alpha\n",
        "    final_ridge_model = Ridge(alpha=best_alpha)\n",
        "    final_ridge_model.fit(X_trn_scaled[selected_features], y_trn)\n",
        "\n",
        "    # Predict on the training set\n",
        "    y_trn_pred = final_ridge_model.predict(X_trn_scaled[selected_features])\n",
        "    r2_train = r2_score(y_trn, y_trn_pred)\n",
        "    adj_r2_train = adjusted_r_squared(r2_train, len(y_trn), X_trn_scaled[selected_features].shape[1])\n",
        "    train_rmse = calc_rmse(y_trn, y_trn_pred)\n",
        "    #print(f\"Training R-squared: {r2_train}\")\n",
        "    #print(f\"Adjusted R-squared (Training): {adj_r2_train}\")\n",
        "    print(f\"Training RMSE: {train_rmse}\")\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_tst_pred_log = final_ridge_model.predict(X_tst_scaled[selected_features])\n",
        "\n",
        "    # Reverse the log-transformation to get predictions in the original scale\n",
        "    y_tst_pred = np.exp(y_tst_pred_log)\n",
        "\n",
        "    # Merge the predictions with actual sale prices from test_y.csv using 'PID'\n",
        "    predictions = pd.DataFrame({\n",
        "        'PID': merged_test_data['PID'],\n",
        "        'Predicted_Sale_Price': y_tst_pred\n",
        "    })\n",
        "    merged_test_data = pd.merge(test_y_data, predictions, on='PID', how='inner')\n",
        "\n",
        "    # Calculate RMSE and R-squared for the test set using actual sale prices\n",
        "    test_rmse = calc_rmse(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    r2_test = r2_score(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    adj_r2_test = adjusted_r_squared(r2_test, len(merged_test_data), X_tst_scaled.shape[1])\n",
        "    print(f\"Test RMSE: {test_rmse}\")\n",
        "    #print(f\"Test R-squared: {r2_test}\")\n",
        "    #print(f\"Adjusted R-squared (Test): {adj_r2_test}\")\n",
        "\n",
        "    # Save the predictions to a CSV file\n",
        "    predictions_file = f'fold{fold_num}_test_predictions.csv'\n",
        "    predictions.to_csv(predictions_file, index=False)\n",
        "    #print(f\"Predictions for fold{fold_num} saved to '{predictions_file}'.\")\n",
        "\n",
        "def main():\n",
        "    # Loop over folds 1 to 10\n",
        "    for fold_num in range(1, 11):\n",
        "        process_fold(fold_num)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f38c069b-af4e-4a37-83d8-658b59443527",
        "id": "a8WmW3ave9ED"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing fold1...\n",
            "Training RMSE: 0.09594588428518307\n",
            "Test RMSE: 0.10469223283328424\n",
            "\n",
            "Processing fold2...\n",
            "Training RMSE: 0.09582819397381018\n",
            "Test RMSE: 0.10917123031397012\n",
            "\n",
            "Processing fold3...\n",
            "Training RMSE: 0.09899029824624016\n",
            "Test RMSE: 0.11063167697189338\n",
            "\n",
            "Processing fold4...\n",
            "Training RMSE: 0.09706420392046199\n",
            "Test RMSE: 0.10138446890311324\n",
            "\n",
            "Processing fold5...\n",
            "Training RMSE: 0.09831349136602181\n",
            "Test RMSE: 0.10311477433681965\n",
            "\n",
            "Processing fold6...\n",
            "Training RMSE: 0.09632037026795903\n",
            "Test RMSE: 0.11490563249669387\n",
            "\n",
            "Processing fold7...\n",
            "Training RMSE: 0.0948991434075486\n",
            "Test RMSE: 0.12160155025132481\n",
            "\n",
            "Processing fold8...\n",
            "Training RMSE: 0.09876341419037267\n",
            "Test RMSE: 0.10385505305489069\n",
            "\n",
            "Processing fold9...\n",
            "Training RMSE: 0.09825948927589798\n",
            "Test RMSE: 0.11316541032209632\n",
            "\n",
            "Processing fold10...\n",
            "Training RMSE: 0.098399397559317\n",
            "Test RMSE: 0.1142611021048103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy import stats  # Importing for z-score calculation\n",
        "\n",
        "# Helper functions\n",
        "def preprocess_data(X_trn):\n",
        "    # Initialize the StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit the scaler on the training data and transform it\n",
        "    X_trn_scaled = scaler.fit_transform(X_trn)\n",
        "\n",
        "    # Return the scaled training data as a DataFrame (to preserve column names) and the scaler object\n",
        "    X_trn_scaled_df = pd.DataFrame(X_trn_scaled, columns=X_trn.columns)\n",
        "\n",
        "    return X_trn_scaled_df, scaler\n",
        "\n",
        "def calc_rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def adjusted_r_squared(r2, n, p):\n",
        "    # Calculate adjusted R-squared\n",
        "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "\n",
        "def process_fold(fold_num):\n",
        "    print(f\"\\nProcessing fold{fold_num}...\")\n",
        "\n",
        "    # Load the train, test, and test_y datasets\n",
        "    train_file_path = f'fold{fold_num}/train.csv'\n",
        "    test_file_path = f'fold{fold_num}/test.csv'\n",
        "    test_y_file_path = f'fold{fold_num}/test_y.csv'\n",
        "\n",
        "    # Define the list of features to remove (this is just an example, adjust as needed)\n",
        "    features_to_remove = [\"Longitude\", \"Latitude\", \"Street\", \"Utilities\", \"Condition_2\",\n",
        "                      \"Roof_Matl\", \"Heating\", \"Pool_QC\", \"Misc_Feature\",  \"Misc_Val\",\"Low_Qual_Fin_SF\", \"Sale_Type\", \"Pool_Area\"]  # Specify the features you want to remove\n",
        "\n",
        "\n",
        "\n",
        "    housing_data_train = pd.read_csv(train_file_path)\n",
        "    housing_data_test = pd.read_csv(test_file_path)\n",
        "    test_y_data = pd.read_csv(test_y_file_path)  # Load the actual Sale Price for the test set\n",
        "\n",
        "    # Call the remove_features function to remove unwanted features from X_trn\n",
        "    housing_data_train = remove_features(housing_data_train, features_to_remove)\n",
        "\n",
        "    # Ensure that test_y_data and housing_data_test have matching PIDs\n",
        "    merged_test_data = pd.merge(housing_data_test, test_y_data, on='PID', how='inner')\n",
        "\n",
        "    # Identify categorical columns\n",
        "    categorical_columns_test = ['MS_SubClass', 'MS_Zoning', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities',\n",
        "                           'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type',\n",
        "                           'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style', 'Roof_Matl', 'Exterior_1st',\n",
        "                           'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual', 'Exter_Cond', 'Foundation', 'Bsmt_Qual',\n",
        "                           'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating', 'Heating_QC',\n",
        "                           'Central_Air', 'Electrical', 'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type',\n",
        "                           'Garage_Finish', 'Garage_Qual', 'Garage_Cond', 'Paved_Drive', 'Pool_QC', 'Fence',\n",
        "                           'Misc_Feature', 'Sale_Type', 'Sale_Condition']\n",
        "\n",
        "    categorical_columns_train = ['MS_SubClass', 'MS_Zoning', 'Alley', 'Lot_Shape', 'Land_Contour',\n",
        "                            'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Bldg_Type',\n",
        "                            'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style',\n",
        "                            'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual',\n",
        "                            'Exter_Cond', 'Foundation', 'Bsmt_Qual', 'Bsmt_Cond', 'Bsmt_Exposure',\n",
        "                            'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating_QC', 'Central_Air',  'Electrical',\n",
        "                            'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type', 'Garage_Finish',\n",
        "                            'Garage_Qual', 'Garage_Cond', 'Paved_Drive', 'Fence',\n",
        "                            'Sale_Condition']\n",
        "\n",
        "\n",
        "    # Perform dummy encoding on both train and test datasets\n",
        "    housing_data_encoded_train = pd.get_dummies(housing_data_train, columns=categorical_columns_train, drop_first=True)\n",
        "    housing_data_encoded_test = pd.get_dummies(merged_test_data, columns=categorical_columns_test, drop_first=True)\n",
        "\n",
        "    # Align columns of train and test after dummy encoding:\n",
        "    all_columns = list(set(housing_data_encoded_train.columns) | set(housing_data_encoded_test.columns))\n",
        "    all_columns.remove('Sale_Price')  # Remove 'Sale_Price' if it's in all_columns\n",
        "\n",
        "    housing_data_encoded_train = housing_data_encoded_train.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    housing_data_encoded_test = housing_data_encoded_test.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    print(\"housing_data_encoded_train shape\", housing_data_encoded_train.shape)\n",
        "    print(\"housing_data_encoded_test shape\", housing_data_encoded_test.shape)\n",
        "\n",
        "    # Handle missing values by imputing them in train and test data\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    numeric_columns = housing_data_encoded_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numeric_columns = [col for col in numeric_columns if col not in ['Sale_Price', 'PID']]\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_train.copy()\n",
        "    housing_data_encoded_imputed_train[numeric_columns] = imputer.fit_transform(housing_data_encoded_train[numeric_columns])\n",
        "\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_test.copy()\n",
        "    housing_data_encoded_imputed_test[numeric_columns] = imputer.transform(housing_data_encoded_test[numeric_columns])\n",
        "    print(\"housing_data_encoded_imputed_train shape\", housing_data_encoded_imputed_train.shape)\n",
        "    print(\"housing_data_encoded_imputed_test shape\", housing_data_encoded_imputed_test.shape)\n",
        "\n",
        "    # Remove 'PID' from train and test datasets\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_imputed_train.drop(columns=['PID'])\n",
        "    if 'PID' in housing_data_encoded_imputed_test.columns:\n",
        "        housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.drop(columns=['PID'])\n",
        "\n",
        "    # Separate predictors (X) and target variable (y) for training\n",
        "    X_trn = housing_data_encoded_imputed_train.drop(columns='Sale_Price')\n",
        "    y_trn = np.log(housing_data_encoded_imputed_train['Sale_Price'])\n",
        "\n",
        "    # Scale the training data using preprocess_data function (returns DataFrame with column names)\n",
        "    X_trn_scaled, scaler = preprocess_data(X_trn)\n",
        "\n",
        "    # 1. Use Lasso regression for feature selection\n",
        "    lasso_model = Lasso(alpha=0.01)  # Adjust the alpha value as necessary\n",
        "    lasso_model.fit(X_trn_scaled, y_trn)\n",
        "    selected_features = X_trn_scaled.columns[lasso_model.coef_ != 0]\n",
        "    print(f\"Selected features after Lasso: {len(selected_features)} features\")\n",
        "\n",
        "    # Scale the test data using the same scaler\n",
        "    X_tst_scaled = pd.DataFrame(scaler.transform(housing_data_encoded_imputed_test.drop(columns=['Sale_Price'])), columns=housing_data_encoded_imputed_test.drop(columns=['Sale_Price']).columns)\n",
        "    print(\"X_tst_scaled shape\", X_tst_scaled.shape)\n",
        "\n",
        "    # 2. Grid Search for Ridge Regression to find the best alpha\n",
        "    alpha_values = np.exp(np.linspace(-5, 5, 100))\n",
        "    param_grid = {'alpha': alpha_values}\n",
        "\n",
        "    ridge_model = Ridge()\n",
        "    grid_search = GridSearchCV(ridge_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "    grid_search.fit(X_trn_scaled[selected_features], y_trn)\n",
        "    best_alpha = grid_search.best_params_['alpha']\n",
        "    print(\"Best alpha for Ridge regression:\", best_alpha)\n",
        "\n",
        "    # 3. Train the final Ridge model with the best alpha\n",
        "    final_ridge_model = Ridge(alpha=best_alpha)\n",
        "    final_ridge_model.fit(X_trn_scaled[selected_features], y_trn)\n",
        "\n",
        "    # Predict on the training set\n",
        "    y_trn_pred = final_ridge_model.predict(X_trn_scaled[selected_features])\n",
        "    r2_train = r2_score(y_trn, y_trn_pred)\n",
        "    adj_r2_train = adjusted_r_squared(r2_train, len(y_trn), X_trn_scaled[selected_features].shape[1])\n",
        "    train_rmse = calc_rmse(y_trn, y_trn_pred)\n",
        "    print(f\"Training R-squared: {r2_train}\")\n",
        "    print(f\"Adjusted R-squared (Training): {adj_r2_train}\")\n",
        "    print(f\"Training RMSE: {train_rmse}\")\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_tst_pred_log = final_ridge_model.predict(X_tst_scaled[selected_features])\n",
        "\n",
        "    # Reverse the log-transformation to get predictions in the original scale\n",
        "    y_tst_pred = np.exp(y_tst_pred_log)\n",
        "\n",
        "    # Merge the predictions with actual sale prices from test_y.csv using 'PID'\n",
        "    predictions = pd.DataFrame({\n",
        "        'PID': merged_test_data['PID'],\n",
        "        'Predicted_Sale_Price': y_tst_pred\n",
        "    })\n",
        "    merged_test_data = pd.merge(test_y_data, predictions, on='PID', how='inner')\n",
        "\n",
        "    # Calculate RMSE and R-squared for the test set using actual sale prices\n",
        "    test_rmse = calc_rmse(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    r2_test = r2_score(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    adj_r2_test = adjusted_r_squared(r2_test, len(merged_test_data), X_tst_scaled.shape[1])\n",
        "    print(f\"Test RMSE: {test_rmse}\")\n",
        "    print(f\"Test R-squared: {r2_test}\")\n",
        "    print(f\"Adjusted R-squared (Test): {adj_r2_test}\")\n",
        "\n",
        "    # Save the predictions to a CSV file\n",
        "    predictions_file = f'fold{fold_num}_test_predictions.csv'\n",
        "    predictions.to_csv(predictions_file, index=False)\n",
        "    print(f\"Predictions for fold{fold_num} saved to '{predictions_file}'.\")\n",
        "\n",
        "def main():\n",
        "    # Loop over folds 1 to 10\n",
        "    for fold_num in range(1, 11):\n",
        "        process_fold(fold_num)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "037c4d3f-d1cd-404a-a566-1d659d6bf052",
        "id": "KQw9BkikrQJJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing fold1...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-d5ee5b6172ef>\u001b[0m in \u001b[0;36m<cell line: 176>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-64-d5ee5b6172ef>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# Loop over folds 1 to 10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfold_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mprocess_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-d5ee5b6172ef>\u001b[0m in \u001b[0;36mprocess_fold\u001b[0;34m(fold_num)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# Perform dummy encoding on both train and test datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mhousing_data_encoded_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhousing_data_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_columns_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0mhousing_data_encoded_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_test_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_columns_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/encoding.py\u001b[0m in \u001b[0;36mget_dummies\u001b[0;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_dummies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         result = _get_dummies_1d(\n\u001b[0m\u001b[1;32m    227\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/encoding.py\u001b[0m in \u001b[0;36m_get_dummies_1d\u001b[0;34m(data, prefix, prefix_sep, dummy_na, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;31m# if all NaN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdummy_na\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_empty_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0mcodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/encoding.py\u001b[0m in \u001b[0;36mget_empty_frame\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy import stats  # Importing for z-score calculation\n",
        "\n",
        "# Helper functions\n",
        "def preprocess_data(X_trn):\n",
        "    # Initialize the StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit the scaler on the training data and transform it\n",
        "    X_trn_scaled = scaler.fit_transform(X_trn)\n",
        "\n",
        "    # Return the scaled training data as a DataFrame (to preserve column names) and the scaler object\n",
        "    X_trn_scaled_df = pd.DataFrame(X_trn_scaled, columns=X_trn.columns)\n",
        "\n",
        "    return X_trn_scaled_df, scaler\n",
        "\n",
        "def calc_rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def adjusted_r_squared(r2, n, p):\n",
        "    # Calculate adjusted R-squared\n",
        "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "\n",
        "def process_fold(fold_num):\n",
        "    print(f\"\\nProcessing fold{fold_num}...\")\n",
        "\n",
        "    # Load the train, test, and test_y datasets\n",
        "    train_file_path = f'fold{fold_num}/train.csv'\n",
        "    test_file_path = f'fold{fold_num}/test.csv'\n",
        "    test_y_file_path = f'fold{fold_num}/test_y.csv'\n",
        "\n",
        "    # Define the list of features to remove (this is just an example, adjust as needed)\n",
        "    features_to_remove = [\"Longitude\", \"Latitude\", \"Street\", \"Utilities\", \"Condition_2\",\n",
        "                      \"Roof_Matl\", \"Heating\", \"Pool_QC\", \"Misc_Feature\",  \"Misc_Val\",\"Low_Qual_Fin_SF\", \"Sale_Type\", \"Pool_Area\"]  # Specify the features you want to remove\n",
        "\n",
        "\n",
        "\n",
        "    housing_data_train = pd.read_csv(train_file_path)\n",
        "    housing_data_test = pd.read_csv(test_file_path)\n",
        "    test_y_data = pd.read_csv(test_y_file_path)  # Load the actual Sale Price for the test set\n",
        "\n",
        "    # Call the remove_features function to remove unwanted features from X_trn\n",
        "    housing_data_train = remove_features(housing_data_train, features_to_remove)\n",
        "\n",
        "    # Ensure that test_y_data and housing_data_test have matching PIDs\n",
        "    merged_test_data = pd.merge(housing_data_test, test_y_data, on='PID', how='inner')\n",
        "\n",
        "    # Identify categorical columns\n",
        "    categorical_columns_test = ['MS_SubClass', 'MS_Zoning', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities',\n",
        "                           'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type',\n",
        "                           'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style', 'Roof_Matl', 'Exterior_1st',\n",
        "                           'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual', 'Exter_Cond', 'Foundation', 'Bsmt_Qual',\n",
        "                           'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating', 'Heating_QC',\n",
        "                           'Central_Air', 'Electrical', 'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type',\n",
        "                           'Garage_Finish', 'Garage_Qual', 'Garage_Cond', 'Paved_Drive', 'Pool_QC', 'Fence',\n",
        "                           'Misc_Feature', 'Sale_Type', 'Sale_Condition']\n",
        "\n",
        "    categorical_columns_train = ['MS_SubClass', 'MS_Zoning', 'Alley', 'Lot_Shape', 'Land_Contour',\n",
        "                            'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Bldg_Type',\n",
        "                            'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style',\n",
        "                            'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual',\n",
        "                            'Exter_Cond', 'Foundation', 'Bsmt_Qual', 'Bsmt_Cond', 'Bsmt_Exposure',\n",
        "                            'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating_QC', 'Central_Air',  'Electrical',\n",
        "                            'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type', 'Garage_Finish',\n",
        "                            'Garage_Qual', 'Garage_Cond', 'Paved_Drive', 'Fence',\n",
        "                            'Sale_Condition']\n",
        "\n",
        "\n",
        "    # Perform dummy encoding on both train and test datasets\n",
        "    housing_data_encoded_train = pd.get_dummies(housing_data_train, columns=categorical_columns_train, drop_first=True)\n",
        "    housing_data_encoded_test = pd.get_dummies(merged_test_data, columns=categorical_columns_test, drop_first=True)\n",
        "\n",
        "    # Align columns of train and test after dummy encoding:\n",
        "    all_columns = list(set(housing_data_encoded_train.columns) | set(housing_data_encoded_test.columns))\n",
        "    all_columns.remove('Sale_Price')  # Remove 'Sale_Price' if it's in all_columns\n",
        "\n",
        "    housing_data_encoded_train = housing_data_encoded_train.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    housing_data_encoded_test = housing_data_encoded_test.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    print(\"housing_data_encoded_train shape\", housing_data_encoded_train.shape)\n",
        "    print(\"housing_data_encoded_test shape\", housing_data_encoded_test.shape)\n",
        "\n",
        "    # Handle missing values by imputing them in train and test data\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    numeric_columns = housing_data_encoded_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numeric_columns = [col for col in numeric_columns if col not in ['Sale_Price', 'PID']]\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_train.copy()\n",
        "    housing_data_encoded_imputed_train[numeric_columns] = imputer.fit_transform(housing_data_encoded_train[numeric_columns])\n",
        "\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_test.copy()\n",
        "    housing_data_encoded_imputed_test[numeric_columns] = imputer.transform(housing_data_encoded_test[numeric_columns])\n",
        "    print(\"housing_data_encoded_imputed_train shape\", housing_data_encoded_imputed_train.shape)\n",
        "    print(\"housing_data_encoded_imputed_test shape\", housing_data_encoded_imputed_test.shape)\n",
        "\n",
        "    # Remove 'PID' from train and test datasets\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_imputed_train.drop(columns=['PID'])\n",
        "    if 'PID' in housing_data_encoded_imputed_test.columns:\n",
        "        housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.drop(columns=['PID'])\n",
        "\n",
        "    # Separate predictors (X) and target variable (y) for training\n",
        "    X_trn = housing_data_encoded_imputed_train.drop(columns='Sale_Price')\n",
        "    y_trn = np.log(housing_data_encoded_imputed_train['Sale_Price'])\n",
        "\n",
        "    # Scale the training data using preprocess_data function (returns DataFrame with column names)\n",
        "    X_trn_scaled, scaler = preprocess_data(X_trn)\n",
        "\n",
        "    # 1. Use Lasso regression for feature selection\n",
        "    lasso_model = Lasso(alpha=0.01)  # Adjust the alpha value as necessary\n",
        "    lasso_model.fit(X_trn_scaled, y_trn)\n",
        "    selected_features = X_trn_scaled.columns[lasso_model.coef_ != 0]\n",
        "    print(f\"Selected features after Lasso: {len(selected_features)} features\")\n",
        "\n",
        "    # Scale the test data using the same scaler\n",
        "    X_tst_scaled = pd.DataFrame(scaler.transform(housing_data_encoded_imputed_test.drop(columns=['Sale_Price'])), columns=housing_data_encoded_imputed_test.drop(columns=['Sale_Price']).columns)\n",
        "    print(\"X_tst_scaled shape\", X_tst_scaled.shape)\n",
        "\n",
        "    # 2. Grid Search for Ridge Regression to find the best alpha\n",
        "    alpha_values = np.exp(np.linspace(-5, 5, 100))\n",
        "    param_grid = {'alpha': alpha_values}\n",
        "\n",
        "    ridge_model = Ridge()\n",
        "    grid_search = GridSearchCV(ridge_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "    grid_search.fit(X_trn_scaled[selected_features], y_trn)\n",
        "    best_alpha = grid_search.best_params_['alpha']\n",
        "    print(\"Best alpha for Ridge regression:\", best_alpha)\n",
        "\n",
        "    # 3. Train the final Ridge model with the best alpha\n",
        "    final_ridge_model = Ridge(alpha=best_alpha)\n",
        "    final_ridge_model.fit(X_trn_scaled[selected_features], y_trn)\n",
        "\n",
        "    # Predict on the training set\n",
        "    y_trn_pred = final_ridge_model.predict(X_trn_scaled[selected_features])\n",
        "    r2_train = r2_score(y_trn, y_trn_pred)\n",
        "    adj_r2_train = adjusted_r_squared(r2_train, len(y_trn), X_trn_scaled[selected_features].shape[1])\n",
        "    train_rmse = calc_rmse(y_trn, y_trn_pred)\n",
        "    print(f\"Training R-squared: {r2_train}\")\n",
        "    print(f\"Adjusted R-squared (Training): {adj_r2_train}\")\n",
        "    print(f\"Training RMSE: {train_rmse}\")\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_tst_pred_log = final_ridge_model.predict(X_tst_scaled[selected_features])\n",
        "\n",
        "    # Reverse the log-transformation to get predictions in the original scale\n",
        "    y_tst_pred = np.exp(y_tst_pred_log)\n",
        "\n",
        "    # Merge the predictions with actual sale prices from test_y.csv using 'PID'\n",
        "    predictions = pd.DataFrame({\n",
        "        'PID': merged_test_data['PID'],\n",
        "        'Predicted_Sale_Price': y_tst_pred\n",
        "    })\n",
        "    merged_test_data = pd.merge(test_y_data, predictions, on='PID', how='inner')\n",
        "\n",
        "    # Calculate RMSE and R-squared for the test set using actual sale prices\n",
        "    test_rmse = calc_rmse(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    r2_test = r2_score(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    adj_r2_test = adjusted_r_squared(r2_test, len(merged_test_data), X_tst_scaled.shape[1])\n",
        "    print(f\"Test RMSE: {test_rmse}\")\n",
        "    print(f\"Test R-squared: {r2_test}\")\n",
        "    print(f\"Adjusted R-squared (Test): {adj_r2_test}\")\n",
        "\n",
        "    # Save the predictions to a CSV file\n",
        "    predictions_file = f'fold{fold_num}_test_predictions.csv'\n",
        "    predictions.to_csv(predictions_file, index=False)\n",
        "    print(f\"Predictions for fold{fold_num} saved to '{predictions_file}'.\")\n",
        "\n",
        "def main():\n",
        "    # Loop over folds 1 to 10\n",
        "    for fold_num in range(1, 11):\n",
        "        process_fold(fold_num)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "56e10d39-2a0d-4873-9630-f94f8fb55ecd",
        "id": "DflhX1FNrKc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing fold1...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-d5ee5b6172ef>\u001b[0m in \u001b[0;36m<cell line: 176>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-67-d5ee5b6172ef>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# Loop over folds 1 to 10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfold_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mprocess_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-d5ee5b6172ef>\u001b[0m in \u001b[0;36mprocess_fold\u001b[0;34m(fold_num)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# Perform dummy encoding on both train and test datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mhousing_data_encoded_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhousing_data_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_columns_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0mhousing_data_encoded_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_test_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_columns_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/encoding.py\u001b[0m in \u001b[0;36mget_dummies\u001b[0;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_dummies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         result = _get_dummies_1d(\n\u001b[0m\u001b[1;32m    227\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/encoding.py\u001b[0m in \u001b[0;36m_get_dummies_1d\u001b[0;34m(data, prefix, prefix_sep, dummy_na, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;31m# if all NaN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdummy_na\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_empty_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0mcodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/encoding.py\u001b[0m in \u001b[0;36mget_empty_frame\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_SjD7zjA8uZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy import stats  # Importing for z-score calculation\n",
        "\n",
        "# Helper functions\n",
        "def preprocess_data(X_trn):\n",
        "    # Initialize the StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit the scaler on the training data and transform it\n",
        "    X_trn_scaled = scaler.fit_transform(X_trn)\n",
        "\n",
        "    # Return the scaled training data as a DataFrame (to preserve column names) and the scaler object\n",
        "    X_trn_scaled_df = pd.DataFrame(X_trn_scaled, columns=X_trn.columns)\n",
        "\n",
        "    return X_trn_scaled_df, scaler\n",
        "\n",
        "def calc_rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def adjusted_r_squared(r2, n, p):\n",
        "    # Calculate adjusted R-squared\n",
        "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "\n",
        "def process_fold(fold_num):\n",
        "    print(f\"\\nProcessing fold{fold_num}...\")\n",
        "\n",
        "    # Load the train, test, and test_y datasets\n",
        "    train_file_path = f'fold{fold_num}/train.csv'\n",
        "    test_file_path = f'fold{fold_num}/test.csv'\n",
        "    test_y_file_path = f'fold{fold_num}/test_y.csv'\n",
        "\n",
        "    # Define the list of features to remove (this is just an example, adjust as needed)\n",
        "    features_to_remove = [\"Longitude\", \"Latitude\", \"Street\", \"Utilities\", \"Condition_2\",\n",
        "                      \"Roof_Matl\", \"Heating\", \"Pool_QC\", \"Misc_Feature\",  \"Misc_Val\",\"Low_Qual_Fin_SF\", \"Sale_Type\", \"Pool_Area\"]  # Specify the features you want to remove\n",
        "\n",
        "\n",
        "\n",
        "    housing_data_train = pd.read_csv(train_file_path)\n",
        "    housing_data_test = pd.read_csv(test_file_path)\n",
        "    test_y_data = pd.read_csv(test_y_file_path)  # Load the actual Sale Price for the test set\n",
        "\n",
        "    # Call the remove_features function to remove unwanted features from X_trn\n",
        "    housing_data_train = remove_features(housing_data_train, features_to_remove)\n",
        "\n",
        "    # Ensure that test_y_data and housing_data_test have matching PIDs\n",
        "    merged_test_data = pd.merge(housing_data_test, test_y_data, on='PID', how='inner')\n",
        "\n",
        "    # Identify categorical columns\n",
        "    categorical_columns_test = ['MS_SubClass', 'MS_Zoning', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities',\n",
        "                           'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type',\n",
        "                           'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style', 'Roof_Matl', 'Exterior_1st',\n",
        "                           'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual', 'Exter_Cond', 'Foundation', 'Bsmt_Qual',\n",
        "                           'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating', 'Heating_QC',\n",
        "                           'Central_Air', 'Electrical', 'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type',\n",
        "                           'Garage_Finish', 'Garage_Qual', 'Garage_Cond', 'Paved_Drive', 'Pool_QC', 'Fence',\n",
        "                           'Misc_Feature', 'Sale_Type', 'Sale_Condition']\n",
        "\n",
        "    categorical_columns_train = ['MS_SubClass', 'MS_Zoning', 'Alley', 'Lot_Shape', 'Land_Contour',\n",
        "                            'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Bldg_Type',\n",
        "                            'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style',\n",
        "                            'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual',\n",
        "                            'Exter_Cond', 'Foundation', 'Bsmt_Qual', 'Bsmt_Cond', 'Bsmt_Exposure',\n",
        "                            'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating_QC', 'Central_Air',  'Electrical',\n",
        "                            'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type', 'Garage_Finish',\n",
        "                            'Garage_Qual', 'Garage_Cond', 'Paved_Drive', 'Fence',\n",
        "                            'Sale_Type', 'Sale_Condition']\n",
        "\n",
        "\n",
        "    # Perform dummy encoding on both train and test datasets\n",
        "    housing_data_encoded_train = pd.get_dummies(housing_data_train, columns=categorical_columns_train, drop_first=True)\n",
        "    housing_data_encoded_test = pd.get_dummies(merged_test_data, columns=categorical_columns_test, drop_first=True)\n",
        "\n",
        "    # Align columns of train and test after dummy encoding:\n",
        "    all_columns = list(set(housing_data_encoded_train.columns) | set(housing_data_encoded_test.columns))\n",
        "    all_columns.remove('Sale_Price')  # Remove 'Sale_Price' if it's in all_columns\n",
        "\n",
        "    housing_data_encoded_train = housing_data_encoded_train.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    housing_data_encoded_test = housing_data_encoded_test.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    print(\"housing_data_encoded_train shape\", housing_data_encoded_train.shape)\n",
        "    print(\"housing_data_encoded_test shape\", housing_data_encoded_test.shape)\n",
        "\n",
        "    # Handle missing values by imputing them in train and test data\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    numeric_columns = housing_data_encoded_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numeric_columns = [col for col in numeric_columns if col not in ['Sale_Price', 'PID']]\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_train.copy()\n",
        "    housing_data_encoded_imputed_train[numeric_columns] = imputer.fit_transform(housing_data_encoded_train[numeric_columns])\n",
        "\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_test.copy()\n",
        "    housing_data_encoded_imputed_test[numeric_columns] = imputer.transform(housing_data_encoded_test[numeric_columns])\n",
        "    print(\"housing_data_encoded_imputed_train shape\", housing_data_encoded_imputed_train.shape)\n",
        "    print(\"housing_data_encoded_imputed_test shape\", housing_data_encoded_imputed_test.shape)\n",
        "\n",
        "    # Remove 'PID' from train and test datasets\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_imputed_train.drop(columns=['PID'])\n",
        "    if 'PID' in housing_data_encoded_imputed_test.columns:\n",
        "        housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.drop(columns=['PID'])\n",
        "\n",
        "    # Separate predictors (X) and target variable (y) for training\n",
        "    X_trn = housing_data_encoded_imputed_train.drop(columns='Sale_Price')\n",
        "    y_trn = np.log(housing_data_encoded_imputed_train['Sale_Price'])\n",
        "\n",
        "    # Scale the training data using preprocess_data function (returns DataFrame with column names)\n",
        "    X_trn_scaled, scaler = preprocess_data(X_trn)\n",
        "\n",
        "    # 1. Use Lasso regression for feature selection\n",
        "    lasso_model = Lasso(alpha=0.01)  # Adjust the alpha value as necessary\n",
        "    lasso_model.fit(X_trn_scaled, y_trn)\n",
        "    selected_features = X_trn_scaled.columns[lasso_model.coef_ != 0]\n",
        "    print(f\"Selected features after Lasso: {len(selected_features)} features\")\n",
        "\n",
        "    # Scale the test data using the same scaler\n",
        "    X_tst_scaled = pd.DataFrame(scaler.transform(housing_data_encoded_imputed_test.drop(columns=['Sale_Price'])), columns=housing_data_encoded_imputed_test.drop(columns=['Sale_Price']).columns)\n",
        "    print(\"X_tst_scaled shape\", X_tst_scaled.shape)\n",
        "\n",
        "    # 2. Grid Search for Ridge Regression to find the best alpha\n",
        "    alpha_values = np.exp(np.linspace(-5, 5, 100))\n",
        "    param_grid = {'alpha': alpha_values}\n",
        "\n",
        "    ridge_model = Ridge()\n",
        "    grid_search = GridSearchCV(ridge_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "    grid_search.fit(X_trn_scaled[selected_features], y_trn)\n",
        "    best_alpha = grid_search.best_params_['alpha']\n",
        "    print(\"Best alpha for Ridge regression:\", best_alpha)\n",
        "\n",
        "    # 3. Train the final Ridge model with the best alpha\n",
        "    final_ridge_model = Ridge(alpha=best_alpha)\n",
        "    final_ridge_model.fit(X_trn_scaled[selected_features], y_trn)\n",
        "\n",
        "    # Predict on the training set\n",
        "    y_trn_pred = final_ridge_model.predict(X_trn_scaled[selected_features])\n",
        "    r2_train = r2_score(y_trn, y_trn_pred)\n",
        "    adj_r2_train = adjusted_r_squared(r2_train, len(y_trn), X_trn_scaled[selected_features].shape[1])\n",
        "    train_rmse = calc_rmse(y_trn, y_trn_pred)\n",
        "    print(f\"Training R-squared: {r2_train}\")\n",
        "    print(f\"Adjusted R-squared (Training): {adj_r2_train}\")\n",
        "    print(f\"Training RMSE: {train_rmse}\")\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_tst_pred_log = final_ridge_model.predict(X_tst_scaled[selected_features])\n",
        "\n",
        "    # Reverse the log-transformation to get predictions in the original scale\n",
        "    y_tst_pred = np.exp(y_tst_pred_log)\n",
        "\n",
        "    # Merge the predictions with actual sale prices from test_y.csv using 'PID'\n",
        "    predictions = pd.DataFrame({\n",
        "        'PID': merged_test_data['PID'],\n",
        "        'Predicted_Sale_Price': y_tst_pred\n",
        "    })\n",
        "    merged_test_data = pd.merge(test_y_data, predictions, on='PID', how='inner')\n",
        "\n",
        "    # Calculate RMSE and R-squared for the test set using actual sale prices\n",
        "    test_rmse = calc_rmse(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    r2_test = r2_score(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    adj_r2_test = adjusted_r_squared(r2_test, len(merged_test_data), X_tst_scaled.shape[1])\n",
        "    print(f\"Test RMSE: {test_rmse}\")\n",
        "    print(f\"Test R-squared: {r2_test}\")\n",
        "    print(f\"Adjusted R-squared (Test): {adj_r2_test}\")\n",
        "\n",
        "    # Save the predictions to a CSV file\n",
        "    predictions_file = f'fold{fold_num}_test_predictions.csv'\n",
        "    predictions.to_csv(predictions_file, index=False)\n",
        "    print(f\"Predictions for fold{fold_num} saved to '{predictions_file}'.\")\n",
        "\n",
        "def main():\n",
        "    # Loop over folds 1 to 10\n",
        "    for fold_num in range(1, 11):\n",
        "        process_fold(fold_num)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "a4019c1a-6034-4303-b021-b95e2c9cc3a4",
        "id": "LwSHHFGE8u8L"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing fold1...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-4e733b8cf4f3>\u001b[0m in \u001b[0;36m<cell line: 176>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-66-4e733b8cf4f3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# Loop over folds 1 to 10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfold_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mprocess_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-66-4e733b8cf4f3>\u001b[0m in \u001b[0;36mprocess_fold\u001b[0;34m(fold_num)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# Perform dummy encoding on both train and test datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mhousing_data_encoded_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhousing_data_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_columns_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0mhousing_data_encoded_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_test_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_columns_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/encoding.py\u001b[0m in \u001b[0;36mget_dummies\u001b[0;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_dummies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         result = _get_dummies_1d(\n\u001b[0m\u001b[1;32m    227\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/encoding.py\u001b[0m in \u001b[0;36m_get_dummies_1d\u001b[0;34m(data, prefix, prefix_sep, dummy_na, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;31m# if all NaN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdummy_na\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_empty_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0mcodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/encoding.py\u001b[0m in \u001b[0;36mget_empty_frame\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy import stats  # Importing for z-score calculation\n",
        "\n",
        "# Helper functions\n",
        "def preprocess_data(X_trn):\n",
        "    # Initialize the StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit the scaler on the training data and transform it\n",
        "    X_trn_scaled = scaler.fit_transform(X_trn)\n",
        "\n",
        "    # Return the scaled training data as a DataFrame (to preserve column names) and the scaler object\n",
        "    X_trn_scaled_df = pd.DataFrame(X_trn_scaled, columns=X_trn.columns)\n",
        "\n",
        "    return X_trn_scaled_df, scaler\n",
        "\n",
        "def calc_rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def adjusted_r_squared(r2, n, p):\n",
        "    # Calculate adjusted R-squared\n",
        "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "\n",
        "def main():\n",
        "    # Load the train, test, and test_y datasets\n",
        "    train_file_path = 'fold8/train.csv'\n",
        "    test_file_path = 'fold8/test.csv'\n",
        "    test_y_file_path = 'fold8/test_y.csv'\n",
        "    housing_data_train = pd.read_csv(train_file_path)\n",
        "    housing_data_test = pd.read_csv(test_file_path)\n",
        "    test_y_data = pd.read_csv(test_y_file_path)  # Load the actual Sale Price for the test set\n",
        "\n",
        "    # Ensure that test_y_data and housing_data_test have matching PIDs\n",
        "    merged_test_data = pd.merge(housing_data_test, test_y_data, on='PID', how='inner')\n",
        "\n",
        "    # Identify categorical columns\n",
        "    categorical_columns = ['MS_SubClass', 'MS_Zoning', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities',\n",
        "                           'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type',\n",
        "                           'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style', 'Roof_Matl', 'Exterior_1st',\n",
        "                           'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual', 'Exter_Cond', 'Foundation', 'Bsmt_Qual',\n",
        "                           'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating', 'Heating_QC',\n",
        "                           'Central_Air', 'Electrical', 'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type',\n",
        "                           'Garage_Finish', 'Garage_Qual', 'Garage_Cond', 'Paved_Drive', 'Pool_QC', 'Fence',\n",
        "                           'Misc_Feature', 'Sale_Type', 'Sale_Condition']\n",
        "\n",
        "    # Perform dummy encoding on both train and test datasets\n",
        "    housing_data_encoded_train = pd.get_dummies(housing_data_train, columns=categorical_columns, drop_first=True)\n",
        "    housing_data_encoded_test = pd.get_dummies(merged_test_data, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "    # Align columns of train and test after dummy encoding:\n",
        "    all_columns = list(set(housing_data_encoded_train.columns) | set(housing_data_encoded_test.columns))\n",
        "    all_columns.remove('Sale_Price')  # Remove 'Sale_Price' if it's in all_columns\n",
        "\n",
        "    housing_data_encoded_train = housing_data_encoded_train.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    housing_data_encoded_test = housing_data_encoded_test.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    print(\"housing_data_encoded_train shape\", housing_data_encoded_train.shape)\n",
        "    print(\"housing_data_encoded_test shape\", housing_data_encoded_test.shape)\n",
        "\n",
        "    # Handle missing values by imputing them in train and test data\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    numeric_columns = housing_data_encoded_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    #numeric_columns = [col for col in numeric_columns if col != 'Sale_Price']\n",
        "    # Ensure 'Sale_Price' and 'PID' are excluded from numeric_columns\n",
        "    numeric_columns = [col for col in numeric_columns if col not in ['Sale_Price', 'PID']]\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_train.copy()\n",
        "    housing_data_encoded_imputed_train[numeric_columns] = imputer.fit_transform(housing_data_encoded_train[numeric_columns])\n",
        "\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_test.copy()\n",
        "    housing_data_encoded_imputed_test[numeric_columns] = imputer.transform(housing_data_encoded_test[numeric_columns])\n",
        "    print(\"housing_data_encoded_imputed_train shape\", housing_data_encoded_imputed_train.shape)\n",
        "    print(\"housing_data_encoded_imputed_test shape\", housing_data_encoded_imputed_test.shape)\n",
        "\n",
        "    # Remove 'PID' from train and test datasets\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_imputed_train.drop(columns=['PID'])\n",
        "    if 'PID' in housing_data_encoded_imputed_test.columns:\n",
        "        housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.drop(columns=['PID'])\n",
        "\n",
        "    # Separate predictors (X) and target variable (y) for training\n",
        "    X_trn = housing_data_encoded_imputed_train.drop(columns='Sale_Price')\n",
        "    y_trn = np.log(housing_data_encoded_imputed_train['Sale_Price'])\n",
        "\n",
        "\n",
        "    # EXperiment starts\n",
        "\n",
        "    # Experiment ends\n",
        "    # Scale the training data using preprocess_data function (returns DataFrame with column names)\n",
        "    X_trn_scaled, scaler = preprocess_data(X_trn)\n",
        "\n",
        "    # 1. Use Lasso regression for feature selection\n",
        "    lasso_model = Lasso(alpha=0.01)  # Adjust the alpha value as necessary\n",
        "    #lasso_model.fit(X_trn_scaled, y_trn)\n",
        "    lasso_model.fit(X_trn_scaled, y_trn)\n",
        "    # Select the features with non-zero coefficients\n",
        "    selected_features = X_trn_scaled.columns[lasso_model.coef_ != 0]\n",
        "    print(f\"Selected features after Lasso: {len(selected_features)} features\")\n",
        "\n",
        "    # Scale the test data using the same scaler\n",
        "    X_tst_scaled = pd.DataFrame(scaler.transform(housing_data_encoded_imputed_test.drop(columns=['Sale_Price'])), columns=housing_data_encoded_imputed_test.drop(columns=['Sale_Price']).columns)\n",
        "    print(\"X_tst_scaled shape\", X_tst_scaled.shape)\n",
        "\n",
        "\n",
        "\n",
        "    # 2. Grid Search for Ridge Regression to find the best alpha\n",
        "    #param_grid = {'alpha': [0.01, 0.1, 1.0, 10, 100]}\n",
        "\n",
        "    # Generate 100 alpha values using np.exp and np.linspace\n",
        "    alpha_values = np.exp(np.linspace(-5, 5, 100))\n",
        "\n",
        "    # Set the alpha values in the parameter grid for Lasso\n",
        "    param_grid = {'alpha': alpha_values}\n",
        "\n",
        "\n",
        "    ridge_model = Ridge()\n",
        "    grid_search = GridSearchCV(ridge_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "    #grid_search.fit(X_trn_scaled[selected_features], y_trn)\n",
        "    grid_search.fit(X_trn_scaled[selected_features], y_trn)\n",
        "    # Get the best alpha value from grid search\n",
        "    best_alpha = grid_search.best_params_['alpha']\n",
        "    print(\"Best alpha for Ridge regression:\", best_alpha)\n",
        "\n",
        "    # 3. Train the final Ridge model with the best alpha\n",
        "    final_ridge_model = Ridge(alpha=best_alpha)\n",
        "    #final_ridge_model.fit(X_trn_scaled[selected_features], y_trn)\n",
        "    final_ridge_model.fit(X_trn_scaled[selected_features], y_trn)\n",
        "    # Predict on the training set\n",
        "    y_trn_pred = final_ridge_model.predict(X_trn_scaled[selected_features])\n",
        "\n",
        "    # Calculate R-squared and RMSE for training data\n",
        "    #r2_train = r2_score(y_trn, y_trn_pred)\n",
        "    r2_train = r2_score(y_trn, y_trn_pred)\n",
        "    adj_r2_train = adjusted_r_squared(r2_train, len(y_trn), X_trn_scaled[selected_features].shape[1])\n",
        "    train_rmse = calc_rmse(y_trn, y_trn_pred)\n",
        "    print(f\"Training R-squared: {r2_train}\")\n",
        "    print(f\"Adjusted R-squared (Training): {adj_r2_train}\")\n",
        "    print(f\"Training RMSE: {train_rmse}\")\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_tst_pred_log = final_ridge_model.predict(X_tst_scaled[selected_features])\n",
        "\n",
        "    # Reverse the log-transformation to get predictions in the original scale\n",
        "    y_tst_pred = np.exp(y_tst_pred_log)\n",
        "\n",
        "    # Merge the predictions with actual sale prices from test_y.csv using 'PID'\n",
        "    predictions = pd.DataFrame({\n",
        "        'PID': merged_test_data['PID'],\n",
        "        'Predicted_Sale_Price': y_tst_pred\n",
        "    })\n",
        "    merged_test_data = pd.merge(test_y_data, predictions, on='PID', how='inner')\n",
        "\n",
        "    # Calculate RMSE and R-squared for the test set using actual sale prices\n",
        "    test_rmse = calc_rmse(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    r2_test = r2_score(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    adj_r2_test = adjusted_r_squared(r2_test, len(merged_test_data), X_tst_scaled.shape[1])\n",
        "    print(f\"Test RMSE: {test_rmse}\")\n",
        "    print(f\"Test R-squared: {r2_test}\")\n",
        "    print(f\"Adjusted R-squared (Test): {adj_r2_test}\")\n",
        "\n",
        "    # Save the predictions to a CSV file\n",
        "    predictions.to_csv('test_predictions.csv', index=False)\n",
        "    print(\"Predictions for test set saved to 'test_predictions.csv'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "mXq0OTtEm829",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f4fe26b-f5b5-4b45-b03a-d9c1a16117b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "housing_data_encoded_train shape (2051, 307)\n",
            "housing_data_encoded_test shape (879, 307)\n",
            "housing_data_encoded_imputed_train shape (2051, 307)\n",
            "housing_data_encoded_imputed_test shape (879, 307)\n",
            "Selected features after Lasso: 69 features\n",
            "X_tst_scaled shape (879, 305)\n",
            "Best alpha for Ridge regression: 148.4131591025766\n",
            "Training R-squared: 0.9162166366964976\n",
            "Adjusted R-squared (Training): 0.9132983872931953\n",
            "Training RMSE: 0.11969708675889157\n",
            "Test RMSE: 0.13213083693906497\n",
            "Test R-squared: 0.8868549781982086\n",
            "Adjusted R-squared (Test): 0.8266294430332062\n",
            "Predictions for test set saved to 'test_predictions.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run in a For loop"
      ],
      "metadata": {
        "id": "0D9gYooZ9P4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy import stats  # Importing for z-score calculation\n",
        "\n",
        "# Helper functions\n",
        "def preprocess_data(X_trn):\n",
        "    # Initialize the StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit the scaler on the training data and transform it\n",
        "    X_trn_scaled = scaler.fit_transform(X_trn)\n",
        "\n",
        "    # Return the scaled training data as a DataFrame (to preserve column names) and the scaler object\n",
        "    X_trn_scaled_df = pd.DataFrame(X_trn_scaled, columns=X_trn.columns)\n",
        "\n",
        "    return X_trn_scaled_df, scaler\n",
        "\n",
        "def calc_rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def adjusted_r_squared(r2, n, p):\n",
        "    # Calculate adjusted R-squared\n",
        "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "\n",
        "def process_fold(fold_num):\n",
        "    print(f\"\\nProcessing fold{fold_num}...\")\n",
        "\n",
        "    # Load the train, test, and test_y datasets\n",
        "    train_file_path = f'fold{fold_num}/train.csv'\n",
        "    test_file_path = f'fold{fold_num}/test.csv'\n",
        "    test_y_file_path = f'fold{fold_num}/test_y.csv'\n",
        "\n",
        "    housing_data_train = pd.read_csv(train_file_path)\n",
        "    housing_data_test = pd.read_csv(test_file_path)\n",
        "    test_y_data = pd.read_csv(test_y_file_path)  # Load the actual Sale Price for the test set\n",
        "\n",
        "    # Ensure that test_y_data and housing_data_test have matching PIDs\n",
        "    merged_test_data = pd.merge(housing_data_test, test_y_data, on='PID', how='inner')\n",
        "\n",
        "    # Identify categorical columns\n",
        "    categorical_columns = ['MS_SubClass', 'MS_Zoning', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities',\n",
        "                           'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type',\n",
        "                           'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style', 'Roof_Matl', 'Exterior_1st',\n",
        "                           'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual', 'Exter_Cond', 'Foundation', 'Bsmt_Qual',\n",
        "                           'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating', 'Heating_QC',\n",
        "                           'Central_Air', 'Electrical', 'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type',\n",
        "                           'Garage_Finish', 'Garage_Qual', 'Garage_Cond', 'Paved_Drive', 'Pool_QC', 'Fence',\n",
        "                           'Misc_Feature', 'Sale_Type', 'Sale_Condition']\n",
        "\n",
        "    # Perform dummy encoding on both train and test datasets\n",
        "    housing_data_encoded_train = pd.get_dummies(housing_data_train, columns=categorical_columns, drop_first=True)\n",
        "    housing_data_encoded_test = pd.get_dummies(merged_test_data, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "    # Align columns of train and test after dummy encoding:\n",
        "    all_columns = list(set(housing_data_encoded_train.columns) | set(housing_data_encoded_test.columns))\n",
        "    all_columns.remove('Sale_Price')  # Remove 'Sale_Price' if it's in all_columns\n",
        "\n",
        "    housing_data_encoded_train = housing_data_encoded_train.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    housing_data_encoded_test = housing_data_encoded_test.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    print(\"housing_data_encoded_train shape\", housing_data_encoded_train.shape)\n",
        "    print(\"housing_data_encoded_test shape\", housing_data_encoded_test.shape)\n",
        "\n",
        "    # Handle missing values by imputing them in train and test data\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    numeric_columns = housing_data_encoded_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numeric_columns = [col for col in numeric_columns if col not in ['Sale_Price', 'PID']]\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_train.copy()\n",
        "    housing_data_encoded_imputed_train[numeric_columns] = imputer.fit_transform(housing_data_encoded_train[numeric_columns])\n",
        "\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_test.copy()\n",
        "    housing_data_encoded_imputed_test[numeric_columns] = imputer.transform(housing_data_encoded_test[numeric_columns])\n",
        "    print(\"housing_data_encoded_imputed_train shape\", housing_data_encoded_imputed_train.shape)\n",
        "    print(\"housing_data_encoded_imputed_test shape\", housing_data_encoded_imputed_test.shape)\n",
        "\n",
        "    # Remove 'PID' from train and test datasets\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_imputed_train.drop(columns=['PID'])\n",
        "    if 'PID' in housing_data_encoded_imputed_test.columns:\n",
        "        housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.drop(columns=['PID'])\n",
        "\n",
        "    # Separate predictors (X) and target variable (y) for training\n",
        "    X_trn = housing_data_encoded_imputed_train.drop(columns='Sale_Price')\n",
        "    y_trn = np.log(housing_data_encoded_imputed_train['Sale_Price'])\n",
        "\n",
        "    # Scale the training data using preprocess_data function (returns DataFrame with column names)\n",
        "    X_trn_scaled, scaler = preprocess_data(X_trn)\n",
        "\n",
        "    # 1. Use Lasso regression for feature selection\n",
        "    lasso_model = Lasso(alpha=0.01)  # Adjust the alpha value as necessary\n",
        "    lasso_model.fit(X_trn_scaled, y_trn)\n",
        "    selected_features = X_trn_scaled.columns[lasso_model.coef_ != 0]\n",
        "    print(f\"Selected features after Lasso: {len(selected_features)} features\")\n",
        "\n",
        "    # Scale the test data using the same scaler\n",
        "    X_tst_scaled = pd.DataFrame(scaler.transform(housing_data_encoded_imputed_test.drop(columns=['Sale_Price'])), columns=housing_data_encoded_imputed_test.drop(columns=['Sale_Price']).columns)\n",
        "    print(\"X_tst_scaled shape\", X_tst_scaled.shape)\n",
        "\n",
        "    # 2. Grid Search for Ridge Regression to find the best alpha\n",
        "    alpha_values = np.exp(np.linspace(-5, 5, 100))\n",
        "    param_grid = {'alpha': alpha_values}\n",
        "\n",
        "    ridge_model = Ridge()\n",
        "    grid_search = GridSearchCV(ridge_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "    grid_search.fit(X_trn_scaled[selected_features], y_trn)\n",
        "    best_alpha = grid_search.best_params_['alpha']\n",
        "    print(\"Best alpha for Ridge regression:\", best_alpha)\n",
        "\n",
        "    # 3. Train the final Ridge model with the best alpha\n",
        "    final_ridge_model = Ridge(alpha=best_alpha)\n",
        "    final_ridge_model.fit(X_trn_scaled[selected_features], y_trn)\n",
        "\n",
        "    # Predict on the training set\n",
        "    y_trn_pred = final_ridge_model.predict(X_trn_scaled[selected_features])\n",
        "    r2_train = r2_score(y_trn, y_trn_pred)\n",
        "    adj_r2_train = adjusted_r_squared(r2_train, len(y_trn), X_trn_scaled[selected_features].shape[1])\n",
        "    train_rmse = calc_rmse(y_trn, y_trn_pred)\n",
        "    print(f\"Training R-squared: {r2_train}\")\n",
        "    print(f\"Adjusted R-squared (Training): {adj_r2_train}\")\n",
        "    print(f\"Training RMSE: {train_rmse}\")\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_tst_pred_log = final_ridge_model.predict(X_tst_scaled[selected_features])\n",
        "\n",
        "    # Reverse the log-transformation to get predictions in the original scale\n",
        "    y_tst_pred = np.exp(y_tst_pred_log)\n",
        "\n",
        "    # Merge the predictions with actual sale prices from test_y.csv using 'PID'\n",
        "    predictions = pd.DataFrame({\n",
        "        'PID': merged_test_data['PID'],\n",
        "        'Predicted_Sale_Price': y_tst_pred\n",
        "    })\n",
        "    merged_test_data = pd.merge(test_y_data, predictions, on='PID', how='inner')\n",
        "\n",
        "    # Calculate RMSE and R-squared for the test set using actual sale prices\n",
        "    test_rmse = calc_rmse(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    r2_test = r2_score(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    adj_r2_test = adjusted_r_squared(r2_test, len(merged_test_data), X_tst_scaled.shape[1])\n",
        "    print(f\"Test RMSE: {test_rmse}\")\n",
        "    print(f\"Test R-squared: {r2_test}\")\n",
        "    print(f\"Adjusted R-squared (Test): {adj_r2_test}\")\n",
        "\n",
        "    # Save the predictions to a CSV file\n",
        "    predictions_file = f'fold{fold_num}_test_predictions.csv'\n",
        "    predictions.to_csv(predictions_file, index=False)\n",
        "    print(f\"Predictions for fold{fold_num} saved to '{predictions_file}'.\")\n",
        "\n",
        "def main():\n",
        "    # Loop over folds 1 to 10\n",
        "    for fold_num in range(1, 11):\n",
        "        process_fold(fold_num)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xuRMJVi9Snj",
        "outputId": "2b3a6105-6e05-4ace-ecf4-fbcdded40e7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing fold1...\n",
            "housing_data_encoded_train shape (2051, 307)\n",
            "housing_data_encoded_test shape (879, 307)\n",
            "housing_data_encoded_imputed_train shape (2051, 307)\n",
            "housing_data_encoded_imputed_test shape (879, 307)\n",
            "Selected features after Lasso: 62 features\n",
            "X_tst_scaled shape (879, 305)\n",
            "Best alpha for Ridge regression: 99.08316975251431\n",
            "Training R-squared: 0.9295738799235193\n",
            "Adjusted R-squared (Training): 0.9273774918728444\n",
            "Training RMSE: 0.11054379841181113\n",
            "Test RMSE: 0.15655045343161975\n",
            "Test R-squared: 0.8350420203182289\n",
            "Adjusted R-squared (Test): 0.7472371620233944\n",
            "Predictions for fold1 saved to 'fold1_test_predictions.csv'.\n",
            "\n",
            "Processing fold2...\n",
            "housing_data_encoded_train shape (2051, 307)\n",
            "housing_data_encoded_test shape (879, 307)\n",
            "housing_data_encoded_imputed_train shape (2051, 307)\n",
            "housing_data_encoded_imputed_test shape (879, 307)\n",
            "Selected features after Lasso: 71 features\n",
            "X_tst_scaled shape (879, 305)\n",
            "Best alpha for Ridge regression: 148.4131591025766\n",
            "Training R-squared: 0.9205382592349518\n",
            "Adjusted R-squared (Training): 0.917687433770415\n",
            "Training RMSE: 0.11591902049745081\n",
            "Test RMSE: 0.14137164042570988\n",
            "Test R-squared: 0.8741952655533404\n",
            "Adjusted R-squared (Test): 0.8072311398880154\n",
            "Predictions for fold2 saved to 'fold2_test_predictions.csv'.\n",
            "\n",
            "Processing fold3...\n",
            "housing_data_encoded_train shape (2051, 307)\n",
            "housing_data_encoded_test shape (879, 307)\n",
            "housing_data_encoded_imputed_train shape (2051, 307)\n",
            "housing_data_encoded_imputed_test shape (879, 307)\n",
            "Selected features after Lasso: 70 features\n",
            "X_tst_scaled shape (879, 305)\n",
            "Best alpha for Ridge regression: 148.4131591025766\n",
            "Training R-squared: 0.917636574570707\n",
            "Adjusted R-squared (Training): 0.9147247362979543\n",
            "Training RMSE: 0.11976985188879566\n",
            "Test RMSE: 0.13699373525079148\n",
            "Test R-squared: 0.8723559667093188\n",
            "Adjusted R-squared (Test): 0.8044128076278916\n",
            "Predictions for fold3 saved to 'fold3_test_predictions.csv'.\n",
            "\n",
            "Processing fold4...\n",
            "housing_data_encoded_train shape (2051, 306)\n",
            "housing_data_encoded_test shape (879, 306)\n",
            "housing_data_encoded_imputed_train shape (2051, 306)\n",
            "housing_data_encoded_imputed_test shape (879, 306)\n",
            "Selected features after Lasso: 69 features\n",
            "X_tst_scaled shape (879, 304)\n",
            "Best alpha for Ridge regression: 89.56364545447869\n",
            "Training R-squared: 0.9259457669108139\n",
            "Adjusted R-squared (Training): 0.9233663918057388\n",
            "Training RMSE: 0.11097041670528318\n",
            "Test RMSE: 0.15173804422061443\n",
            "Test R-squared: 0.860799084487713\n",
            "Adjusted R-squared (Test): 0.7870759515334704\n",
            "Predictions for fold4 saved to 'fold4_test_predictions.csv'.\n",
            "\n",
            "Processing fold5...\n",
            "housing_data_encoded_train shape (2051, 307)\n",
            "housing_data_encoded_test shape (879, 307)\n",
            "housing_data_encoded_imputed_train shape (2051, 307)\n",
            "housing_data_encoded_imputed_test shape (879, 307)\n",
            "Selected features after Lasso: 73 features\n",
            "X_tst_scaled shape (879, 305)\n",
            "Best alpha for Ridge regression: 148.4131591025766\n",
            "Training R-squared: 0.9187867993484992\n",
            "Adjusted R-squared (Training): 0.915788031696724\n",
            "Training RMSE: 0.11886377118276717\n",
            "Test RMSE: 0.12936988010858816\n",
            "Test R-squared: 0.8864774968718119\n",
            "Adjusted R-squared (Test): 0.8260510336011359\n",
            "Predictions for fold5 saved to 'fold5_test_predictions.csv'.\n",
            "\n",
            "Processing fold6...\n",
            "housing_data_encoded_train shape (2051, 307)\n",
            "housing_data_encoded_test shape (879, 307)\n",
            "housing_data_encoded_imputed_train shape (2051, 307)\n",
            "housing_data_encoded_imputed_test shape (879, 307)\n",
            "Selected features after Lasso: 63 features\n",
            "X_tst_scaled shape (879, 305)\n",
            "Best alpha for Ridge regression: 121.2651897160414\n",
            "Training R-squared: 0.9281241346846358\n",
            "Adjusted R-squared (Training): 0.9258452320601427\n",
            "Training RMSE: 0.11068550129622037\n",
            "Test RMSE: 0.1627914786541962\n",
            "Test R-squared: 0.829879285942752\n",
            "Adjusted R-squared (Test): 0.7393263753189114\n",
            "Predictions for fold6 saved to 'fold6_test_predictions.csv'.\n",
            "\n",
            "Processing fold7...\n",
            "housing_data_encoded_train shape (2051, 307)\n",
            "housing_data_encoded_test shape (879, 307)\n",
            "housing_data_encoded_imputed_train shape (2051, 307)\n",
            "housing_data_encoded_imputed_test shape (879, 307)\n",
            "Selected features after Lasso: 66 features\n",
            "X_tst_scaled shape (879, 305)\n",
            "Best alpha for Ridge regression: 148.4131591025766\n",
            "Training R-squared: 0.9193185637605393\n",
            "Adjusted R-squared (Training): 0.9166346046920896\n",
            "Training RMSE: 0.11571300233972191\n",
            "Test RMSE: 0.1530427514054921\n",
            "Test R-squared: 0.8589990504846594\n",
            "Adjusted R-squared (Test): 0.7839461890497923\n",
            "Predictions for fold7 saved to 'fold7_test_predictions.csv'.\n",
            "\n",
            "Processing fold8...\n",
            "housing_data_encoded_train shape (2051, 307)\n",
            "housing_data_encoded_test shape (879, 307)\n",
            "housing_data_encoded_imputed_train shape (2051, 307)\n",
            "housing_data_encoded_imputed_test shape (879, 307)\n",
            "Selected features after Lasso: 69 features\n",
            "X_tst_scaled shape (879, 305)\n",
            "Best alpha for Ridge regression: 148.4131591025766\n",
            "Training R-squared: 0.9162166366964976\n",
            "Adjusted R-squared (Training): 0.9132983872931953\n",
            "Training RMSE: 0.11969708675889157\n",
            "Test RMSE: 0.13213083693906497\n",
            "Test R-squared: 0.8868549781982086\n",
            "Adjusted R-squared (Test): 0.8266294430332062\n",
            "Predictions for fold8 saved to 'fold8_test_predictions.csv'.\n",
            "\n",
            "Processing fold9...\n",
            "housing_data_encoded_train shape (2051, 306)\n",
            "housing_data_encoded_test shape (879, 306)\n",
            "housing_data_encoded_imputed_train shape (2051, 306)\n",
            "housing_data_encoded_imputed_test shape (879, 306)\n",
            "Selected features after Lasso: 67 features\n",
            "X_tst_scaled shape (879, 304)\n",
            "Best alpha for Ridge regression: 109.61450350070197\n",
            "Training R-squared: 0.9233011056443426\n",
            "Adjusted R-squared (Training): 0.9207096654417057\n",
            "Training RMSE: 0.11195133933340953\n",
            "Test RMSE: 0.15999244707806856\n",
            "Test R-squared: 0.8513378632623054\n",
            "Adjusted R-squared (Test): 0.7726039093106345\n",
            "Predictions for fold9 saved to 'fold9_test_predictions.csv'.\n",
            "\n",
            "Processing fold10...\n",
            "housing_data_encoded_train shape (2051, 307)\n",
            "housing_data_encoded_test shape (879, 307)\n",
            "housing_data_encoded_imputed_train shape (2051, 307)\n",
            "housing_data_encoded_imputed_test shape (879, 307)\n",
            "Selected features after Lasso: 71 features\n",
            "X_tst_scaled shape (879, 305)\n",
            "Best alpha for Ridge regression: 148.4131591025766\n",
            "Training R-squared: 0.9172182308348976\n",
            "Adjusted R-squared (Training): 0.9142482936895099\n",
            "Training RMSE: 0.11891156163351171\n",
            "Test RMSE: 0.1397521488787725\n",
            "Test R-squared: 0.8737446583557655\n",
            "Adjusted R-squared (Test): 0.8065406806917315\n",
            "Predictions for fold10 saved to 'fold10_test_predictions.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess to remove features"
      ],
      "metadata": {
        "id": "cmcG0UvnnCf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_rare_categories(df, categorical_columns, threshold=0.02):\n",
        "    \"\"\"\n",
        "    Combine rare categories into 'Other' for categorical columns in the dataframe.\n",
        "\n",
        "    Args:\n",
        "    df: pandas DataFrame - The dataset.\n",
        "    categorical_columns: list - List of categorical columns to process.\n",
        "    threshold: float - The minimum proportion a category should have to not be grouped into 'Other'.\n",
        "\n",
        "    Returns:\n",
        "    df: pandas DataFrame - The dataset with rare categories combined into 'Other'.\n",
        "    \"\"\"\n",
        "    for col in categorical_columns:\n",
        "        # Calculate the frequency of each category in the column\n",
        "        freq = df[col].value_counts(normalize=True)\n",
        "\n",
        "        # Categories to be replaced with 'Other'\n",
        "        rare_categories = freq[freq < threshold].index\n",
        "\n",
        "        # Replace rare categories with 'Other'\n",
        "        df[col] = df[col].replace(rare_categories, 'Other')\n",
        "    print(\"df\", df)\n",
        "    return df\n",
        "def remove_high_nan_columns(df, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Remove columns from the DataFrame that have more than the specified threshold of NaN values.\n",
        "\n",
        "    Args:\n",
        "    df: pandas DataFrame - The dataset.\n",
        "    threshold: float - The proportion of NaN values required to drop the column (default is 0.5 for 50%).\n",
        "\n",
        "    Returns:\n",
        "    df: pandas DataFrame - The dataset with high NaN columns removed.\n",
        "    \"\"\"\n",
        "    # Calculate the proportion of NaN values for each column\n",
        "    nan_proportions = df.isnull().mean()\n",
        "\n",
        "    print(\"NaN Proportions for each column:\")\n",
        "    print(nan_proportions)\n",
        "    # Identify columns with NaN proportions greater than the threshold\n",
        "    columns_to_drop = nan_proportions[nan_proportions > threshold].index\n",
        "\n",
        "    # Drop the columns with high NaN values\n",
        "    df = df.drop(columns=columns_to_drop)\n",
        "\n",
        "    print(f\"Columns dropped due to high NaN values: {list(columns_to_drop)}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Usage:\n",
        "\n",
        "\n",
        "def process_fold(fold_num):\n",
        "    print(f\"\\nProcessing fold{fold_num}...\")\n",
        "\n",
        "    # Load the train, test, and test_y datasets\n",
        "    train_file_path = f'fold{fold_num}/train.csv'\n",
        "    test_file_path = f'fold{fold_num}/test.csv'\n",
        "    test_y_file_path = f'fold{fold_num}/test_y.csv'\n",
        "\n",
        "    housing_data_train = pd.read_csv(train_file_path)\n",
        "    housing_data_test = pd.read_csv(test_file_path)\n",
        "    test_y_data = pd.read_csv(test_y_file_path)  # Load the actual Sale Price for the test set\n",
        "\n",
        "    # Ensure that test_y_data and housing_data_test have matching PIDs\n",
        "    merged_test_data = pd.merge(housing_data_test, test_y_data, on='PID', how='inner')\n",
        "\n",
        "    # Identify categorical columns\n",
        "    categorical_columns = ['MS_SubClass', 'MS_Zoning', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities',\n",
        "                           'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type',\n",
        "                           'House_Style', 'Overall_Qual', 'Overall_Cond', 'Roof_Style', 'Roof_Matl', 'Exterior_1st',\n",
        "                           'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Qual', 'Exter_Cond', 'Foundation', 'Bsmt_Qual',\n",
        "                           'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating', 'Heating_QC',\n",
        "                           'Central_Air', 'Electrical', 'Kitchen_Qual', 'Functional', 'Fireplace_Qu', 'Garage_Type',\n",
        "                           'Garage_Finish', 'Garage_Qual', 'Garage_Cond', 'Paved_Drive', 'Pool_QC', 'Fence',\n",
        "                           'Sale_Type', 'Sale_Condition']\n",
        "\n",
        "    # Combine rare categories in both train and test datasets\n",
        "    housing_data_train = combine_rare_categories(housing_data_train, categorical_columns)\n",
        "    housing_data_test = combine_rare_categories(housing_data_test, categorical_columns)\n",
        "\n",
        "    # Remove \"Misc_Feature\" only from the train dataset\n",
        "    housing_data_train = housing_data_train.drop(columns=['Misc_Feature'], errors='ignore')\n",
        "    housing_data_test = housing_data_test.drop(columns=['Misc_Feature'], errors='ignore')\n",
        "    # Perform dummy encoding on both train and test datasets\n",
        "    housing_data_encoded_train = pd.get_dummies(housing_data_train, columns=categorical_columns, drop_first=True)\n",
        "    housing_data_encoded_test = pd.get_dummies(merged_test_data, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "    # Align columns of train and test after dummy encoding:\n",
        "    all_columns = list(set(housing_data_encoded_train.columns) | set(housing_data_encoded_test.columns))\n",
        "    all_columns.remove('Sale_Price')  # Remove 'Sale_Price' if it's in all_columns\n",
        "\n",
        "    housing_data_encoded_train = housing_data_encoded_train.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    housing_data_encoded_test = housing_data_encoded_test.reindex(columns=all_columns + ['Sale_Price'], fill_value=0)\n",
        "    print(\"housing_data_encoded_train shape\", housing_data_encoded_train.shape)\n",
        "    print(\"housing_data_encoded_test shape\", housing_data_encoded_test.shape)\n",
        "\n",
        "    # Handle missing values by imputing them in train and test data\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    numeric_columns = housing_data_encoded_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numeric_columns = [col for col in numeric_columns if col not in ['Sale_Price', 'PID']]\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_train.copy()\n",
        "    housing_data_encoded_imputed_train[numeric_columns] = imputer.fit_transform(housing_data_encoded_train[numeric_columns])\n",
        "\n",
        "    # Coding starts for removing NaN features\n",
        "\n",
        "    # Assuming housing_data_encoded_imputed_train is your DataFrame\n",
        "    threshold = 0.5  # For example, set a threshold of 50% missing values\n",
        "    housing_data_encoded_imputed_train_cleaned = remove_high_nan_columns(housing_data_encoded_imputed_train, threshold)\n",
        "\n",
        "    print(\"Cleaned dataset shape:\", housing_data_encoded_imputed_train_cleaned.shape)\n",
        "\n",
        "\n",
        "\n",
        "    # Coding ends for removing NAn features\n",
        "\n",
        "\n",
        "    housing_data_encoded_imputed_test = housing_data_encoded_test.copy()\n",
        "    housing_data_encoded_imputed_test[numeric_columns] = imputer.transform(housing_data_encoded_test[numeric_columns])\n",
        "    print(\"housing_data_encoded_imputed_train shape\", housing_data_encoded_imputed_train.shape)\n",
        "    print(\"housing_data_encoded_imputed_test shape\", housing_data_encoded_imputed_test.shape)\n",
        "\n",
        "    # Remove 'PID' from train and test datasets\n",
        "    housing_data_encoded_imputed_train = housing_data_encoded_imputed_train.drop(columns=['PID'])\n",
        "    if 'PID' in housing_data_encoded_imputed_test.columns:\n",
        "        housing_data_encoded_imputed_test = housing_data_encoded_imputed_test.drop(columns=['PID'])\n",
        "\n",
        "    # Separate predictors (X) and target variable (y) for training\n",
        "    X_trn = housing_data_encoded_imputed_train.drop(columns='Sale_Price')\n",
        "    y_trn = np.log(housing_data_encoded_imputed_train['Sale_Price'])\n",
        "\n",
        "    # Scale the training data using preprocess_data function (returns DataFrame with column names)\n",
        "    X_trn_scaled, scaler = preprocess_data(X_trn)\n",
        "\n",
        "    # 1. Use Lasso regression for feature selection\n",
        "    lasso_model = Lasso(alpha=0.01)  # Adjust the alpha value as necessary\n",
        "    lasso_model.fit(X_trn_scaled, y_trn)\n",
        "    selected_features = X_trn_scaled.columns[lasso_model.coef_ != 0]\n",
        "    print(f\"Selected features after Lasso: {len(selected_features)} features\")\n",
        "\n",
        "    # Scale the test data using the same scaler\n",
        "    X_tst_scaled = pd.DataFrame(scaler.transform(housing_data_encoded_imputed_test.drop(columns=['Sale_Price'])), columns=housing_data_encoded_imputed_test.drop(columns=['Sale_Price']).columns)\n",
        "    print(\"X_tst_scaled shape\", X_tst_scaled.shape)\n",
        "\n",
        "    # 2. Grid Search for Ridge Regression to find the best alpha\n",
        "    alpha_values = np.exp(np.linspace(-5, 5, 100))\n",
        "    param_grid = {'alpha': alpha_values}\n",
        "\n",
        "    ridge_model = Ridge()\n",
        "    grid_search = GridSearchCV(ridge_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "    grid_search.fit(X_trn_scaled[selected_features], y_trn)\n",
        "    best_alpha = grid_search.best_params_['alpha']\n",
        "    print(\"Best alpha for Ridge regression:\", best_alpha)\n",
        "\n",
        "    # 3. Train the final Ridge model with the best alpha\n",
        "    final_ridge_model = Ridge(alpha=best_alpha)\n",
        "    final_ridge_model.fit(X_trn_scaled[selected_features], y_trn)\n",
        "\n",
        "    # Predict on the training set\n",
        "    y_trn_pred = final_ridge_model.predict(X_trn_scaled[selected_features])\n",
        "    r2_train = r2_score(y_trn, y_trn_pred)\n",
        "    adj_r2_train = adjusted_r_squared(r2_train, len(y_trn), X_trn_scaled[selected_features].shape[1])\n",
        "    train_rmse = calc_rmse(y_trn, y_trn_pred)\n",
        "    print(f\"Training R-squared: {r2_train}\")\n",
        "    print(f\"Adjusted R-squared (Training): {adj_r2_train}\")\n",
        "    print(f\"Training RMSE: {train_rmse}\")\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_tst_pred_log = final_ridge_model.predict(X_tst_scaled[selected_features])\n",
        "\n",
        "    # Reverse the log-transformation to get predictions in the original scale\n",
        "    y_tst_pred = np.exp(y_tst_pred_log)\n",
        "\n",
        "    # Merge the predictions with actual sale prices from test_y.csv using 'PID'\n",
        "    predictions = pd.DataFrame({\n",
        "        'PID': merged_test_data['PID'],\n",
        "        'Predicted_Sale_Price': y_tst_pred\n",
        "    })\n",
        "    merged_test_data = pd.merge(test_y_data, predictions, on='PID', how='inner')\n",
        "\n",
        "    # Calculate RMSE and R-squared for the test set using actual sale prices\n",
        "    test_rmse = calc_rmse(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    r2_test = r2_score(np.log(merged_test_data['Sale_Price']), np.log(merged_test_data['Predicted_Sale_Price']))\n",
        "    adj_r2_test = adjusted_r_squared(r2_test, len(merged_test_data), X_tst_scaled.shape[1])\n",
        "    print(f\"Test RMSE: {test_rmse}\")\n",
        "    print(f\"Test R-squared: {r2_test}\")\n",
        "    print(f\"Adjusted R-squared (Test): {adj_r2_test}\")\n",
        "\n",
        "    # Save the predictions to a CSV file\n",
        "    predictions_file = f'fold{fold_num}_test_predictions.csv'\n",
        "    predictions.to_csv(predictions_file, index=False)\n",
        "    print(f\"Predictions for fold{fold_num} saved to '{predictions_file}'.\")\n",
        "\n",
        "def main():\n",
        "    # Loop over folds 1 to 10\n",
        "    for fold_num in range(1, 11):\n",
        "        process_fold(fold_num)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h_ZUoxOwKMIJ",
        "outputId": "c6f15765-9bbe-41f9-dc84-4f305f911e57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing fold1...\n",
            "df             PID                          MS_SubClass                MS_Zoning  \\\n",
            "0     526350040  One_Story_1946_and_Newer_All_Styles                    Other   \n",
            "1     526351010  One_Story_1946_and_Newer_All_Styles  Residential_Low_Density   \n",
            "2     527105010             Two_Story_1946_and_Newer  Residential_Low_Density   \n",
            "3     527105030             Two_Story_1946_and_Newer  Residential_Low_Density   \n",
            "4     527127150         One_Story_PUD_1946_and_Newer  Residential_Low_Density   \n",
            "...         ...                                  ...                      ...   \n",
            "2046  923275080                  Split_or_Multilevel  Residential_Low_Density   \n",
            "2047  923276100  One_Story_1946_and_Newer_All_Styles  Residential_Low_Density   \n",
            "2048  923400125                                Other  Residential_Low_Density   \n",
            "2049  924100070  One_Story_1946_and_Newer_All_Styles  Residential_Low_Density   \n",
            "2050  924151050             Two_Story_1946_and_Newer  Residential_Low_Density   \n",
            "\n",
            "      Lot_Frontage  Lot_Area Street            Alley           Lot_Shape  \\\n",
            "0               80     11622   Pave  No_Alley_Access             Regular   \n",
            "1               81     14267   Pave  No_Alley_Access  Slightly_Irregular   \n",
            "2               74     13830   Pave  No_Alley_Access  Slightly_Irregular   \n",
            "3               78      9978   Pave  No_Alley_Access  Slightly_Irregular   \n",
            "4               41      4920   Pave  No_Alley_Access             Regular   \n",
            "...            ...       ...    ...              ...                 ...   \n",
            "2046            37      7937   Pave  No_Alley_Access  Slightly_Irregular   \n",
            "2047             0      8885   Pave  No_Alley_Access  Slightly_Irregular   \n",
            "2048            62     10441   Pave  No_Alley_Access             Regular   \n",
            "2049            77     10010   Pave  No_Alley_Access             Regular   \n",
            "2050            74      9627   Pave  No_Alley_Access             Regular   \n",
            "\n",
            "     Land_Contour Utilities  ...            Fence Misc_Feature Misc_Val  \\\n",
            "0             Lvl    AllPub  ...  Minimum_Privacy          NaN        0   \n",
            "1             Lvl    AllPub  ...         No_Fence         Gar2    12500   \n",
            "2             Lvl    AllPub  ...  Minimum_Privacy          NaN        0   \n",
            "3             Lvl    AllPub  ...         No_Fence          NaN        0   \n",
            "4             Lvl    AllPub  ...         No_Fence          NaN        0   \n",
            "...           ...       ...  ...              ...          ...      ...   \n",
            "2046          Lvl    AllPub  ...     Good_Privacy          NaN        0   \n",
            "2047          Low    AllPub  ...  Minimum_Privacy          NaN        0   \n",
            "2048          Lvl    AllPub  ...  Minimum_Privacy         Shed      700   \n",
            "2049          Lvl    AllPub  ...         No_Fence          NaN        0   \n",
            "2050          Lvl    AllPub  ...         No_Fence          NaN        0   \n",
            "\n",
            "     Mo_Sold Year_Sold Sale_Type Sale_Condition  Longitude   Latitude  \\\n",
            "0          6      2010       WD          Normal -93.619756  42.053014   \n",
            "1          6      2010       WD          Normal -93.619387  42.052659   \n",
            "2          3      2010       WD          Normal -93.638933  42.060899   \n",
            "3          6      2010       WD          Normal -93.638925  42.060779   \n",
            "4          4      2010       WD          Normal -93.633792  42.062978   \n",
            "...      ...       ...       ...            ...        ...        ...   \n",
            "2046       3      2006       WD          Normal -93.604776  41.988964   \n",
            "2047       6      2006       WD          Normal -93.602680  41.988314   \n",
            "2048       7      2006       WD          Normal -93.606847  41.986510   \n",
            "2049       4      2006       WD          Normal -93.600190  41.990921   \n",
            "2050      11      2006       WD          Normal -93.599996  41.989265   \n",
            "\n",
            "      Sale_Price  \n",
            "0         105000  \n",
            "1         172000  \n",
            "2         189900  \n",
            "3         195500  \n",
            "4         213500  \n",
            "...          ...  \n",
            "2046      142500  \n",
            "2047      131000  \n",
            "2048      132000  \n",
            "2049      170000  \n",
            "2050      188000  \n",
            "\n",
            "[2051 rows x 83 columns]\n",
            "df            PID                           MS_SubClass  \\\n",
            "0    908276150   One_Story_1946_and_Newer_All_Styles   \n",
            "1    903451090              Two_Story_1945_and_Older   \n",
            "2    527110080              Two_Story_1946_and_Newer   \n",
            "3    535457010   One_Story_1946_and_Newer_All_Styles   \n",
            "4    903484020  One_and_Half_Story_Finished_All_Ages   \n",
            "..         ...                                   ...   \n",
            "874  527353080              Two_Story_1946_and_Newer   \n",
            "875  908152110              Two_Story_1946_and_Newer   \n",
            "876  528429090   One_Story_1946_and_Newer_All_Styles   \n",
            "877  533210060          Two_Story_PUD_1946_and_Newer   \n",
            "878  527225035  One_and_Half_Story_Finished_All_Ages   \n",
            "\n",
            "                        MS_Zoning  Lot_Frontage  Lot_Area Street  \\\n",
            "0         Residential_Low_Density             0      8926   Pave   \n",
            "1      Residential_Medium_Density            57      6876   Pave   \n",
            "2         Residential_Low_Density             0     13869   Pave   \n",
            "3         Residential_Low_Density            87     10000   Pave   \n",
            "4      Residential_Medium_Density            60     10320   Pave   \n",
            "..                            ...           ...       ...    ...   \n",
            "874       Residential_Low_Density            80     11584   Pave   \n",
            "875       Residential_Low_Density            72      7200   Pave   \n",
            "876       Residential_Low_Density            84     11670   Pave   \n",
            "877  Floating_Village_Residential            30      3215   Pave   \n",
            "878       Residential_Low_Density           152     12134   Pave   \n",
            "\n",
            "               Alley             Lot_Shape Land_Contour Utilities  ...  \\\n",
            "0    No_Alley_Access    Slightly_Irregular          Lvl    AllPub  ...   \n",
            "1    No_Alley_Access               Regular          Lvl    AllPub  ...   \n",
            "2    No_Alley_Access  Moderately_Irregular          Lvl    AllPub  ...   \n",
            "3    No_Alley_Access    Slightly_Irregular          Lvl    AllPub  ...   \n",
            "4             Gravel               Regular          Lvl    AllPub  ...   \n",
            "..               ...                   ...          ...       ...  ...   \n",
            "874  No_Alley_Access               Regular          Lvl    AllPub  ...   \n",
            "875  No_Alley_Access               Regular          Lvl    AllPub  ...   \n",
            "876  No_Alley_Access    Slightly_Irregular          Lvl    AllPub  ...   \n",
            "877            Other               Regular          Lvl    AllPub  ...   \n",
            "878  No_Alley_Access    Slightly_Irregular          Bnk    AllPub  ...   \n",
            "\n",
            "     Pool_QC            Fence Misc_Feature Misc_Val Mo_Sold Year_Sold  \\\n",
            "0    No_Pool  Minimum_Privacy          NaN        0      10      2009   \n",
            "1    No_Pool  Minimum_Privacy          NaN        0       8      2009   \n",
            "2    No_Pool         No_Fence          NaN        0       7      2007   \n",
            "3    No_Pool         No_Fence          NaN        0       2      2010   \n",
            "4    No_Pool  Minimum_Privacy          NaN        0       6      2008   \n",
            "..       ...              ...          ...      ...     ...       ...   \n",
            "874  No_Pool         No_Fence          NaN        0      11      2007   \n",
            "875  No_Pool  Minimum_Privacy          NaN        0       2      2008   \n",
            "876  No_Pool         No_Fence          NaN        0       3      2007   \n",
            "877  No_Pool         No_Fence          NaN        0       4      2010   \n",
            "878  No_Pool         No_Fence          NaN        0       6      2010   \n",
            "\n",
            "    Sale_Type Sale_Condition  Longitude   Latitude  \n",
            "0         COD        Abnorml -93.663302  42.018592  \n",
            "1         WD          Normal -93.628299  42.025306  \n",
            "2         WD          Normal -93.636146  42.061664  \n",
            "3         WD          Normal -93.605943  42.034748  \n",
            "4         WD          Normal -93.624580  42.023716  \n",
            "..        ...            ...        ...        ...  \n",
            "874       WD          Normal -93.638554  42.052353  \n",
            "875       WD          Normal -93.675173  42.018912  \n",
            "876       WD          Normal -93.642069  42.054318  \n",
            "877     Other         Normal -93.645599  42.048566  \n",
            "878       WD          Normal -93.623595  42.060351  \n",
            "\n",
            "[879 rows x 82 columns]\n",
            "housing_data_encoded_train shape (2051, 307)\n",
            "housing_data_encoded_test shape (879, 307)\n",
            "NaN Proportions for each column:\n",
            "Heating_QC_Other                                         0.0\n",
            "Garage_Type_Basment                                      0.0\n",
            "Neighborhood_Northridge                                  0.0\n",
            "BsmtFin_Type_2_LwQ                                       0.0\n",
            "MS_SubClass_Other                                        0.0\n",
            "                                                        ... \n",
            "MS_SubClass_Two_Family_conversion_All_Styles_and_Ages    0.0\n",
            "Fireplace_Qu_No_Fireplace                                0.0\n",
            "Fence_Minimum_Privacy                                    0.0\n",
            "Condition_2_Feedr                                        0.0\n",
            "Sale_Price                                               0.0\n",
            "Length: 307, dtype: float64\n",
            "Columns dropped due to high NaN values: []\n",
            "Cleaned dataset shape: (2051, 307)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot use median strategy with non-numeric data:\ncould not convert string to float: 'Shed'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-d9129ebf1f1d>\u001b[0m in \u001b[0;36m<cell line: 199>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-d9129ebf1f1d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# Loop over folds 1 to 10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfold_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mprocess_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-d9129ebf1f1d>\u001b[0m in \u001b[0;36mprocess_fold\u001b[0;34m(fold_num)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mhousing_data_encoded_imputed_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhousing_data_encoded_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mhousing_data_encoded_imputed_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumeric_columns\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimputer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhousing_data_encoded_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumeric_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"housing_data_encoded_imputed_train shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhousing_data_encoded_imputed_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"housing_data_encoded_imputed_test shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhousing_data_encoded_imputed_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/impute/_base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_fit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0mstatistics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/impute/_base.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, X, in_fit)\u001b[0m\n\u001b[1;32m    346\u001b[0m                     )\n\u001b[1;32m    347\u001b[0m                 )\n\u001b[0;32m--> 348\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mnew_ve\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot use median strategy with non-numeric data:\ncould not convert string to float: 'Shed'"
          ]
        }
      ]
    }
  ]
}